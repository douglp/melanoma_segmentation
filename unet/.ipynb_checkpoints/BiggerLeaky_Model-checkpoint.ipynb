{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from imutils import paths\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "#from keras import backend as keras\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "\n",
    "#import skimage.io as io\n",
    "#import skimage.transform as trans\n",
    "\n",
    "#K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data'\n",
    "training_data_dir = os.path.join(root_dir, 'train/images')\n",
    "training_data_mask_dir = os.path.join(root_dir, 'train/masks')\n",
    "\n",
    "val_data_dir = os.path.join(root_dir, 'val/images')\n",
    "val_data_pred_dir = os.path.join(root_dir, 'val/predict')\n",
    "val_data_mask_dir = os.path.join(root_dir, 'val/masks')\n",
    "\n",
    "test_data_dir = os.path.join(root_dir, 'test/images')\n",
    "test_data_pred_dir = os.path.join(root_dir, 'test/predict')\n",
    "test_data_mask_dir = os.path.join(root_dir, 'test/masks')\n",
    "\n",
    "model_localtion = \"BiggerLeaky_512_lesion.hdf5\"\n",
    "result_file = \"BiggerLeaky_512_test_result.csv\"\n",
    "img_rows = 512\n",
    "img_cols = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    j = -jaccard_coef(y_true, y_pred)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT USE\n",
    "def normalizeData_rgb(img,mask):\n",
    "    for i in range(3):\n",
    "        mean = np.mean(img[:,:,i])  # mean for data centering\n",
    "        std = np.std(img[:,:,i])  # std for data normalization\n",
    "        img[:,:,i] -= mean\n",
    "        img[:,:,i] /= std\n",
    "    mask = mask /255\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask <= 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(img,mask):\n",
    "    mean = np.mean(img)  # mean for data centering\n",
    "    std = np.std(img)  # std for data normalization\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    mask = mask /255\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask <= 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"rgb\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (512,512),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        #img,mask = normalizeData_rgb(img,mask)\n",
    "        img, mask = normalizeData(img, mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"rgb\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (512,512),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        #img,mask = normalizeData_rgb(img,mask)\n",
    "        img, mask = normalizeData(img, mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiggerLeakyUnetModel():\n",
    "    inputs = Input((img_rows, img_cols,3))\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\")(inputs)\n",
    "    acti1 = LeakyReLU(alpha=0.001)(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\")(acti1)\n",
    "    acti1 = LeakyReLU(alpha=0.001)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(acti1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\")(pool1)\n",
    "    acti2 = LeakyReLU(alpha=0.001)(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\")(acti2)\n",
    "    acti2 = LeakyReLU(alpha=0.001)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(acti2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\")(pool2)\n",
    "    acti3 = LeakyReLU(alpha=0.001)(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\")(acti3)\n",
    "    acti3 = LeakyReLU(alpha=0.001)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(acti3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\")(pool3)\n",
    "    acti4 = LeakyReLU(alpha=0.001)(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\")(acti4)\n",
    "    acti4 = LeakyReLU(alpha=0.001)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(acti4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\")(pool4)\n",
    "    acti5 = LeakyReLU(alpha=0.001)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\")(acti5)\n",
    "    acti5 = LeakyReLU(alpha=0.001)(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(acti5)\n",
    "\n",
    "    conv6 = Conv2D(1024, (3, 3), padding=\"same\")(pool5)\n",
    "    acti6 = LeakyReLU(alpha=0.001)(conv6)\n",
    "    conv6 = Conv2D(1024, (3, 3), padding=\"same\")(acti6)\n",
    "    acti6 = LeakyReLU(alpha=0.001)(conv6)\n",
    "    pool6 = MaxPooling2D(pool_size=(2, 2))(acti6)\n",
    "    \n",
    "    conv7 = Conv2D(2048, (3, 3), padding=\"same\")(pool6)\n",
    "    acti7 = LeakyReLU(alpha=0.001)(conv7)\n",
    "    conv7 = Conv2D(2048, (3, 3), padding=\"same\")(acti7)\n",
    "    acti7 = LeakyReLU(alpha=0.001)(conv7)\n",
    "\n",
    "    \n",
    "    right_up6 = concatenate([UpSampling2D(size=(2, 2))(acti7), acti6], axis=3)\n",
    "    right_conv6 = Conv2D(1024, (3, 3), padding=\"same\")(right_up6)\n",
    "    right_acti6 = LeakyReLU(alpha=0.001)(right_conv6)\n",
    "    right_conv6 = Conv2D(1024, (3, 3), padding=\"same\")(right_acti6)\n",
    "    right_acti6 = LeakyReLU(alpha=0.001)(right_conv6)\n",
    "\n",
    "    right_up5 = concatenate([UpSampling2D(size=(2, 2))(right_acti6), acti5], axis=3)\n",
    "    right_conv5 = Conv2D(512, (3, 3), padding=\"same\")(right_up5)\n",
    "    right_acti5 = LeakyReLU(alpha=0.001)(right_conv5)\n",
    "    right_conv5 = Conv2D(512, (3, 3), padding=\"same\")(right_acti5)\n",
    "    right_acti5 = LeakyReLU(alpha=0.001)(right_conv5)\n",
    "\n",
    "    right_up4 = concatenate([UpSampling2D(size=(2, 2))(right_acti5), acti4], axis=3)\n",
    "    right_conv4 = Conv2D(256, (3, 3), padding=\"same\")(right_up4)\n",
    "    right_acti4 = LeakyReLU(alpha=0.001)(right_conv4)\n",
    "    right_conv4 = Conv2D(256, (3, 3), padding=\"same\")(right_acti4)\n",
    "    right_acti4 = LeakyReLU(alpha=0.001)(right_conv4)\n",
    "\n",
    "    right_up3 = concatenate([UpSampling2D(size=(2, 2))(right_acti4), acti3], axis=3)\n",
    "    right_conv3 = Conv2D(128, (3, 3), padding=\"same\")(right_up3)\n",
    "    right_acti3 = LeakyReLU(alpha=0.001)(right_conv3)\n",
    "    right_conv3 = Conv2D(128, (3, 3), padding=\"same\")(right_acti3)\n",
    "    right_acti3 = LeakyReLU(alpha=0.001)(right_conv3)\n",
    "\n",
    "    right_up2 = concatenate([UpSampling2D(size=(2, 2))(right_acti3), acti2], axis=3)\n",
    "    right_conv2 = Conv2D(64, (3, 3), padding=\"same\")(right_up2)\n",
    "    right_acti2 = LeakyReLU(alpha=0.001)(right_conv2)\n",
    "    right_conv2 = Conv2D(64, (3, 3), padding=\"same\")(right_acti2)\n",
    "    right_acti2 = LeakyReLU(alpha=0.001)(right_conv2)\n",
    "\n",
    "    right_up1 = concatenate([UpSampling2D(size=(2, 2))(right_acti2), acti1], axis=3)\n",
    "    right_conv1 = Conv2D(32, (3, 3), padding=\"same\")(right_up1)\n",
    "    right_acti1 = LeakyReLU(alpha=0.001)(right_conv1)\n",
    "    right_conv1 = Conv2D(32, (3, 3), padding=\"same\")(right_acti1)\n",
    "    right_acti1 = LeakyReLU(alpha=0.001)(right_conv1)\n",
    "\n",
    "    output = Conv2D(1, (1, 1), activation='sigmoid')(right_acti1)\n",
    "\n",
    "    model = Model(input=inputs, output=output)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=5e-5), loss=jaccard_coef_loss, metrics=[jaccard_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define UNet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/udir/gwang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 512, 512, 32) 896         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 512, 512, 32) 0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 512, 512, 32) 9248        leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 512, 512, 32) 0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 256, 256, 32) 0           leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 256, 256, 64) 18496       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 256, 256, 64) 36928       leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 128, 128, 64) 0           leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 128 73856       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)      (None, 128, 128, 128 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 128, 128 147584      leaky_re_lu_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)      (None, 128, 128, 128 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 64, 64, 128)  0           leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 64, 64, 256)  295168      max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 64, 64, 256)  0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 64, 64, 256)  590080      leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 64, 64, 256)  0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 32, 32, 256)  0           leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 512)  1180160     max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)      (None, 32, 32, 512)  0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 512)  2359808     leaky_re_lu_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)      (None, 32, 32, 512)  0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 512)  0           leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 16, 16, 1024) 4719616     max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)      (None, 16, 16, 1024) 0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 16, 16, 1024) 9438208     leaky_re_lu_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)      (None, 16, 16, 1024) 0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 8, 8, 1024)   0           leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 2048)   18876416    max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)      (None, 8, 8, 2048)   0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 2048)   37750784    leaky_re_lu_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 8, 8, 2048)   0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 16, 16, 2048) 0           leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 3072) 0           up_sampling2d_7[0][0]            \n",
      "                                                                 leaky_re_lu_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 16, 16, 512)  14156288    concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 16, 16, 512)  2359808     leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 16, 16, 512)  0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 32, 32, 512)  0           leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 1024) 0           up_sampling2d_8[0][0]            \n",
      "                                                                 leaky_re_lu_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 32, 32, 512)  0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 512)  2359808     leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 32, 32, 512)  0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 64, 64, 512)  0           leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 64, 64, 768)  0           up_sampling2d_9[0][0]            \n",
      "                                                                 leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 64, 256)  1769728     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 64, 64, 256)  0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 64, 256)  590080      leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 64, 64, 256)  0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 128, 128, 256 0           leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128, 128, 384 0           up_sampling2d_10[0][0]           \n",
      "                                                                 leaky_re_lu_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 128, 128, 128 442496      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 128, 128, 128 0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 128, 128, 128 147584      leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 128, 128, 128 0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 256, 256, 128 0           leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256, 256, 192 0           up_sampling2d_11[0][0]           \n",
      "                                                                 leaky_re_lu_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 256, 256, 64) 110656      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 256, 256, 64) 36928       leaky_re_lu_49[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 256, 256, 64) 0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 512, 512, 64) 0           leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 512, 512, 96) 0           up_sampling2d_12[0][0]           \n",
      "                                                                 leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 512, 512, 32) 27680       concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 512, 512, 32) 0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 512, 512, 32) 9248        leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 512, 512, 32) 0           conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 512, 512, 1)  33          leaky_re_lu_52[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 102,226,689\n",
      "Trainable params: 102,226,689\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Training data generation\n",
    "data_gen_args = dict(\n",
    "#    samplewise_center = True,\n",
    "#    samplewise_std_normalization = True,\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip = True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#Validation data generation\n",
    "data_val_gen_args = dict(\n",
    "    #samplewise_center = True,\n",
    "    #samplewise_std_normalization = True\n",
    "    )\n",
    "\n",
    "#Create UNet Model\n",
    "#model = FullUnetModel()\n",
    "#model = UnetModel()\n",
    "#model = AttentionUnetModel()\n",
    "model = BiggerLeakyUnetModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup generator\n",
    "batch_size = 2\n",
    "\n",
    "myGene = trainGenerator(batch_size,'data/train','images','masks',data_gen_args)\n",
    "myValGene = validationGenerator(batch_size,'data/val','images','masks',data_val_gen_args)\n",
    "\n",
    "#Setup Checkpoint to only capture best estimate\n",
    "model_checkpoint = ModelCheckpoint(model_localtion, monitor='loss',verbose=1, save_best_only=True)\n",
    "\n",
    "#Enable tensorboard\n",
    "tensorBoard = TensorBoard(\n",
    "    log_dir='./logs', \n",
    "    histogram_freq=0, \n",
    "    batch_size=batch_size, \n",
    "    write_graph=True, \n",
    "    write_grads=False, \n",
    "    write_images=True, \n",
    "    embeddings_freq=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Found 1555 images belonging to 1 classes.\n",
      "Found 519 images belonging to 1 classes.\n",
      "Found 1555 images belonging to 1 classes.\n",
      "Found 519 images belonging to 1 classes.\n",
      "1000/1000 [==============================] - 717s 717ms/step - loss: -0.0214 - jaccard_coef: 0.0214 - val_loss: -1.6067e-05 - val_jaccard_coef: 1.6067e-05\n",
      "\n",
      "Epoch 00001: loss improved from inf to -0.02140, saving model to BiggerLeaky_512_lesion.hdf5\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 682s 682ms/step - loss: -1.6770e-05 - jaccard_coef: 1.6770e-05 - val_loss: -1.7038e-05 - val_jaccard_coef: 1.7038e-05\n",
      "\n",
      "Epoch 00002: loss did not improve from -0.02140\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 718s 718ms/step - loss: -1.7161e-05 - jaccard_coef: 1.7161e-05 - val_loss: -1.6757e-05 - val_jaccard_coef: 1.6757e-05\n",
      "\n",
      "Epoch 00003: loss did not improve from -0.02140\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 685s 685ms/step - loss: -1.8467e-05 - jaccard_coef: 1.8467e-05 - val_loss: -1.5247e-05 - val_jaccard_coef: 1.5247e-05\n",
      "\n",
      "Epoch 00004: loss did not improve from -0.02140\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: -1.6898e-05 - jaccard_coef: 1.6898e-05 - val_loss: -1.7574e-05 - val_jaccard_coef: 1.7574e-05\n",
      "\n",
      "Epoch 00005: loss did not improve from -0.02140\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 686s 686ms/step - loss: -1.7204e-05 - jaccard_coef: 1.7204e-05 - val_loss: -1.6398e-05 - val_jaccard_coef: 1.6398e-05\n",
      "\n",
      "Epoch 00006: loss did not improve from -0.02140\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 693s 693ms/step - loss: -1.7030e-05 - jaccard_coef: 1.7030e-05 - val_loss: -1.5709e-05 - val_jaccard_coef: 1.5709e-05\n",
      "\n",
      "Epoch 00007: loss did not improve from -0.02140\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 691s 691ms/step - loss: -1.7449e-05 - jaccard_coef: 1.7449e-05 - val_loss: -1.7365e-05 - val_jaccard_coef: 1.7365e-05\n",
      "\n",
      "Epoch 00008: loss did not improve from -0.02140\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 699s 699ms/step - loss: -1.8342e-05 - jaccard_coef: 1.8342e-05 - val_loss: -1.5691e-05 - val_jaccard_coef: 1.5691e-05\n",
      "\n",
      "Epoch 00009: loss did not improve from -0.02140\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 692s 692ms/step - loss: -1.6978e-05 - jaccard_coef: 1.6978e-05 - val_loss: -1.7101e-05 - val_jaccard_coef: 1.7101e-05\n",
      "\n",
      "Epoch 00010: loss did not improve from -0.02140\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: -1.7329e-05 - jaccard_coef: 1.7329e-05 - val_loss: -1.6896e-05 - val_jaccard_coef: 1.6896e-05\n",
      "\n",
      "Epoch 00011: loss did not improve from -0.02140\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 686s 686ms/step - loss: -1.7511e-05 - jaccard_coef: 1.7511e-05 - val_loss: -1.6830e-05 - val_jaccard_coef: 1.6830e-05\n",
      "\n",
      "Epoch 00012: loss did not improve from -0.02140\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 689s 689ms/step - loss: -0.0204 - jaccard_coef: 0.0204 - val_loss: -1.7365e-05 - val_jaccard_coef: 1.7365e-05\n",
      "\n",
      "Epoch 00013: loss did not improve from -0.02140\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 688s 688ms/step - loss: -1.6777e-05 - jaccard_coef: 1.6777e-05 - val_loss: -1.6740e-05 - val_jaccard_coef: 1.6740e-05\n",
      "\n",
      "Epoch 00014: loss did not improve from -0.02140\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 690s 690ms/step - loss: -1.7304e-05 - jaccard_coef: 1.7304e-05 - val_loss: -1.8434e-05 - val_jaccard_coef: 1.8434e-05\n",
      "\n",
      "Epoch 00015: loss did not improve from -0.02140\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 689s 689ms/step - loss: -1.6884e-05 - jaccard_coef: 1.6884e-05 - val_loss: -1.7611e-05 - val_jaccard_coef: 1.7611e-05\n",
      "\n",
      "Epoch 00016: loss did not improve from -0.02140\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 687s 687ms/step - loss: -1.6555e-05 - jaccard_coef: 1.6555e-05 - val_loss: -1.6814e-05 - val_jaccard_coef: 1.6814e-05\n",
      "\n",
      "Epoch 00017: loss did not improve from -0.02140\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 707s 707ms/step - loss: -1.7424e-05 - jaccard_coef: 1.7424e-05 - val_loss: -1.7187e-05 - val_jaccard_coef: 1.7187e-05\n",
      "\n",
      "Epoch 00018: loss did not improve from -0.02140\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 692s 692ms/step - loss: -1.7009e-05 - jaccard_coef: 1.7009e-05 - val_loss: -1.5691e-05 - val_jaccard_coef: 1.5691e-05\n",
      "\n",
      "Epoch 00019: loss did not improve from -0.02140\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 698s 698ms/step - loss: -1.7443e-05 - jaccard_coef: 1.7443e-05 - val_loss: -1.7048e-05 - val_jaccard_coef: 1.7048e-05\n",
      "\n",
      "Epoch 00020: loss did not improve from -0.02140\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 684s 684ms/step - loss: -1.7197e-05 - jaccard_coef: 1.7197e-05 - val_loss: -1.6383e-05 - val_jaccard_coef: 1.6383e-05\n",
      "\n",
      "Epoch 00021: loss did not improve from -0.02140\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 694s 694ms/step - loss: -1.7406e-05 - jaccard_coef: 1.7406e-05 - val_loss: -1.5858e-05 - val_jaccard_coef: 1.5858e-05\n",
      "\n",
      "Epoch 00022: loss did not improve from -0.02140\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 678s 678ms/step - loss: -1.7022e-05 - jaccard_coef: 1.7022e-05 - val_loss: -1.6283e-05 - val_jaccard_coef: 1.6283e-05\n",
      "\n",
      "Epoch 00023: loss did not improve from -0.02140\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 696s 696ms/step - loss: -1.7730e-05 - jaccard_coef: 1.7730e-05 - val_loss: -1.7482e-05 - val_jaccard_coef: 1.7482e-05\n",
      "\n",
      "Epoch 00024: loss did not improve from -0.02140\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 692s 692ms/step - loss: -1.7618e-05 - jaccard_coef: 1.7618e-05 - val_loss: -1.7104e-05 - val_jaccard_coef: 1.7104e-05\n",
      "\n",
      "Epoch 00025: loss did not improve from -0.02140\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 686s 686ms/step - loss: -1.5950e-05 - jaccard_coef: 1.5950e-05 - val_loss: -1.7706e-05 - val_jaccard_coef: 1.7706e-05\n",
      "\n",
      "Epoch 00026: loss did not improve from -0.02140\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 692s 692ms/step - loss: -1.7451e-05 - jaccard_coef: 1.7451e-05 - val_loss: -1.7919e-05 - val_jaccard_coef: 1.7919e-05\n",
      "\n",
      "Epoch 00027: loss did not improve from -0.02140\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 692s 692ms/step - loss: -1.7610e-05 - jaccard_coef: 1.7610e-05 - val_loss: -1.5967e-05 - val_jaccard_coef: 1.5967e-05\n",
      "\n",
      "Epoch 00028: loss did not improve from -0.02140\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 694s 694ms/step - loss: -1.7363e-05 - jaccard_coef: 1.7363e-05 - val_loss: -1.6808e-05 - val_jaccard_coef: 1.6808e-05\n",
      "\n",
      "Epoch 00029: loss did not improve from -0.02140\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 682s 682ms/step - loss: -1.7120e-05 - jaccard_coef: 1.7120e-05 - val_loss: -1.4970e-05 - val_jaccard_coef: 1.4970e-05\n",
      "\n",
      "Epoch 00030: loss did not improve from -0.02140\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 697s 697ms/step - loss: -1.6942e-05 - jaccard_coef: 1.6942e-05 - val_loss: -1.8520e-05 - val_jaccard_coef: 1.8520e-05\n",
      "\n",
      "Epoch 00031: loss did not improve from -0.02140\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 679s 679ms/step - loss: -1.6704e-05 - jaccard_coef: 1.6704e-05 - val_loss: -1.7981e-05 - val_jaccard_coef: 1.7981e-05\n",
      "\n",
      "Epoch 00032: loss did not improve from -0.02140\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 697s 697ms/step - loss: -1.7413e-05 - jaccard_coef: 1.7413e-05 - val_loss: -1.5495e-05 - val_jaccard_coef: 1.5495e-05\n",
      "\n",
      "Epoch 00033: loss did not improve from -0.02140\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 687s 687ms/step - loss: -1.7680e-05 - jaccard_coef: 1.7680e-05 - val_loss: -1.6602e-05 - val_jaccard_coef: 1.6602e-05\n",
      "\n",
      "Epoch 00034: loss did not improve from -0.02140\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 693s 693ms/step - loss: -1.7006e-05 - jaccard_coef: 1.7006e-05 - val_loss: -1.6320e-05 - val_jaccard_coef: 1.6320e-05\n",
      "\n",
      "Epoch 00035: loss did not improve from -0.02140\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 693s 693ms/step - loss: -1.7507e-05 - jaccard_coef: 1.7507e-05 - val_loss: -1.4712e-05 - val_jaccard_coef: 1.4712e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: loss did not improve from -0.02140\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 696s 696ms/step - loss: -1.8038e-05 - jaccard_coef: 1.8038e-05 - val_loss: -1.8011e-05 - val_jaccard_coef: 1.8011e-05\n",
      "\n",
      "Epoch 00037: loss did not improve from -0.02140\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 687s 687ms/step - loss: -1.5925e-05 - jaccard_coef: 1.5925e-05 - val_loss: -1.7607e-05 - val_jaccard_coef: 1.7607e-05\n",
      "\n",
      "Epoch 00038: loss did not improve from -0.02140\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 691s 691ms/step - loss: -1.7946e-05 - jaccard_coef: 1.7946e-05 - val_loss: -1.7313e-05 - val_jaccard_coef: 1.7313e-05\n",
      "\n",
      "Epoch 00039: loss did not improve from -0.02140\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 685s 685ms/step - loss: -1.7005e-05 - jaccard_coef: 1.7005e-05 - val_loss: -1.5796e-05 - val_jaccard_coef: 1.5796e-05\n",
      "\n",
      "Epoch 00040: loss did not improve from -0.02140\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: -1.7092e-05 - jaccard_coef: 1.7092e-05 - val_loss: -1.5727e-05 - val_jaccard_coef: 1.5727e-05\n",
      "\n",
      "Epoch 00041: loss did not improve from -0.02140\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 687s 687ms/step - loss: -1.7157e-05 - jaccard_coef: 1.7157e-05 - val_loss: -1.7511e-05 - val_jaccard_coef: 1.7511e-05\n",
      "\n",
      "Epoch 00042: loss did not improve from -0.02140\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 684s 684ms/step - loss: -1.7146e-05 - jaccard_coef: 1.7146e-05 - val_loss: -1.7671e-05 - val_jaccard_coef: 1.7671e-05\n",
      "\n",
      "Epoch 00043: loss did not improve from -0.02140\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 716s 716ms/step - loss: -1.5948e-05 - jaccard_coef: 1.5948e-05 - val_loss: -1.5398e-05 - val_jaccard_coef: 1.5398e-05\n",
      "\n",
      "Epoch 00044: loss did not improve from -0.02140\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 687s 687ms/step - loss: -1.7734e-05 - jaccard_coef: 1.7734e-05 - val_loss: -1.5520e-05 - val_jaccard_coef: 1.5520e-05\n",
      "\n",
      "Epoch 00045: loss did not improve from -0.02140\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 684s 684ms/step - loss: -1.7779e-05 - jaccard_coef: 1.7779e-05 - val_loss: -1.7086e-05 - val_jaccard_coef: 1.7086e-05\n",
      "\n",
      "Epoch 00046: loss did not improve from -0.02140\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 694s 694ms/step - loss: -1.7334e-05 - jaccard_coef: 1.7334e-05 - val_loss: -1.8245e-05 - val_jaccard_coef: 1.8245e-05\n",
      "\n",
      "Epoch 00047: loss did not improve from -0.02140\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 681s 681ms/step - loss: -1.8307e-05 - jaccard_coef: 1.8307e-05 - val_loss: -1.6543e-05 - val_jaccard_coef: 1.6543e-05\n",
      "\n",
      "Epoch 00048: loss did not improve from -0.02140\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 690s 690ms/step - loss: -1.7122e-05 - jaccard_coef: 1.7122e-05 - val_loss: -1.6637e-05 - val_jaccard_coef: 1.6637e-05\n",
      "\n",
      "Epoch 00049: loss did not improve from -0.02140\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 690s 690ms/step - loss: -1.7239e-05 - jaccard_coef: 1.7239e-05 - val_loss: -1.7368e-05 - val_jaccard_coef: 1.7368e-05\n",
      "\n",
      "Epoch 00050: loss did not improve from -0.02140\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 690s 690ms/step - loss: -1.7451e-05 - jaccard_coef: 1.7451e-05 - val_loss: -1.6037e-05 - val_jaccard_coef: 1.6037e-05\n",
      "\n",
      "Epoch 00051: loss did not improve from -0.02140\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 697s 697ms/step - loss: -1.6917e-05 - jaccard_coef: 1.6917e-05 - val_loss: -1.5943e-05 - val_jaccard_coef: 1.5943e-05\n",
      "\n",
      "Epoch 00052: loss did not improve from -0.02140\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 695s 695ms/step - loss: -1.7039e-05 - jaccard_coef: 1.7039e-05 - val_loss: -1.6916e-05 - val_jaccard_coef: 1.6916e-05\n",
      "\n",
      "Epoch 00053: loss did not improve from -0.02140\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 699s 699ms/step - loss: -1.6710e-05 - jaccard_coef: 1.6710e-05 - val_loss: -1.6420e-05 - val_jaccard_coef: 1.6420e-05\n",
      "\n",
      "Epoch 00054: loss did not improve from -0.02140\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 690s 690ms/step - loss: -1.8525e-05 - jaccard_coef: 1.8525e-05 - val_loss: -1.6048e-05 - val_jaccard_coef: 1.6048e-05\n",
      "\n",
      "Epoch 00055: loss did not improve from -0.02140\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 686s 686ms/step - loss: -1.7929e-05 - jaccard_coef: 1.7929e-05 - val_loss: -1.6888e-05 - val_jaccard_coef: 1.6888e-05\n",
      "\n",
      "Epoch 00056: loss did not improve from -0.02140\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 681s 681ms/step - loss: -1.8179e-05 - jaccard_coef: 1.8179e-05 - val_loss: -1.7502e-05 - val_jaccard_coef: 1.7502e-05\n",
      "\n",
      "Epoch 00057: loss did not improve from -0.02140\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 680s 680ms/step - loss: -1.6922e-05 - jaccard_coef: 1.6922e-05 - val_loss: -1.4852e-05 - val_jaccard_coef: 1.4852e-05\n",
      "\n",
      "Epoch 00058: loss did not improve from -0.02140\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: -1.7651e-05 - jaccard_coef: 1.7651e-05 - val_loss: -1.6678e-05 - val_jaccard_coef: 1.6678e-05\n",
      "\n",
      "Epoch 00059: loss did not improve from -0.02140\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 687s 687ms/step - loss: -1.7641e-05 - jaccard_coef: 1.7641e-05 - val_loss: -1.7720e-05 - val_jaccard_coef: 1.7720e-05\n",
      "\n",
      "Epoch 00060: loss did not improve from -0.02140\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: -1.7046e-05 - jaccard_coef: 1.7046e-05 - val_loss: -1.5387e-05 - val_jaccard_coef: 1.5387e-05\n",
      "\n",
      "Epoch 00061: loss did not improve from -0.02140\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 695s 695ms/step - loss: -1.7115e-05 - jaccard_coef: 1.7115e-05 - val_loss: -1.7426e-05 - val_jaccard_coef: 1.7426e-05\n",
      "\n",
      "Epoch 00062: loss did not improve from -0.02140\n",
      "Epoch 63/100\n",
      "  32/1000 [..............................] - ETA: 7:13 - loss: -1.4018e-05 - jaccard_coef: 1.4018e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-286019e1edce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorBoard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyValGene\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     validation_steps=300)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#Train\n",
    "history = model.fit_generator(\n",
    "    myGene,\n",
    "    steps_per_epoch = 1000, \n",
    "    #epochs=100,\n",
    "    epochs = 100,\n",
    "    callbacks=[model_checkpoint,tensorBoard],\n",
    "    validation_data=myValGene,\n",
    "    validation_steps=300)\n",
    "print (history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhistory2 = model.fit_generator(\\n    myGene,\\n    steps_per_epoch = 1000, \\n    epochs=100,\\n    callbacks=[model_checkpoint,tensorBoard], \\n    initial_epoch = 125,\\n    validation_data=myValGene,\\n    validation_steps=200)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Continue traing\n",
    "#Use initial_epoch \n",
    "'''\n",
    "history2 = model.fit_generator(\n",
    "    myGene,\n",
    "    steps_per_epoch = 1000, \n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint,tensorBoard], \n",
    "    initial_epoch = 125,\n",
    "    validation_data=myValGene,\n",
    "    validation_steps=200)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "print (history.history['val_loss'])\n",
    "plt.plot(history.history['jaccard_coef'])\n",
    "plt.plot(history.history['val_jaccard_coef'])\n",
    "plt.title('Coefficiency')\n",
    "plt.ylabel('Coefficiency')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.legend(['Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/udir/gwang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "\n",
    "model = AttentionUnetModel()\n",
    "model.load_weights(model_localtion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Model in module keras.engine.training object:\n",
      "\n",
      "class Model(keras.engine.network.Network)\n",
      " |  Model(*args, **kwargs)\n",
      " |  \n",
      " |  The `Model` class adds training & evaluation routines to a `Network`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Model\n",
      " |      keras.engine.network.Network\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          optimizer: String (name of optimizer) or optimizer instance.\n",
      " |              See [optimizers](/optimizers).\n",
      " |          loss: String (name of objective function) or objective function.\n",
      " |              See [losses](/losses).\n",
      " |              If the model has multiple outputs, you can use a different loss\n",
      " |              on each output by passing a dictionary or a list of losses.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the sum of all individual losses.\n",
      " |          metrics: List of metrics to be evaluated by the model\n",
      " |              during training and testing.\n",
      " |              Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary,\n",
      " |              such as `metrics={'output_a': 'accuracy'}`.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions\n",
      " |              of different model outputs.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the *weighted sum* of all individual losses,\n",
      " |              weighted by the `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping\n",
      " |              to the model's outputs. If a tensor, it is expected to map\n",
      " |              output names (strings) to scalar coefficients.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create placeholders for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensors (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument. It can be\n",
      " |              a single tensor (for a single-output model), a list of tensors,\n",
      " |              or a dict mapping output names to target tensors.\n",
      " |          **kwargs: When using the Theano/CNTK backends, these arguments\n",
      " |              are passed into `K.function`.\n",
      " |              When using the TensorFlow backend,\n",
      " |              these arguments are passed into `tf.Session.run`.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of test data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per evaluation step.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          verbose: 0 or 1. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the test samples, used for weighting the loss function.\n",
      " |              You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data\n",
      " |      as accepted by `test_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding tuples (inputs, targets)\n",
      " |              or (inputs, targets, sample_weights)\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: if True, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
      " |      Trains the model for a given number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling.\n",
      " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
      " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined.\n",
      " |          validation_steps: Only relevant if `steps_per_epoch`\n",
      " |              is specified. Total number of steps (batches of samples)\n",
      " |              to validate before stopping.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Trains the model on data generated batch-by-batch by a Python generator\n",
      " |      (or an instance of `Sequence`).\n",
      " |      \n",
      " |      The generator is run in parallel to the model, for efficiency.\n",
      " |      For instance, this allows you to do real-time data augmentation\n",
      " |      on images on CPU in parallel to training your model on GPU.\n",
      " |      \n",
      " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
      " |      and guarantees the single use of every input per epoch when\n",
      " |      using `use_multiprocessing=True`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: A generator or an instance of `Sequence`\n",
      " |              (`keras.utils.Sequence`) object in order to avoid\n",
      " |              duplicate data when using multiprocessing.\n",
      " |              The output of the generator must be either\n",
      " |              - a tuple `(inputs, targets)`\n",
      " |              - a tuple `(inputs, targets, sample_weights)`.\n",
      " |              This tuple (a single output of the generator) makes a single\n",
      " |              batch. Therefore, all arrays in this tuple must have the same\n",
      " |              length (equal to the size of this batch). Different batches may\n",
      " |              have different sizes. For example, the last batch of the epoch\n",
      " |              is commonly smaller than the others, if the size of the dataset\n",
      " |              is not divisible by the batch size.\n",
      " |              The generator is expected to loop over its data\n",
      " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
      " |              batches have been seen by the model.\n",
      " |          steps_per_epoch: Integer.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before declaring one epoch\n",
      " |              finished and starting the next epoch. It should typically\n",
      " |              be equal to the number of samples of your dataset\n",
      " |              divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire data provided,\n",
      " |              as defined by `steps_per_epoch`.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_data: This can be either\n",
      " |              - a generator or a `Sequence` object for the validation data\n",
      " |              - tuple `(x_val, y_val)`\n",
      " |              - tuple `(x_val, y_val, val_sample_weights)`\n",
      " |              on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |          validation_steps: Only relevant if `validation_data`\n",
      " |              is a generator. Total number of steps (batches of samples)\n",
      " |              to yield from `validation_data` generator before stopping\n",
      " |              at the end of every epoch. It should typically\n",
      " |              be equal to the number of samples of your\n",
      " |              validation dataset divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(validation_data)` as a number of steps.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only). This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples\n",
      " |              from an under-represented class.\n",
      " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation\n",
      " |              relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      " |              the beginning of each epoch. Only used with instances\n",
      " |              of `Sequence` (`keras.utils.Sequence`).\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields data in an invalid format.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |      def generate_arrays_from_file(path):\n",
      " |          while True:\n",
      " |              with open(path) as f:\n",
      " |                  for line in f:\n",
      " |                      # create numpy arrays of input data\n",
      " |                      # and labels, from each line in the file\n",
      " |                      x1, x2, y = process_line(line)\n",
      " |                      yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      " |      \n",
      " |      model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      " |                          steps_per_epoch=10000, epochs=10)\n",
      " |      ```\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: The input data, as a Numpy array\n",
      " |              (or list of Numpy arrays if the model has multiple inputs).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data as accepted by\n",
      " |      `predict_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding batches of input samples\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: Maximum size for the generator queue.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: If `True`, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Input samples, as a Numpy array.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |  \n",
      " |  test_on_batch(self, x, y, sample_weight=None)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of test data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |          class_weight: Optional dictionary mapping\n",
      " |              class indices (integers) to\n",
      " |              a weight (float) to apply to the model's loss for the samples\n",
      " |              from this class during training.\n",
      " |              This can be useful to tell the model to \"pay more attention\" to\n",
      " |              samples from an under-represented class.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.network.Network:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  call(self, inputs, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      A model is callable on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      \n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False)\n",
      " |      Loads all layer weights from a HDF5 save file.\n",
      " |      \n",
      " |      If `by_name` is False (default) weights are loaded\n",
      " |      based on the network's topology, meaning the architecture\n",
      " |      should be the same as when the weights were saved.\n",
      " |      Note that layers that don't have weights are not taken\n",
      " |      into account in the topological ordering, so adding or\n",
      " |      removing layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers\n",
      " |      only if they share the same name. This is useful\n",
      " |      for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |          by_name: Boolean, whether to load weights by name\n",
      " |              or by topological order.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers\n",
      " |              where there is a mismatch in the number of weights,\n",
      " |              or a mismatch in the shape of the weight\n",
      " |              (only valid when `by_name`=True).\n",
      " |          reshape: Reshape weights to fit the layer when the correct number\n",
      " |              of weight arrays is present but their shape does not match.\n",
      " |      \n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  run_internal_graph(self, inputs, masks=None)\n",
      " |      Computes output tensors for new inputs.\n",
      " |      \n",
      " |      # Note:\n",
      " |          - Expects `inputs` to be a list (potentially with 1 element).\n",
      " |          - Can be run on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: List of tensors\n",
      " |          masks: List of masks (tensors or None).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Three lists: output_tensors, output_masks, output_shapes\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
      " |      Saves the model to a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model`\n",
      " |      is a compiled model ready to be used (unless the saved model\n",
      " |      was never compiled in the first place).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Dumps all layer weights to a HDF5 file.\n",
      " |      \n",
      " |      The weight file has:\n",
      " |          - `layer_names` (attribute), a list of strings\n",
      " |              (ordered names of model layers).\n",
      " |          - For every layer, a `group` named `layer.name`\n",
      " |              - For every such layer group, a group attribute `weight_names`,\n",
      " |                  a list of strings\n",
      " |                  (ordered names of weights tensor of the layer).\n",
      " |              - For every weight in the layer, a dataset\n",
      " |                  storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the model.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: A list of Numpy arrays with shapes and types matching\n",
      " |              the output of `model.get_weights()`.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |              It defaults to `print` (prints to stdout).\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A YAML string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.network.Network:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A model instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.network.Network:\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the model's input specs.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  losses\n",
      " |      Retrieves the model's losses.\n",
      " |      \n",
      " |      Will only include losses that are either\n",
      " |      unconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include losses that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of loss tensors.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |      Retrieves the model's updates.\n",
      " |      \n",
      " |      Will only include updates that are either\n",
      " |      unconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include updates that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  uses_learning_phase\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/udir/gwang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC_0012852.jpg\n",
      "ISIC_0010570.jpg\n",
      "ISIC_0014735.jpg\n",
      "ISIC_0010461.jpg\n",
      "ISIC_0012187.jpg\n",
      "ISIC_0013663.jpg\n",
      "ISIC_0009504.jpg\n",
      "ISIC_0000384.jpg\n",
      "ISIC_0010587.jpg\n",
      "ISIC_0014308.jpg\n",
      "ISIC_0012257.jpg\n",
      "ISIC_0013670.jpg\n",
      "ISIC_0015980.jpg\n",
      "ISIC_0010089.jpg\n",
      "ISIC_0006776.jpg\n",
      "ISIC_0010010.jpg\n",
      "ISIC_0015153.jpg\n",
      "ISIC_0015955.jpg\n",
      "ISIC_0011292.jpg\n",
      "ISIC_0013048.jpg\n",
      "ISIC_0009910.jpg\n",
      "ISIC_0016051.jpg\n",
      "ISIC_0014395.jpg\n",
      "ISIC_0011317.jpg\n",
      "ISIC_0014635.jpg\n",
      "ISIC_0006350.jpg\n",
      "ISIC_0011347.jpg\n",
      "ISIC_0013908.jpg\n",
      "ISIC_0014625.jpg\n",
      "ISIC_0000457.jpg\n",
      "ISIC_0013982.jpg\n",
      "ISIC_0013493.jpg\n",
      "ISIC_0010047.jpg\n",
      "ISIC_0013736.jpg\n",
      "ISIC_0000524.jpg\n",
      "ISIC_0011327.jpg\n",
      "ISIC_0015276.jpg\n",
      "ISIC_0013925.jpg\n",
      "ISIC_0014957.jpg\n",
      "ISIC_0015526.jpg\n",
      "ISIC_0013738.jpg\n",
      "ISIC_0000494.jpg\n",
      "ISIC_0000488.jpg\n",
      "ISIC_0012836.jpg\n",
      "ISIC_0015440.jpg\n",
      "ISIC_0014961.jpg\n",
      "ISIC_0001247.jpg\n",
      "ISIC_0000537.jpg\n",
      "ISIC_0000326.jpg\n",
      "ISIC_0012901.jpg\n",
      "ISIC_0014422.jpg\n",
      "ISIC_0000208.jpg\n",
      "ISIC_0000105.jpg\n",
      "ISIC_0012285.jpg\n",
      "ISIC_0015997.jpg\n",
      "ISIC_0014863.jpg\n",
      "ISIC_0013226.jpg\n",
      "ISIC_0010028.jpg\n",
      "ISIC_0000037.jpg\n",
      "ISIC_0014829.jpg\n",
      "ISIC_0000189.jpg\n",
      "ISIC_0015212.jpg\n",
      "ISIC_0011304.jpg\n",
      "ISIC_0013637.jpg\n",
      "ISIC_0013203.jpg\n",
      "ISIC_0014324.jpg\n",
      "ISIC_0012260.jpg\n",
      "ISIC_0000530.jpg\n",
      "ISIC_0014619.jpg\n",
      "ISIC_0013558.jpg\n",
      "ISIC_0000322.jpg\n",
      "ISIC_0000138.jpg\n",
      "ISIC_0010457.jpg\n",
      "ISIC_0013961.jpg\n",
      "ISIC_0015631.jpg\n",
      "ISIC_0015401.jpg\n",
      "ISIC_0014478.jpg\n",
      "ISIC_0012109.jpg\n",
      "ISIC_0013399.jpg\n",
      "ISIC_0001204.jpg\n",
      "ISIC_0010598.jpg\n",
      "ISIC_0010320.jpg\n",
      "ISIC_0013032.jpg\n",
      "ISIC_0010041.jpg\n",
      "ISIC_0000361.jpg\n",
      "ISIC_0012810.jpg\n",
      "ISIC_0016027.jpg\n",
      "ISIC_0000552.jpg\n",
      "ISIC_0010022.jpg\n",
      "ISIC_0014720.jpg\n",
      "ISIC_0015171.jpg\n",
      "ISIC_0012699.jpg\n",
      "ISIC_0012945.jpg\n",
      "ISIC_0015443.jpg\n",
      "ISIC_0015395.jpg\n",
      "ISIC_0000341.jpg\n",
      "ISIC_0013191.jpg\n",
      "ISIC_0014780.jpg\n",
      "ISIC_0012653.jpg\n",
      "ISIC_0004168.jpg\n",
      "ISIC_0013045.jpg\n",
      "ISIC_0000223.jpg\n",
      "ISIC_0015163.jpg\n",
      "ISIC_0012213.jpg\n",
      "ISIC_0010488.jpg\n",
      "ISIC_0013223.jpg\n",
      "ISIC_0012678.jpg\n",
      "ISIC_0013427.jpg\n",
      "ISIC_0009955.jpg\n",
      "ISIC_0009934.jpg\n",
      "ISIC_0009974.jpg\n",
      "ISIC_0010086.jpg\n",
      "ISIC_0013972.jpg\n",
      "ISIC_0013395.jpg\n",
      "ISIC_0000314.jpg\n",
      "ISIC_0000275.jpg\n",
      "ISIC_0014249.jpg\n",
      "ISIC_0010090.jpg\n",
      "ISIC_0013777.jpg\n",
      "ISIC_0000316.jpg\n",
      "ISIC_0008626.jpg\n",
      "ISIC_0013688.jpg\n",
      "ISIC_0012095.jpg\n",
      "ISIC_0010476.jpg\n",
      "ISIC_0009929.jpg\n",
      "ISIC_0014511.jpg\n",
      "ISIC_0000999.jpg\n",
      "ISIC_0014769.jpg\n",
      "ISIC_0000226.jpg\n",
      "ISIC_0005247.jpg\n",
      "ISIC_0005548.jpg\n",
      "ISIC_0010018.jpg\n",
      "ISIC_0013554.jpg\n",
      "ISIC_0000188.jpg\n",
      "ISIC_0012314.jpg\n",
      "ISIC_0015973.jpg\n",
      "ISIC_0000145.jpg\n",
      "ISIC_0012320.jpg\n",
      "ISIC_0012744.jpg\n",
      "ISIC_0000042.jpg\n",
      "ISIC_0014583.jpg\n",
      "ISIC_0014044.jpg\n",
      "ISIC_0000043.jpg\n",
      "ISIC_0009564.jpg\n",
      "ISIC_0014574.jpg\n",
      "ISIC_0011207.jpg\n",
      "ISIC_0015005.jpg\n",
      "ISIC_0010062.jpg\n",
      "ISIC_0009904.jpg\n",
      "ISIC_0014585.jpg\n",
      "ISIC_0015232.jpg\n",
      "ISIC_0009909.jpg\n",
      "ISIC_0010176.jpg\n",
      "ISIC_0013996.jpg\n",
      "ISIC_0000071.jpg\n",
      "ISIC_0000395.jpg\n",
      "ISIC_0014628.jpg\n",
      "ISIC_0015991.jpg\n",
      "ISIC_0014640.jpg\n",
      "ISIC_0000127.jpg\n",
      "ISIC_0011084.jpg\n",
      "ISIC_0016004.jpg\n",
      "ISIC_0013888.jpg\n",
      "ISIC_0013181.jpg\n",
      "ISIC_0009298.jpg\n",
      "ISIC_0014716.jpg\n",
      "ISIC_0013789.jpg\n",
      "ISIC_0000882.jpg\n",
      "ISIC_0009942.jpg\n",
      "ISIC_0013749.jpg\n",
      "ISIC_0010073.jpg\n",
      "ISIC_0012756.jpg\n",
      "ISIC_0015019.jpg\n",
      "ISIC_0000089.jpg\n",
      "ISIC_0014506.jpg\n",
      "ISIC_0010851.jpg\n",
      "ISIC_0001367.jpg\n",
      "ISIC_0014609.jpg\n",
      "ISIC_0008807.jpg\n",
      "ISIC_0000253.jpg\n",
      "ISIC_0013579.jpg\n",
      "ISIC_0000206.jpg\n",
      "ISIC_0014685.jpg\n",
      "ISIC_0016061.jpg\n",
      "ISIC_0009914.jpg\n",
      "ISIC_0014912.jpg\n",
      "ISIC_0009966.jpg\n",
      "ISIC_0001213.jpg\n",
      "ISIC_0013696.jpg\n",
      "ISIC_0009877.jpg\n",
      "ISIC_0009936.jpg\n",
      "ISIC_0000330.jpg\n",
      "ISIC_0015999.jpg\n",
      "ISIC_0010554.jpg\n",
      "ISIC_0015311.jpg\n",
      "ISIC_0000555.jpg\n",
      "ISIC_0000242.jpg\n",
      "ISIC_0014576.jpg\n",
      "ISIC_0013159.jpg\n",
      "ISIC_0015256.jpg\n",
      "ISIC_0010077.jpg\n",
      "ISIC_0001184.jpg\n",
      "ISIC_0000535.jpg\n",
      "ISIC_0014835.jpg\n",
      "ISIC_0011362.jpg\n",
      "ISIC_0014687.jpg\n",
      "ISIC_0010059.jpg\n",
      "ISIC_0010262.jpg\n",
      "ISIC_0000448.jpg\n",
      "ISIC_0011218.jpg\n",
      "ISIC_0009901.jpg\n",
      "ISIC_0012758.jpg\n",
      "ISIC_0011155.jpg\n",
      "ISIC_0001152.jpg\n",
      "ISIC_0012425.jpg\n",
      "ISIC_0012246.jpg\n",
      "ISIC_0013193.jpg\n",
      "ISIC_0001296.jpg\n",
      "ISIC_0001134.jpg\n",
      "ISIC_0000201.jpg\n",
      "ISIC_0010017.jpg\n",
      "ISIC_0009964.jpg\n",
      "ISIC_0010590.jpg\n",
      "ISIC_0015418.jpg\n",
      "ISIC_0014486.jpg\n",
      "ISIC_0001769.jpg\n",
      "ISIC_0013213.jpg\n",
      "ISIC_0014372.jpg\n",
      "ISIC_0012715.jpg\n",
      "ISIC_0015645.jpg\n",
      "ISIC_0014441.jpg\n",
      "ISIC_0011357.jpg\n",
      "ISIC_0014722.jpg\n",
      "ISIC_0016037.jpg\n",
      "ISIC_0014397.jpg\n",
      "ISIC_0012549.jpg\n",
      "ISIC_0014779.jpg\n",
      "ISIC_0000413.jpg\n",
      "ISIC_0000217.jpg\n",
      "ISIC_0013523.jpg\n",
      "ISIC_0014868.jpg\n",
      "ISIC_0012789.jpg\n",
      "ISIC_0012282.jpg\n",
      "ISIC_0012245.jpg\n",
      "ISIC_0010184.jpg\n",
      "ISIC_0013319.jpg\n",
      "ISIC_0012713.jpg\n",
      "ISIC_0010064.jpg\n",
      "ISIC_0012701.jpg\n",
      "ISIC_0000499.jpg\n",
      "ISIC_0012889.jpg\n",
      "ISIC_0015950.jpg\n",
      "ISIC_0012520.jpg\n",
      "ISIC_0015136.jpg\n",
      "ISIC_0010360.jpg\n",
      "ISIC_0001131.jpg\n",
      "ISIC_0011104.jpg\n",
      "ISIC_0011132.jpg\n",
      "ISIC_0013844.jpg\n",
      "ISIC_0014163.jpg\n",
      "ISIC_0013035.jpg\n",
      "ISIC_0009078.jpg\n",
      "ISIC_0012092.jpg\n",
      "ISIC_0015208.jpg\n",
      "ISIC_0012969.jpg\n",
      "ISIC_0013287.jpg\n",
      "ISIC_0014393.jpg\n",
      "ISIC_0016016.jpg\n",
      "ISIC_0015009.jpg\n",
      "ISIC_0015184.jpg\n",
      "ISIC_0012981.jpg\n",
      "ISIC_0015173.jpg\n",
      "ISIC_0012391.jpg\n",
      "ISIC_0012448.jpg\n",
      "ISIC_0014947.jpg\n",
      "ISIC_0009945.jpg\n",
      "ISIC_0010182.jpg\n",
      "ISIC_0014032.jpg\n",
      "ISIC_0000149.jpg\n",
      "ISIC_0012099.jpg\n",
      "ISIC_0000209.jpg\n",
      "ISIC_0015245.jpg\n",
      "ISIC_0004715.jpg\n",
      "ISIC_0016035.jpg\n",
      "ISIC_0011118.jpg\n",
      "ISIC_0008992.jpg\n",
      "ISIC_0000319.jpg\n",
      "ISIC_0011167.jpg\n",
      "ISIC_0001119.jpg\n",
      "ISIC_0014541.jpg\n",
      "ISIC_0013998.jpg\n",
      "ISIC_0000425.jpg\n",
      "ISIC_0000518.jpg\n",
      "ISIC_0009915.jpg\n",
      "ISIC_0013335.jpg\n",
      "ISIC_0013490.jpg\n",
      "ISIC_0012357.jpg\n",
      "ISIC_0000027.jpg\n",
      "ISIC_0013494.jpg\n",
      "ISIC_0010173.jpg\n",
      "ISIC_0013364.jpg\n",
      "ISIC_0000023.jpg\n",
      "ISIC_0016060.jpg\n",
      "ISIC_0013690.jpg\n",
      "ISIC_0001449.jpg\n",
      "ISIC_0012086.jpg\n",
      "ISIC_0014694.jpg\n",
      "ISIC_0014715.jpg\n",
      "ISIC_0013474.jpg\n",
      "ISIC_0000016.jpg\n",
      "ISIC_0014748.jpg\n",
      "ISIC_0000245.jpg\n",
      "ISIC_0011109.jpg\n",
      "ISIC_0011129.jpg\n",
      "ISIC_0013615.jpg\n",
      "ISIC_0014651.jpg\n",
      "ISIC_0013271.jpg\n",
      "ISIC_0011326.jpg\n",
      "ISIC_0008256.jpg\n",
      "ISIC_0010042.jpg\n",
      "ISIC_0000202.jpg\n",
      "ISIC_0013288.jpg\n",
      "ISIC_0012325.jpg\n",
      "ISIC_0000234.jpg\n",
      "ISIC_0012660.jpg\n",
      "ISIC_0000264.jpg\n",
      "ISIC_0014929.jpg\n",
      "ISIC_0000123.jpg\n",
      "ISIC_0014765.jpg\n",
      "ISIC_0009860.jpg\n",
      "ISIC_0014745.jpg\n",
      "ISIC_0008600.jpg\n",
      "ISIC_0014994.jpg\n",
      "ISIC_0007788.jpg\n",
      "ISIC_0000019.jpg\n",
      "ISIC_0013321.jpg\n",
      "ISIC_0016012.jpg\n",
      "ISIC_0011385.jpg\n",
      "ISIC_0014772.jpg\n",
      "ISIC_0010380.jpg\n",
      "ISIC_0013134.jpg\n",
      "ISIC_0000192.jpg\n",
      "ISIC_0014771.jpg\n",
      "ISIC_0013863.jpg\n",
      "ISIC_0014833.jpg\n",
      "ISIC_0014783.jpg\n",
      "ISIC_0000120.jpg\n",
      "ISIC_0008998.jpg\n",
      "ISIC_0010364.jpg\n",
      "ISIC_0009583.jpg\n",
      "ISIC_0014987.jpg\n",
      "ISIC_0000474.jpg\n",
      "ISIC_0015218.jpg\n",
      "ISIC_0014903.jpg\n",
      "ISIC_0001427.jpg\n",
      "ISIC_0015002.jpg\n",
      "ISIC_0000383.jpg\n",
      "ISIC_0013304.jpg\n",
      "ISIC_0014942.jpg\n",
      "ISIC_0010252.jpg\n",
      "ISIC_0010327.jpg\n",
      "ISIC_0011137.jpg\n",
      "ISIC_0012334.jpg\n",
      "ISIC_0012665.jpg\n",
      "ISIC_0011085.jpg\n",
      "ISIC_0014762.jpg\n",
      "ISIC_0014928.jpg\n",
      "ISIC_0000098.jpg\n",
      "ISIC_0012666.jpg\n",
      "ISIC_0000009.jpg\n",
      "ISIC_0000504.jpg\n",
      "ISIC_0000247.jpg\n",
      "ISIC_0014092.jpg\n",
      "ISIC_0014336.jpg\n",
      "ISIC_0011114.jpg\n",
      "ISIC_0011131.jpg\n",
      "ISIC_0000299.jpg\n",
      "ISIC_0012746.jpg\n",
      "ISIC_0000092.jpg\n",
      "ISIC_0010232.jpg\n",
      "ISIC_0015476.jpg\n",
      "ISIC_0010019.jpg\n",
      "ISIC_0007141.jpg\n",
      "ISIC_0007322.jpg\n",
      "ISIC_0012806.jpg\n",
      "ISIC_0010862.jpg\n",
      "ISIC_0013567.jpg\n",
      "ISIC_0012447.jpg\n",
      "ISIC_0010337.jpg\n",
      "ISIC_0000032.jpg\n",
      "ISIC_0000151.jpg\n",
      "ISIC_0013472.jpg\n",
      "ISIC_0000470.jpg\n",
      "ISIC_0010465.jpg\n",
      "ISIC_0013516.jpg\n",
      "ISIC_0012494.jpg\n",
      "ISIC_0000546.jpg\n",
      "ISIC_0013212.jpg\n",
      "ISIC_0000134.jpg\n",
      "ISIC_0000197.jpg\n",
      "ISIC_0000556.jpg\n",
      "ISIC_0015357.jpg\n",
      "ISIC_0014725.jpg\n",
      "ISIC_0001372.jpg\n",
      "ISIC_0013027.jpg\n",
      "ISIC_0015969.jpg\n",
      "ISIC_0000292.jpg\n",
      "ISIC_0014502.jpg\n",
      "ISIC_0012898.jpg\n",
      "ISIC_0003462.jpg\n",
      "ISIC_0000013.jpg\n",
      "ISIC_0012369.jpg\n",
      "ISIC_0013096.jpg\n",
      "ISIC_0014753.jpg\n",
      "ISIC_0014798.jpg\n",
      "ISIC_0014603.jpg\n",
      "ISIC_0000323.jpg\n",
      "ISIC_0015481.jpg\n",
      "ISIC_0002438.jpg\n",
      "ISIC_0014643.jpg\n",
      "ISIC_0010332.jpg\n",
      "ISIC_0013897.jpg\n",
      "ISIC_0000230.jpg\n",
      "ISIC_0011117.jpg\n",
      "ISIC_0000508.jpg\n",
      "ISIC_0000055.jpg\n",
      "ISIC_0015018.jpg\n",
      "ISIC_0015985.jpg\n",
      "ISIC_0013796.jpg\n",
      "ISIC_0012177.jpg\n",
      "ISIC_0012523.jpg\n",
      "ISIC_0010005.jpg\n",
      "ISIC_0012876.jpg\n",
      "ISIC_0013639.jpg\n",
      "ISIC_0000444.jpg\n",
      "ISIC_0013691.jpg\n",
      "ISIC_0013311.jpg\n",
      "ISIC_0012313.jpg\n",
      "ISIC_0014800.jpg\n",
      "ISIC_0015219.jpg\n",
      "ISIC_0014974.jpg\n",
      "ISIC_0010044.jpg\n",
      "ISIC_0010492.jpg\n",
      "ISIC_0015044.jpg\n",
      "ISIC_0012720.jpg\n",
      "ISIC_0010036.jpg\n",
      "ISIC_0013369.jpg\n",
      "ISIC_0013291.jpg\n",
      "ISIC_0000503.jpg\n",
      "ISIC_0010350.jpg\n",
      "ISIC_0010557.jpg\n",
      "ISIC_0010487.jpg\n",
      "ISIC_0012904.jpg\n",
      "ISIC_0010858.jpg\n",
      "ISIC_0013617.jpg\n",
      "ISIC_0011158.jpg\n",
      "ISIC_0002206.jpg\n",
      "ISIC_0000182.jpg\n",
      "ISIC_0011296.jpg\n",
      "ISIC_0013721.jpg\n",
      "ISIC_0000087.jpg\n",
      "ISIC_0007332.jpg\n",
      "ISIC_0009941.jpg\n",
      "ISIC_0010213.jpg\n",
      "ISIC_0000353.jpg\n",
      "ISIC_0014327.jpg\n",
      "ISIC_0000215.jpg\n",
      "ISIC_0013244.jpg\n",
      "ISIC_0015279.jpg\n",
      "ISIC_0013842.jpg\n",
      "ISIC_0011159.jpg\n",
      "ISIC_0013600.jpg\n",
      "ISIC_0014702.jpg\n",
      "ISIC_0000268.jpg\n",
      "ISIC_0010244.jpg\n",
      "ISIC_0000339.jpg\n",
      "ISIC_0013109.jpg\n",
      "ISIC_0010596.jpg\n",
      "ISIC_0010102.jpg\n",
      "ISIC_0013089.jpg\n",
      "ISIC_0013356.jpg\n",
      "ISIC_0011130.jpg\n",
      "ISIC_0014195.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC_0008116.jpg\n",
      "ISIC_0013082.jpg\n",
      "ISIC_0014049.jpg\n",
      "ISIC_0012155.jpg\n",
      "ISIC_0010848.jpg\n",
      "ISIC_0013580.jpg\n",
      "ISIC_0015133.jpg\n",
      "ISIC_0012891.jpg\n",
      "ISIC_0013094.jpg\n",
      "ISIC_0011202.jpg\n",
      "ISIC_0013075.jpg\n",
      "ISIC_0015566.jpg\n",
      "ISIC_0000203.jpg\n",
      "ISIC_0016022.jpg\n",
      "ISIC_0012690.jpg\n",
      "ISIC_0014558.jpg\n",
      "ISIC_0013488.jpg\n",
      "ISIC_0013799.jpg\n",
      "ISIC_0000249.jpg\n",
      "ISIC_0014110.jpg\n",
      "ISIC_0013216.jpg\n",
      "ISIC_0014726.jpg\n",
      "ISIC_0015455.jpg\n",
      "ISIC_0014171.jpg\n",
      "ISIC_0010568.jpg\n",
      "ISIC_0015040.jpg\n",
      "ISIC_0013865.jpg\n",
      "ISIC_0015118.jpg\n",
      "ISIC_0015157.jpg\n",
      "ISIC_0014740.jpg\n",
      "ISIC_0011324.jpg\n",
      "ISIC_0010592.jpg\n",
      "ISIC_0014696.jpg\n",
      "ISIC_0010480.jpg\n",
      "ISIC_0012118.jpg\n",
      "ISIC_0012379.jpg\n",
      "ISIC_0012228.jpg\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsvc+PbVmW3/VZe59f996I9yszK7Oq23YjbCwhIU+QPUBC/BCImUcgYMIAqUee4zEj/wv0wBITBEwsGFgYhIRgAvQAGBi1cdO229X1I6sq870XEffH2WfvxWDttc+5EZFdmd1d5pX0thSK9+Kec+45++y9fnzXd60lqsrH8XF8HB/HdoT/v2/g4/g4Po4Pb3wUDB/Hx/FxPBkfBcPH8XF8HE/GR8HwcXwcH8eT8VEwfBwfx8fxZHwUDB/Hx/FxPBm/MsEgIv+OiPwDEfl9Efmbv6rv+Tg+jo/jz37Ir4LHICIR+H+Afwv4IfC7wH+gqv/3n/mXfRwfx8fxZz5+VRbDXwV+X1X/QFVn4L8E/vqv6Ls+jo/j4/gzHt2v6Lq/AfzTzf9/CPy1bzp46Pa60z1ayjdeUEKAEKCLaB8pETQKKkD9UfGD1/NUgLD+JiiIIgLivzGrqWj9V72QyGpNiQDo9tJ2/fp1QRQRJdRzBCXIetTj8/xcRSj+ffj9+Kft26+usf2Ox6PU6ymCKvW3tGfzew1SCJtnf+5ewuaZfS60Xsev6/9+7l5UxebY5wclyvqOtZ7l19mO7TnbGRT/19W9y+Y8bc/+3NDN7G7vf3v+9l35/V/Nsa5/2c43er1mtsc9uYpu5rzUuWynSl1Y+uj/fufSroECRQgXiAnCXJB5gZzbfhIR9DBxd/+jn6vqZ8/PzPX4VQmGb9oH6wEivw38NsAUbvir/BubiXj+bJEOGXaEly/QlzdcvrghD4EyCHkQ8mi/l52gETRA6WHZQx6V5aDoPtMdErv9hT5m+li4HS8ElNPSc38ZOM89S4qEWOi6Qh8zIRRiUIaY6WMGIJfQXv6r6cSb8YFX/YkxLOzjzD7MAAQpRGwz9mLnFg1khEvpOZaBVCJBlCkkesn0kgl1ExVdDbteMmNIHMKl/n+5OuZ92XEpPWftuJSepJFjHvinp9ccl4GA8mI48f3pPW+6B27iGaAeN3KfR5JGxrC0e9mHmTEkIoVMaNc913u/lK4+pxKxe84EHpYRgF2c2ceZ190Dn3V37ZhZI2cd2v36cwQpTLJwG0/s63P682UCkdL+7qNsjN9c59bmOJDrO4qiZBWSdva8ZWzPUeqSHWWhDzankyz0shBEGeo8z/Vc/7mUnnPpeb9MnPJAUaEL9o7n0vGwDMy529ynCZKUIwXhmHp+8f5AunSUS4S81XDYniiCZIGC6wiTFVERFcJZePkPhBd/uLD/vZ9SfvIlJV+u9lOML/h7/O1/8mRffcP4VQmGHwJ/bvP/3wR+tD1AVX8H+B2Al/1nyjcbC1dDRCAGtAvkKaAiaBA0CqUTSgdlMKGgEfIAy14pg6J9IewWdvsL37+94+V4YoqJV/2JooFLfZH/6P0b3h8nhi4z9gtDzIzdQpTCvpuZoi2WRQNzjsyl4/u797wZHnjTPbAPM/tw4RAubXMDDJLpZSGiZISsgaQdD2XgruzoJXPw8ygMkpk1krSz86qAmSSxDxcGCr0UAkpByAjH0nNW+3koIw9l4KwDL7sTX6c9SSM38cL3hvf8uf4rPon3gG3So468zXvu8o4xJF6EE1NITLL+AMzYpnooo31X6YlSCBRiXYxZha/yDZfSA/AyHvmsu+PP9b8gVh2RNDJjG/Ou7MgbfTJJ4kU48yYe23wVtWeMKKPkq+8C2ubOCGkjHLbvwAXMTORt3jchMWskitLLwiC5zb89U+Egs70zQnv2pLE9/1fhhvs8kquA2oeZpJFT7jmVod3jUiKX0nHOHefcc1k68hIpS4AlmAAAVNQsXK2WSBIk0wSDBkxgKMhi54RUfG892Te6LN9ug9XxqxIMvwv8JRH554A/Av594D/8xqNDREKPLolvsgHD7S3hk9fkT19w+mLP5WXg9FmwCZJVCORJmb8/I30hdoWuz3xyOHE7Xng9Hnk9HLmJFz7pH3jZHeklN01Yqjb5F29/zD89vwZgDEvV0iYMYtX6rtGLBpJGAF7GE59177mNJw4yt8179Ryi9HUD+WK5aGSuC2qSzEEWAhAFssJZQ9sEPvaiTCL0EohbbclM0jNJlXM9tyCcp8iDDtyViYhyG068ChdeVu2W6uXPGnhXRnrJ3IZEj9ILDCKMYt9TVEk8MKuSlPaEjwGrd6XnoQqpXjIHmXkTZg5B6rGpfrfyVXlP2lhGZn0on8XCKIEIZApJ7du2z51RippdnZthminArEoEolwbsVmVYxWKdrQ0ARSvjVsC0FeFnRVSfOCssf70FA2876crYXkbTu38QiBr4Kz9lZVREN4tO35XhfvzyOnck1OkzNGsgmiupRahTMWsia4gQQmdMo6JGAvLEvnqdsfps5GXn/+A1//nBD/8Cfn+AYq9XxlHeOBbj1+JYFDVRUT+BvD3gAj8bVX9+3/iC4oQbg6Umz3p5cjlVWC+EZbd5jur21AGkL7QjwuH3YXdkPh8fwfAoZsZw8IYXPuWZtYCTdrfxDOv+hNLqaZo/UELEAio/VQhEaQ0zThrNFN2sxDDMy5SRNtmL9WHzAg9hV6gb+faET5ssQcQJSLEqiUDgShCD/QoSTJRC1ELSSFgG22KiaKhCrf1un293SiFRLL7qEJhEmGSSKjzU6SAGkYxiG2yjL1ou4Zr8ESvhbMuzRrwzboKNSFSmCQ3qwDMP4+izKoEClSh5M9bVEHsd8YMzn4zHy4g/J5sjuz8jBJE2GP3XYDRcSa0CbhydV4d9Tt7Cqkeaa5PIklnioNc58GuOZA401NKaBZLQUglspTY8BQw3IMsUARUzXLwmwgVg4pKiJlpSAxdJveBy6EnvYw8fD8w/eIF+/cPyOWCXjJ/kvGrshhQ1b8L/N1veTBQvtFaQAJ62JFfTqTbyLIzPKGMUKIantCBBnMZpv3Mze7Cp/sHXgxnvpjeN6vA/OWlgm+r6XjW3oRE3SClf8+5+tJuEWytBff1/bq9uwnfIASAK1N5q117KU0oRFHb3CJtEYNWjWma3b/jvNWeIm3jhmpuFrQBVr0UJi2kamY/N2L9Ocjy6G/X2jYQ6MWEDUCiEDfP5MfvRW1etJBVmruTtKFmIIEgUjff6hJFlKSBu6IkMcHRV4Hj85JUyds1I1xtahdYg6xCabvxbY5t+HUGkXaevyef+ytBIYYeuqsySGaq7gPtmibccrUYejEBGZCqWOw7+2gu67xEcopoMFyBpT6Qg46xPqtC32d2Q+Kmn1k08H6aSIeedBM5f9Kxu90T3o/kS8Viuu+21X9lguG7DUXLNwkFIQw9+c0Nxy9GTp8E0q2w7GF+WcyVCIr2LlmVaUjs+8SL4czL/sT3+juiFI7FwDff2ED1K03ru78JZjX0kjmWgVJMulNfMIF2PkCkmLlIaH7noNmuVTepL4KiQhHBbI919BSO2hFQchUOdm1h8H1ZtWRkhWTOWgjVUvBRKNV6CGTNm41QLRVy23xbR8dN7t4FmVZjXwsZZXoEKa+Cy7S1j1A3omlwEyCpftdcff+z0jZ8FCHWecpNeMR6XMdRMntZmKS0Z/CNmrfLRrV+fr2Rtxvdn9PmwwSSHxfr/6nCwcfj79rO2ayxWj1mkfWSG0C6BT7tOu6KGZ51LAMPeSDlyLxE0tw1rIECkg1Eb8JBsZiLmGWRS2AukS4U+j4zj5nlELi8EJY3B/qvd3D/AFqQaeS7jA9DMHxTMEIEiRHZ7bh8MnF+E5hfCec3ShmVcrs0Keq+GArz0nFeMnOOlC5wLANTSHVzQwlCr4uBWSrEUEADpVoMWc28NavClsGldA2tj1JIGglartBwH6X6k5Om+hwQVa8sh7niBn1dUH7MRSODFgahacheq2lcNZwv9FJ/Lloomugl0Is9Y35kfbkm7Ov9BFUSZq62J5Cn5yWtlgoKZCLSNn652jxuuj8XkNr46CjHGsVIBBKZCeWhhCuhkLQz4JVITyaFMymYazKIRzXWuS9VCyfWed4+l82J4TZ2n37e+sxZ1ePSVy7IOg8rDmHHKIXqHlQrK4q5b8HBWBcGBFP6bQ0ElhJJJTLnaG5rDtWNMKHgoKIGbeFLEQjBQuMpB6BfQ6RR0aDkXeDyeqB7eYt8/RaKovvp2ffyTePDEAxo9d8fDQnIOCIvb7m8iswvzFLIh4LuM9KXq/g62X4u554QCl9f9gRRdnGmRLny9T2U9ZzpD1Qw0n5SidznkVPuuelmigqhW89rWAM1VCZmWcwa7bONhm7fX3EF//4t8GURC6Wg9BKatojt+2huV1I4V9DtrIV90Gf97PNGAJhWtGskhLmavL7Rt6JuruBlViHJwr6a0PAU0Av+nZspted4pNnBwpMKSQJHf+YaRUja8b5MDfGfQiIRmTlbdISnfnNRYSYQ1dwmd8siihujod6H80vSxpJoFtwjweir0oXQipVsLQEB7QgmJpoyGSSD5LYWIoVLBSCXEjjlnvs0ck4dKXUGOlaBIIsQZkEXQbOiUdEiaFTCLjWr4TSbrRhCoZ8W0j4y3wbOryPDF7f0Pz+gc2J5c3gyZ3/c+DAEw3N7M0TC0BM++4T5N99w/5uB+YWSdzZJFEHn6uWq+WPhHIgnga86HsaJu8MNP3p54d2nOz6b7vn+9I5dTBzz0L5mC0C66ZcqB+CsHe+WHT+9vOBnlxtbfKXjtrMtegldi/dHWZr5mOoiKKwRi4HMXM3g8OiBPQLxqvIeHAjMmIaPxua5mq8oK/7gWr0AqWQmN4lxLQ0/ywMZYZJs/jpKFDgX4agdWYUBw122nw8UHrTjWHreMtWwZW5Crq8hVAdMo4hhG7igcJfEsI1eCmMFGh904H2eKIQWxgV4XyZ+trzgWEYTllnZhwvneOQ2nMyfl8RUOQYexjyWsd6TRZIGXZ/HJ27ralxqRLC5HFUwt2fBBG9CGtC4DZn6d81lJBEI9Z1PkogVkLQ1pkxhoCxmSfooCHdp5HzuKTlCEiQFQjKhEM/Swu5lEEoVEPkQGMfU7mdZbI1JUOSwML8JPMyBPEx89u4L5DRz+uLX0mJ4NESQIMg0WiTiRU+eIO+VMtW3mQXJdfln2kR2D0LIUDohnYS5CL/Y7RnjwqfjPaPaYkolcqYnyMQUUnups5rvd8wjF+14v+x4yIOZeiUwh8hJevqQCaKMmAm5jZVnNS2fEYIGswJkdToccXegrRfT5uMjM9cebY0+pI1VFaprEDGcanverMpQhUOuIUUXUOc2x/lKOxYNnAn05KZtURNSE5kkgfd5IkkkSaphXgMI92Fp+MeVxq2uT2rPIlV4GKo/aCZJbIph3gB3sblwFupspCLiRpDLs0rFwb4ahF6fB9vwDuLmaoq5FeBgYV1SG4xijZTM1ZLMVSCCuQnnGpVyvKpng2NRmJAr4LoLhUOc6UM270W0EZvUmboC2mnl5qiFLHvDE8bOuDVZF+7PYxMOANoreYJ0K+TDgAyRy8vvlv3wYQoGgIotpFcT6Sa2ydKo5mstAZltIqWYYAhJCAniDCECCHkKnOee09I3PgLUcJFGUul4KGPT9sbk6xsZ6P0yclyM0SaizKVj0ExWueI1wMr8i1La4lzBxzWM5cMtBzdvt+E+93s9LNfOEaHoavb7sY8dsa2/3P6GkDU2oAyp5raa4EoaQSOlavTG28UsA7eAsoZGtuplIahp5VDvtSH5T0C/dRdbWDbTk1cCE6H9Nsbl5Qrld7KXCdpEeGTOO6+kVNM9SSSqMsXMgHFDUp3vApUEtd5TlOtQpR/n916qqzMTibpaRn5vDkDHGqUYyE3o2DXK1b+DFLpQ/+ZU9OpGSDHZHfw9OJEvCEuKnLuOrjJwS7G/KaA5QFRj+u6F86cD/X3m8vJ57OebxocnGCrgGMYRvdmT9x3LaJohzAIhGBU0CfGymfSLEGYIi00oigmJi7AsgfPScSoDO01m+kPDDvpgZqxTfN8vE19ebgG4T2OjrxY1TvscIqfQs4uJffD4erEYvIS2AJJ2DdB0J3YgE2UVTh4ZKDwFvB7/LTza6NEXjX/OaiZnuNLeUw2VuebdugK9WDgxsUYFQjWZh/qZCwG0W0O7QK+RgyTbADXG3zAMrhH8xy6UmfgzCDzoUKnMkV4WbkOhl6V9lwvfJmxZcRm4po07aOlCZa8XE1wtvCqc62dX2I9Cv8GcUsVW3LLbgqMFsxqyunVi1qZ/v1GpzUosbjVu0JtQ82qmmAjBIlFtiAKV6VggFIGiUMzVzUPhIj19zHSxWIRijva6F9srGmHZKcfPIrto2Nx3GR+GYBBo3p4EpOuQw558s2PZByMyGacHSfbgTh1tQ9ffks0sk+pizJeO09xzvwzswlzpu1qR4cC7ZU+Uwrn0nHLPV/OBt5dd81/BzMggSgylWgpuRj7erI5GV3NXA9DZoihQJNQFY+b0dmxJQnaugYoT4YlQcBdjewUXJFfCoY5RchNEc9XMVJ8/VJ5Dxkx2D6luCUeGT6T6PJFzGczcF1qeg5F+TNg8BvG2m9iHfy/Q3IpS4/0WpSkcKz08Xllj8ux1t8LB6eYARxlrWHQhqWEdV1iBXFszULEFDVfCzIRCZ9cVt0Bi5SZsrYEVUI4os6zuyDYHJmIWQ0vUEkUFRE3IaoBq2BEWsc+TWTIStCZeCSFopQJJSygsvSJFuLw2lzvvvin09/z4QARDQEK0zbzbEV7cUl6/YP5kIu0raedim7x0mFVRLELkrllY1h+bd7WQTwY9ddyPIz8+vuSce267C2NcjHmmgXSJzB7KVOGce+7m0XzhaFjCeelIOXIYZrpQSGVhKbEBma6dxpC4DefqR+e2AXPDrBOTJgqFIo8jFXWoNpPX4vvFwoSPpi1zTfBpBCODrJsvXWCDGZTGF5gJhGo9uB+eKMwEzqVjZgVOtyNKMUuojrP2Jjgq3nKu13Ras28ut7p8QwbMBckq7EOi1+voTeZsG6tqaDCwz4HHqbI3kwbORN6Xqd5buOKknLXnq7znKKm5Gy0CJIVSPHErM1DYUmoSpu3P1dJyIZS0q0KyRh+Ch6RNeDkNvJfCROZcqfeDLOzDzDv2OA1/7G3e8hjRLGiNBJXB1rAvEclCWGz/d13FL4Il+UmNWtiBBlKWKFxeKxqEdPMtk5Hq+EAEgyAxoBlk6GEc0KmDYBPRPxi7sfQmGDRivmzaXKIKhJAUeTwHNf67lMCcOx5Eq9Ywgsh9GsmlugShkIppTaGi0BV4XHLgYR4YQmaOlgzj0t+zI8ESgHo1reeJOEBLNPLhYBysQsDdCg/vOf7gQOM2RPiYocfm8y1OATS2oycX+SI3arZdxa2HrMJRex50IGlnzyNLM5ldE2+fg83fnOBjocPchISPUs3zUEOkTtdGyobYZZtxklTFaWzf2zZcve/NMmiZldv07rmRpfoWKbD3VsiOYWjgDO2aWWXFgFgzNa+eeSOszErUFo3IGq5cNr/vbVTCrUoRNW5CLGiIhqMpqLO+VNY1XY0xo1eXZtGFrpCXalZXQE5y/W+se+Y7jA9DMASBvkdCRnY7ys1EGTszq7Lx2uPZ/i3DKhzccvRJ02DRCFG1bMvBUF2CTb5uWGhLtRbm3HHJXRMMPdlebmdS3MGhVAK5Is9LNUXtp5ql4rkUNOsA4HGWnrPjthvFw5WBNZlpS79NbjlAswTgKSaxvd4TMJLrjEM3i2EVCLBaFhnhLu94X3ZMMtszbEJws1sFeC7JGuN3ARIoTCEx1BnZbhLjF5iA6DfugVsS/vdtWnmi4jYa6FmjPOu9lytNbs9WrZjq+rSQJMuKg9RrBwrHYuHUbxoRbfeRNtc3wPHS3nPWwEMZySEwkA1n2CiGXjIX6ejE0vlDKJYO0hVUw0Yw2Ds3hq+t8WoQEuqaDpX5qEWMCxEVOtsDZMshanTqbzk+DMFQLQZE0P1E2Q/kMRodOtukxARSHGwTitb8CBfclTZqE2jCI0+w7CDuF4ZhYd/PDHFhKYEuFCOZLD13l5UuGnOkrziCVIzBfxRjVaYcV+wBS8UdWejDptbCN+QjhEfmjIfIIqu1ANu8Cm3EHOQRNVeeFwKwApFtyqBZCkm7VYMixKJMsjBWkeMp4W/znn94+pz7PPIXpq/4vH8HkYb6Wx6AmCntz10FjguOhGn9HkPpm4VVyV8RXSU8K+ErsCF8SWEicda+af+n2M6q3bcWzZaZei49WcITnKK9K41MMa0uF6uF5W6VsVbj5v1aSrwrFAdpH3Q0K6fE9tleLpylb+6GhS3zWswmFHIMxlRUj8LZt1Q80hSiWiQiq9ABfSwM/WLU/SxWimARSxUIlQvR/RoKBhUgRugEHXo0mNaXooQEJdaHrVbCttYC2KQFvw4mMEpnqGyZCn1nufths9G9jsJcIpfUtc9iKGhnceYglt3m0QhYXwhUZHljATwusPJNo4UL67AowPX/G8de4Eo4sGHpcS0o4vVesXus14hO+yU0oXAuA0m6djEnQPm9nbXnHz18wk+Pt/x094J/9ZN/WL/Hnu9cupp5uRrzqUYtvADKVDGXQ7hAuDDoKhyLmv50TsBjZqgXt7HQYGjujF0/kOp5gxQujieUoVlF/gy9LJtCL6Y93NKx79oID+0tcL15phVXWN0Zxy+SRhM24cJAbnMxNO6FAc9uNW0T8LIGbrqZwzAbt2aJ5iIIqASK5/+s+hDt1IhMQq3MZWDlbljvd5kjJYYmVLRT/tgiSM+MD0IwIAJdh4hQqqUgRa1MVQ2/lN5iuI4jaPWlpaK2PnEIjRCi3bWkfEhDwwwWDQ1UzCUwV7JUCDX8EzepzlpfQgktecVdCM/SBNNywdO5m9kamNXSk+Mzut2p0Y6Mu1BYMxGvsx2NybhyDLZX7DfH+Gg06s3Gc8Q+aeSh1DBhCK0qlIfjIso593x1d+Dr+z1/5dUPr6pRJY0Q4FgyU13oZ+25yxOpEsW+WgJvunvoqlsVNlZJxSKyxhpj9rg97RgfJiBsfs5l4K3syJxbctU+LHyZ1yQ2J6x5CvxqiZQm2LbunnMozqW3JDld09KdG7GeLzVZyo7PBCa8kItUerzf70rGmmoG5mFTx2EIC6/HI2McSNnW1yKRHJSSBFGjQbfFHpV+yHTd6o56tGzsF3IJ5lLUvAlXqPw6WgwEQYYB+o70aiKPAQ1CWOxhNNaSbXusfNsIeYR0qxaSTBBSZY11VsIt7wuyy/Tjwn6aGXtjPJ6WvtVAbJq/4gil2OafF5qroNCshRAKY5/Y9YkurFbCNs7uwzfftoKQ+9lZpJnPW4R+qzGThsayi8E+Hxp7UlsOg13Xwqep5ldsnZgtDtGLsQ3PVYS8Lzt+PL/iNp55092zDxcmsbJxvSx83r/lr735x7wYzvzB20/4n7/8i3wyPfDJ+MBnwz238dwK1fQl14hD4MfzK3423/CwjAQpvOhe8YPxLT8YvmYaviTW44AGxmUCB5nNEmAVkgaSGrNwVgtnHus5Z+2tUlatrnRXdvxkeckxm2sYpdTUeDPvp5omj+Tm/jRBoTBrT6KjlAABXnBZcZP6ewqJCXONHIjNanhMDvauvMpWkXAFNgcpHJgh2vO69RCxeh43/YW7eeI+DZxTx/04oUWqhWAAZd9l9qNR51WFebHMzP1QGGJGh4SIknOgZEFnKwsnv44YgwbQaYChZ9lH8iDNp0Kxeo69WQ3LzrCDPCr5kJEUDLysoJl2FsNlKHTjQj8s9F22Wo0h22SW2Oo1ao0+2MRDztUyiKsXqxvX4TAkxrhYqmsVDo+TsxLmmhh+UP9eAcnMtX/73NgmFOVKxvFCJk7h3XL3AcMHhAZOOpi5ZmFuwMeqSYsK93m0DVTzDJLYkpgkMUjmL4w/t+pLceb/+PI3LGNV5aom5LEMTagdy8DP5ht+cTnU+oe2McZwYAqJL7p3FmmQLc/DQb24/r1tyNAsGC9XB1Wg1D1XxJ7rbd5znyeOeWgbz747kbUn1CiJk6S2QOHjLNmkkVlW4e1sT7tXL7vn4KO7F10jN/VVOHh4dDuaoBKhBOGmu9CVwm2/5vD0oRiWVUItWmzCf+wXxsp4TCWgapjX0OWWUBhEiV2mxLiyZn8dXQkNgu5Hq+NYC7s2N93R2VBdhB7KWOsvREXNI8Tr37k7gUDsCjGWRkoKYuhvQVoUYs41AWWD8EIVBvVlaP1MsXDmFBeGsPCYyfdNw9Jy89X/PUS4jfc/NxwTSM3CqJyAytv3a1w0MrLWQbxOKwavbfBQy489lIFjGXmb9uYSodCtdSZcQOzDhd8cfkGUwt/vvuB4Gfg67nk5nLmJFwvB1ZqPAMc8MJeO42KLfC8zS4mcysBdnprP3xiBNZJw9jCj6hUByIHcbcgwabTqVJSNxSHcFRMKx2IuYwnCFCIja8JRruQhoFkNbDavRxncLZoktTCk4SDX1qF//ljY2zmLKbYm7NbUckusSkRRzvrAfZ44dT1ewGXOHUWFVMzlFWzD91XBAcQSSTk2MDxuBIOFPy3MT4FfoouejA9CMJQIy6sJFTGuQm8YA9jvEu0YxwxKbz/0NSwToGQrpCmKhSdjIYRSNXthjAtTTHShWIhICpdavXdeYkOFYZ1DFyR9FQzOfuyC+Z9j1ZiwAnJbbsL6okwrObKNmCn4mATkuMLV3FRtlj0+DVcWimlTT9IyqymKtryFbV5A0o67suNt3vMu7/lqOXApka/SwWpb1vqWqfIHWiHYkHjdPfDp/oGf3t+iKjwsA8d+4EYvrVaF38+lLmq/z0UDl9xxn00oBQovgqVz+aa3cKIJjclrQdb5cm3uQsGKt9alWyBLaBvaUppXsprzCVp17s3m9OH34P9GChkTPvaenr5bv55nUj6OQvVtPWRCE2rrMPclQCVslRDY1bJ7SQNDMKbqUgWDW4a2fu1K59xziR2XUHhuSCwNY5Pw62gxRJhve8JiQsAiDyu5Q6M0clMeqsUwFrppQYHSBwpAixETAAAgAElEQVSdAZW55qzXQppdtRi854Nv7H19cW4x+LT5clnPAamAZFHW8GUFz8xkTVfPs6USg1sMVrQjc/0St8zAq8XKNT8/bBbz8Kh2hSPtqdJ70TU70sFKB+XuysRX+cDP0y2n3PN23hNQdjHRh9zKp6da8CUG05CDLNz0F34R96QSOC4D79KOMVjBm4c8EjDi2KIWDvY5GEImaeChCoZJErPGprEt5j/gyVOOcdjcrC7HY/A2a+BMX9+BZ7magO02XBETrmpcE9ZwchZp1sB24/omT3V7XJWmJzSrxiIihhH4sa4gXNA3erRkHspwxbPw3BOPUkwh1VR+Y4UEKauQ26ypTgqe0LXrE3NeyXVdzBSlWQxlKGgU+u671X78MARDB+c3kf5YWCZp/IRQq9lo5STkSZtQkDEzTrYhcw7MopQULYmkSslShFybebhP7X7vVqM5wLhFed1fcxdDROmDhzwtmWUpsfmsfc1FcLehlfVyP3WDavtGf+w+PDZJPUPTyFPrMVtqZ27uhHMJLM/BK0Stx1n8/aGM/NHlNV/Ne+7TyEMa6EPm55cDh3hh7BZS6ShSwTFd2nluripwWnoeloGfc8Ohu1iZ9FxBwdyzlNDK7JdKNskqHMvAIfRWQWsTLvT+EhnhHHpexeNVTYOVEn29wOe60fy4viZLecFbe59rdMHn0zZoqhhMncMN3tB+I0+yYmMFgVti18ba8He6FopdCWM+POTproULtbHW9PD7velys2q9WE6U1br036mv1lpl6IIJhhAKuS8QhWH4dRQMUTl+LnTHyLKvFpbSwCXtrDdEeqGUFwv9frac9D7Vz4W+X8g5VDQ2EGKhFOE8W3Xevstclo5dl9qmTxWEBFoyC0AuoemIXPnnQywrFwIr2HLKK4vNwams1pCFUBeca29WYNL9YnvJzxN1IsqAJRadS996UjhnotSMx0nSdeRD1oWZdEv8CU0jv+xOnHLPz883/PjtC0SUw2TuDXt42Z24FPPVj3GgaODn6Yb/649+g3TuuHl5Iu6U9/NEEOWupqafln6l6Iry6fTALibGsNAF06xfLweKBu6C1cHYh0tjQf54fsmlWPGb7w3v+ay7s0Y/ss7zoaZiO5rv5dq30YgxpAYORymkYizFM1Z8ZyoLL+MDU5ibMN6moqdiBLBUes5YzoOXg99aKZ6z4e8ry2pF3pWh0cEdH3HWJKwEL4BDmEmaedkd69/tne3jfLWubM2UJvySWnbw+93ELy4Hcy1yxyn1TZmF6kJ4JOPbjg9CMIABi3k0V6GxvWqDDa1NZMpQCENmNyXGfqGP+aobVBQlx5oUU/2unAPzYqW0tBKbnLyUqwXRgFsHIEUbGgxYuLAKhSirj+cjaWwFW8x8XLWT8xpg1Vypmo/ul5c/BoD0wqeWpPT0mIHCLNfHr/cVmnXh9zPIwvf69y3E+fs/+azxM3423PB6OLKPc6uPec9EKpFTHpjfj8gx8hCVN/tTA2G/uhwarbyoWNXjaGX6D92lRTDcZTpWEhJsKOOi7OPM+8U6aXknr0O44Bmr7jJsAcBDuJBLsGhEGdY5rVaifYe20G7KkRLsOq/02ObHLYMA5Grib1moTvH2Go+54gChAoE9GUf6LPuS6sqtbmKjfcvTyJQLn9StrsNYs0y9M5ZbQe7GuIAMoixlXdcpR2JQUqbVh/Tuad92fBiCwTe/YqXbOizdNNt707hSn0MwMkcMZQ0zYpsXlKBAdBMScigtCeqyRMaOq/6CcI0n+Ivc5lU4ZTVWMLMLxaIS4j0gV4JTlNUUjOJWxppgc4WwS2kWw+MIh2uXUCfgcZ6BhdtWUMrHmoJc0e/N565tJkl82t8xhcSrF0d+9uOXnFPgZ0H5bHfPq960490yEVDGkDjlHjkHQoKcTAAMcWEXTQeekrkPPp9DyLVehbWnc6DW29t5wVwKJDEOhFOET7nny/mWXjKfxHvcfrPIALDNPalm+aX0rVWej0u5FsC2JmweexkrJTxdJVwNkq8wBT/Xk6aMCm7c05nIVCuCOXU7N8EUKmDlzYyqomAtEtssCTHL5hBmzjpTYmisUbtXj57Q8lV6WaqCGSkIp84a2Mw5WralrOs4bkLv33Z8GIIB4x6oCHlXIw7Cdb8+gGjx2RhqTLdbmitwydcpRdty7UuOzAssOXBWYVDjl7uFkEsghsK2Ik9WY53FCjz6J0NYalRDG4/hOUbjtqyXC4X12psIBYBk28D1Oa+owfqU/+C1E7daZxsCbRbUI8zC60RYf4PA6+6BP//ia756dyC/HzjFgT+6f8kX03uKBt7OVpOiD9ZbU3tFThYaLioMNTrT3JfKHp066984htQ6eflPkcd8ga6lKI8h8bo78rC85pQHvloOnAdzu6YwrziL51gAURIhFF7GB4K8bsIhFbP8vP5mw2K0cGSgILwpY11L89U72ocLofQNY4hoi1b4+6MyP42s9pSl2diS9c/bzy1Eu86BWSumUF6EE9ab04hdbh1t3QnjjyxXQs1dvkvsOOfM1C2kZS1W81zToz9ufBiCQYyfEPAek2oJILUcPN6AI7ppbznoh37eaKmnj+L0Z5+UXE3mUgLE0qikXaQVvdge95gUso1qdNWleAyGbbtTPZtuLYY9JNZah/7356ofO2jY4utIC4XFjYnsqcjOd3iSiyGrOzMAhYVZO36we8fPP73hD9MbtAhf3+/56sXBmpjMUwNr990MwUqGyTlydx45HzpSF6wK0aO56kKxSEedC8cExpAIm6jKuZZqK9VNuo1nXnQn3qY9Pz6/4F+Yevpo2nGbobodg2QOYeZlPPGOXU2H1wYwby2GKJZ+n4jc5V0DWM007eir6xKlWHsA7Vs6+DYkmRte1K2h0Db3VsDmMbGphV2dLKWBRNh0rrJmvdseJ9t+oFv3qa2toMxquMwuznRhbOtURCubV35NBQPVXWi5xnqdJhoV3zOlGA101ydbrJgA8NZrPpzduIgSSoBuYcnXDUDgqVsB1zRosyauPUK3FrajIFfhKT/usYAwv9NQcctONO+1LQ7WyIhXrAbW6IRP0Yb3sE1AeuxyrK5FaN9v55e6URd+8+Ytd28G3r47cL4f+eH9K/b9zDENHFPPELMh39Eo6IrwcBy5u5247S8MIXMzXEglsFSyTWvsE1Z+QasGLat5Xeo97mvF514XPukfKvEq8C5bTbLbeGLyPI3q62/pxlOYG3jn0ae+Am+XR7Ui/H7O2nEuxp0YyCAL3iToGqvprhLF1mtdZ3JuG820VnQbYPgxd2LL/WgtE8UySb2h8eN3GVH2crm2IGS2UGexUOcQFsBqjJQcLU28PJ/t+03jgxAMEpTlJhPmgN4s9JNRmYErSigYxjDUBJJOSvOHffIXtVTXS4mcc8859w0t90o5Fnosayit21gTlSBExTDsGBpQ6U1COik1ZlxrMdSURwtFrmM1ee3+Bsk8VNQb1lyJwtrwxl0BxwO8huFE5f2z8hSmjcWytSYecynO2reF5BWRvPP1b+ze8uqLE39w8wn/75ef8oc/fsN0mElzR54joS9Mu5npxYXzIsgSyOeOn9zd0sfMnz98zS4mppj4+rKv7yG08G0zrdnUL8BTj5emzYYKtv1g+Jo+LPze/ff539/9Fn/x8DM+7e94E+85hLlFE0r1+cGE7at4bPPm7EfXnt6HtK94RlHhUnreYWX9iiRmiSTpGofCMyNdqHnRGHcx1nkPjaV51r4Rs1yItT4fLiweEbYsu7a09O3baOSvc1mrTmyF4aFGchAriWfvd2h7oA+ZRQ1wf3iYyCm2/hPfdnwQggEwenM0UkbXZ9v81b+HVYN3sbRQlIfB/GUnjXSSOdVwoKdF+3CSk5dsA0tZLSrg31UCKlYfQUSJnsDi7MdHG85HrlrftSDyy1Fg4z9c/y1tYuGexbgdvom2tQofW0HtmYWrzxxRN6GypgBPIUEH39+95+sXO37y/g1njNKs50g+R04K+8OF/sVMOneEfkXAAXZx5rPJ3s0598+arqFaCmuUZi2FZve9PuttOPP96R3/2y9+qwGcNqxDtbtnXlvCuSURKw6TNDZLYR9mjgxQHJS1e2ht8Io1KJqkNEHTsiJ1s6nF53dNfH/8frYVrjLWUczYqJtkKtZ58MpUvsG9unQv2aqJXVkka6EfWIFRtzbMQssc4oWb7sJDNxiO9p2hxw9FMIi2YituHfRdbps35bjGZeuPhQKvgT8P21xy1xBar+Xon0fHFdzCYINBYPhF8xWDthLdHgFx1uOigcVNvYI1yq0g1RZ4tI9DAyFhpcNuadRrfsB1K/bH9RbLcyDlRgA8F91oDEo3axUQ00H7eKFXAwjLIHy+v+fnh1vLzMvB2q6fI+XUITdnbg5nlp3Nz36c2zvqpb6vAe7SxBhtPjwiY98fm0Cze73OOXhcRXkKiZQjf3j/upJ7SiWShcp/WFOlzzq0Te9z3ISomLA41mNszq3DWJKOJB2F1RXN9b69CM1cp9StQd+I27G9d6DNtZ/zHB/BGKtXp+FtEV0AbN3BgNV1GCoQnTbf5ULCWyS8Gk6cc0fsMqrffZt/EIJBwEpaLSunIFRz30x1JZU1U9EtAV8IAWMaLsXqMHrJNq+54JvFQ45DyC2xSsoKTLq1YFmXrMcAY2fRiK0mtLqAxnR8rCG3DLar1NvqNDwnxLec+K2puq0bCTTAsZGw2rVXRtzWavAiLatAWq93G86tNsBYawN8+vqO42Ugl8DpaB2USAba3uzOLVITK1fkkjt2tYt4F0pzZ9a0YuuodQUCUiqT0uZkCqn1jXDNv6+duVKOvJ93fN2bm/K6ezDhW0N2zprckoaA5kq0jErWDWyuREcfMr127NW4L57RmvW6gcz1u125CVtXacvNaOX91N7BFmCMKLNCrK7B7FXEH62FQQt5Q4LzHhzOpvQQqWeeGlhpz7sLMzf9hb7Plkf06wo+hq5QetOGZUPtjKEgJTSSkXMOZMNAXIgVybX4ulNyYWU0xlAIupaA93TgLlQp7jUZVCiV+wC2fz3xqo/ZOPg4OSpQZO0X4ADkFvAbwnWcvFAbkpAbqWa1MHRNNfdFVuPWW0xizZu4Bjr9/tucbrSN8yLcx6eWQHe23j7MxrAMC5/v75kn28THm4Ef9y+YH4aGbr8czsbjr65bqhprZGmRB7un6wjClQbdAHU9TnSqRCxi44Ps+5l3l4mfnw41BLpwE8/mBmyeNWnkLk8t9u9a1BmCsKL6HtI8lYGQvVr10pLG3FJwLd9voiHN7amcBbeE1jk3l2QLjrob0oBlrNgLLAwSKXXeHo+tANj+zddJT25zUDaWCMAuJl50Fw7jTM6B8Iwi+uPGByEYgij9uDAXQYuQl8D9aeTiWY9ivO8QV9JGqi5DwMKHS7ZyXy4UfGPnDRorFbD0DDWPM3uqaxctOSXkyAXbbF0NjZrFkBv/f4046Oqnw1XGpY9VW1WSEWuhEKhNaJ5xAXzceibiZpNnvc4QjDWK8diV6Kt1sg+Xq9JOps0ChzBzYOZB1loAFvqy59iHmcvnHX94esP/8gf/PD/66gVfTzte7s68mk4saiXy7pap0ZndnHVB0HIWcEvqGltwUpGXPgtiPT72YeZfevUj/snxDcfF0rkf8kgqHbN2TDKba1i7S3+97K+A26SRpUYp9rUdXKoJR+5yOqvTrbC/MPyMqVl6a8etfQ1jPtbs11WftQnenuUKbEylaxEZCI1+PYXUEvnWytKl9RHdhl39O7YC3551FZb+MyG86E58cbhra/i7jA9CMCAYO8vLZldTPtdSV6rCNCSyrBaANZiNjHGp5pydG0SbMNjSYL0ALKx+uJmAsWEXRYWgKzVan+Ex+Hl+DQc/nfW41n00S2JbFMTHlh0Jnh35fGiqmY5AdJQdq+Y06+Y5t9HdR/ddar+DbYSj3cdmg3qkwjXqFBIvu2MDtv7X/rc4vxtJDwPnmx5eYcKhBO7KSJCBKSZuulrDsHRGzYXGY3CyzraJzHYu3N0xwTlzG898Mb3nJ+cX7ZiVSh4qiGqpy2NYeJt2AFw8M1ID73XaCCtfJ6uFmDRwv4zsw3yFB61l9ZeG9cRq6bV36+/QCVCb8HJbNx5taq7FpsalLDVsvSZvZbHqXY+zaJ8bDnofwqUmuhn47crupr8wl8hzgPkfNz4MwVBHjDXrMZZW7FJVyNkyy8YNJdkxhlQiXfRqSrbY3taNTrGQWawVoX1yFg10rBu89Q+kRh2ruwGrae4TvVoLaxcl4EooeJpyxOs+rsf4gvDF1xYZXpJ9Y35X/3wFLa9LvA0bAGslOq0gZsDTrrVq0G9+3VOYa/jyzLn662MthWbFWhI/eP2OP/j6c8J9ZE6Br/rMi9GsmeMytLBgL4VdvV4p0lwLhBbyG57x3bdzafNmLs7r7sgy1D4ejy2rKgQtIWvmFFZqdFFLAc8lch82lcApLLpmg44Il2qlbMlk9t7LVYr24/v0ZrbgdR1sGE9lLX8fWTM3W1SmRje2kQ/rt1ldzeo2+DtubN6NW+n3MUmyxL0anfO/76Lxfeby3bb6n0owiMg/Bu4wK3lR1X9ZRN4A/xXwW8A/Bv49Vf36l1/L+Azeem7legNqrb5TzEwdjYrsL76o5d93YgtliJml8h+CmiDw4eHGsqm7CM9TRrdVcb6Jbe6m8ZbdN9QGJPa5to3QagxsfG+Ll4fGLXAN3gqKNr90KxysC7XRap+vHvSYzuJmrtdmePwZ2ObKBO7D1IqUuOk8SeIvv/ySH758xXLcE06BlGKjRr8rsWWvDtFyHnYxrd8VuBJw2wI3T7QrVjwXWIXKAPd5bIJ6G4FwYXxbzelL6TjlgZO7m6I8LCO7aILO3k6NLFVmbAqBS+k4lhGrrrRyJXweIk9dxC2WsIYpVxr11h3J0N7ptQXpZLZI1I6ezCzxGrR+YgWulbB9Hs2lWkPcHhmxVgn/7MHHf11Vf775/98E/kdV/Vsi8jfr//+TX3YRBxTFXYWyLnZVa0ybusglR8YYCHXxrW6BchMtJffVcOS4DJxzT6rZlD6u4snNx11N6SJCqX0Ut9stoDUqUfAwnCdQ+YvfJlT59b3zsadMA82kBtZYuvZNywQta9szmRlZe2X65jKykMMG5VFx2KdjK4gciLRiKRYfNxDUmsnehYljGRvq7s/ypn/gs5f3/NHdABUP8uEl+bMKY7dwiPNKTa7CPga/h9V9yVdCe91kDuK9ikcexIrKTiEZcalaWW59RIx/sA8X6Cx709yGiYJwyj0jRoXuY20NUMy6Oee+Pd99WAvJ9DV6s+anRLZg6rZq9FaguwWRCFdsSQc0/Zyr0CWllpfVJ1aDux1bnMELBT9WCJ7J+4SKrfKN1tk3jV+FK/HXgX+t/vs/B/4nfolgEJzbTfsBmnAQUVBpKdRlcMT+qRTsJXPTzXh5d6DSY3PLm9iGMIOsrkRRczlC1FarQR5ZDOPGZfFr2XWuQ5JRtGmGreaHa2BxOx6KAYBTSBQCvS6t9PlqOdB+x82/3a3IGyvHj137VlpYF63aTkNzV7wSUSY0INU1oGvNKSS+t7/j/acj89zx6ubEvpstWhAXHhgqOzTwkAcO5VITp9Yu29vxxEeHqyKt9j4XDsFYkQ8yPrV25NoKuYlnxpC4z1PTqg/LQNLAqQoMf59zsRJ0d2lq17jP01WIcivAslyXZ9uyErfP4EV6UntDy5VCuj5Wrq6xtRoeG6nOYdnWv/TRy3JFob5oZ6H7EpnzP3segwL/vdju+c9U9XeAz1X1xwCq+mMR+d5zJ4rIbwO/DTB9fsvNdOEOSKlrjMfWpjGYS+EJUClH+prItIuJQ2eIsfuMAeXQXUwYbEwuJydtR0AbGcdH0sAQF+bcXVWULpiAcYvBr/nc8CxGb7I6KzjuHzyktokyeBUjuygMGxQ5ooyyCgKA1qMSM7tntfLxibCp3nTtZmybrlwBaPUQN8lfxSPvmTjWteyl8F/GE3/p9mfc9Bfu08gUF257Lwpr/JC7NHKs/TvczdjFxCkPnLsLY8UC1spTmzR1N783m6gVY8VqeJYYONey/F65yd2uvgLRGeuR8TIeOfYjXch8Ne855Z5UblueSyeFhzzw9rjjq7Djzf7EbVcxlj7yKh65DScmsS5YXgjGXUJ7D5uqT8jVZvX59boJK3axYgBQszmrxThrLR4U4AVnBregnFyna7m/NWlrtWLcArWCQR1/dHzJ2/OutVz8tuNPKxj+FVX9Ud38/4OI/N63PbEKkd8BePGXP9e+gooJntCgvUlTqem+qW7ULpg78XiYSSU1mWQdWwBmKzDcRQDTGmO7jhJyYRYHs0wjby2VbUGQiCfCPKUywyYvQtcU6XbPFZuAtXpxi0pgSylKZd65eXtlhdDa0W2b2HjzmhlL9bb4/yYLkE0Rkbope1kshBjSqtnUQMQ33QNMcNdZAZddTJWyfKRgNRo8ZHyfLNPvGHpeDycKwss4toU8VMH5OD28off+bFI2XITnF/jKO3BsxkJ0vWS+PxgWcrdMV2AyUMv+CfPccxcLb9OukaI8VyVKaa6e3UNsboILfrfAGh9BVp7G/MjK8fOoVGnDB8y9M9q6A5nXgDLyvCLyZK659uCAWm5AI/fzyP15RHb/DDEGVf1R/f2liPwd4K8CPxWR71dr4fvAl7/sOgL00fIjThcTAG4ttGa0aqt+y0uAayALKicC9+VCA6+2JqgJBHsB9n/LxHNLoGiocfxCoG9xbzdNwyOB43/PsvqVj9Oe2aDRz41tfUPXLmuBl2uhEESwLtHrMGBrXfDblvPrM9emsFdazRZVj1fFCRV8S1C4qhsxycxNjI327TH3aUPiGsJSy971/OJ8MLei2kovh3PLaqQYeS3XpiyP3avmvkhi1k26eEgEvS5Q09LaZa27OFShZinbkTTEBkKmYhbn1CWm3NHHgZwDD5eB9/OOMWQu3YlL6TmHnqi2Ju7y1LANor/j5WoTg0e76ruu9HN34ZrC2FhGjTilq4uRNpuczXt1XltjPVbLaZsYt8UYjnPP+dxftbD7NuNPLBhE5AAEVb2r//63gf8U+G+B/wj4W/X3f/PLL2aRhubrF2m16tZjNhutCocxGBPRs+Zc0hsfwfz/5i+XdcKKKBTIIq0KLzhPXRnDTKeRUpO0HpaRuWEK1wlAW4yh32ir7XiMQm+Ha+ttckxrktqAuOdHRMjNpF3b3pdmal5jMSvdd71i8noIsgouj15kWem+dq+2KS7humR6C9X2xhW4lI7URd7PE8dkma3HODBF+yyK9ebYzs/jQjZ+3Wt3a2UvznRPGKXb8wJlLYkW1+jFu7BreEfAIlhDl7kkc0Pn2gU9azAQs4x4/4pjMYzjIl7zceYQrn3+bWGeQK0evVnK2zRsfy57x2apJbHS+LPW3p8xXWXTgpXzO7doieCJW261XL3fHCmLVS/7LuNPYzF8DvwdMdXeAf+Fqv53IvK7wH8tIv8x8IfAv/ttLhZENyWppJGctgjMttzatqzaGJYaP/cqQdISm6Zg3YAu0q/9D+rLiRKYK2pt7EolVAEzytKQ4C1G4eNaKDwmQpWrRevItZNbfmnT21/yedk0wF1jJ9cWStosDv9oS6tum72CZEbPdm3nYJvVPmxuj65CYPsMqSZH+fwfy8C59Hy+uwPgmMxiKKwg5FYjbrke1z0e1pBdmxN1hN+Cjm6q+4bb+v5uOe7DhUxoiuMuT9znkUM3c67hzFYjtISqRNZohHXZttoNF125FDEWMukJPyQjrUisA4/bkn7PuZJOk09e9IVQMy8jBkWvYetmFasdd66g82N8I5VYre3rvfNtxp9YMKjqHwB/5Zm//wL4N/8k1/SqM0Zu8ghFdZahFbb04W7EFFKbRIpV1w1BG+nFF8wxDM0M7TCA7lIBxpB7S6ipNOC14nNnHZpCNhO0bQx9IhD8np4zi92dQGsORdXKjiPkqjHchF5DoKYps08Iem0pbGnRj+5lm6XpTWEfDy8Ysx2NLFPWjEB/hqCWC3Epa5hvuym3FZvKIAbw1a5UV7wGVswGuiuKuH1WTeqNZbAlPnlVH4v/Gyo0kWp4b513C3sm60jm2EMVYKc8cNNf+Dru6SrgPT+yIO071mKz75epRTYcj/H3vm1J6FaiZ4MiGDORNWck4lkjNY1bFiZZXQnrRj6bgGabD6PXRWw1NotiC4ovWosif0ehAB8I81HVMvTOizUU1bJ2fYp9QaTU9nE2MbkEjmm4SjXOhMqjj/xgestNPHMbzs2VmCRtACxbvMc8MoSF37/7jKFWvnndHRsNGGho9LEMjVG3xTQupaOIcMxj87sNtV7rH7T0WEoj7gRRJrLZElLo1foUZkorG5YlWf6HCoji3caSFjIr4Oh6ohfhEApntWIuiYBz6wNKkoW+1RvweoJLY+VFsXL0B1mYal2As/YtRPZQRvsyxwcILUbuPI5eMiOJi1ik59P+/grn+TodyGrFToMUgipD9dMdiKPSi1vVKZ8/D7Gyug6eq1A0QISghSQdsZZ9t3DnpYGRU0gcyoV9uFAQgtxynIy1eUwri3EMC5Osnbl+ml5Yg560o2jg0F3aerIelWsJAK/rMAhNMLkwoFaynjVyxNbMF91b29RAH05MzCYUNPKgVkeij0cilQkr+apvSA6BmA28vuDzYUJ5PyTO09PSe79sfBCCAWodvhxbWBIVCjSB0NrHiVVUWkrglHu8GKk3PMmVJh2jNtPRzaybeL7y//suc9GuxeKdeONaxXnoLlxgTeX1fwPNNL6Uuhh1zaPfmrZXtR9ZE5y803WuPRPW8mNdDeEtZJR5Y4m0q1RLogkHAFHO0LSqdcJeffmh0qNztZoe+7xutq7+fFw5EfUzrz60Hde1MUpz4/ZhZpCltZVza6Oo9eAwIWPuREtJr8JhW1ZuidAAACAASURBVMfA5yarMGu34amYNTNrZ5EdtRZzPrKmNv97LhDs/l/GE3fRytNt3cRlE/YDUw5LCTwslotg7kfhq3SwakuBtTuVYzKygs6eGL+dO1ib5dyVXav8bO8UCDO5jPaeqoLosXaJ/k69t0gL+W72vq/RLhS6rrS6It92fDCCAWjchZUFqXRdZugWgkBRM2/B5uA+jXXRP4Xntv0coiiFwq2cro5J2vG6e+B74z2hciLGkKpAWarsHepCEvbQukM/l+OfNz60x7pX8IwmZKaqkXsP21Fag9ozHVlWzTgTm+ZHlbQB4wIeolSCCBFp/8/oFUGmUbHVy41dU549sStWluVVCFHtPlygwDUn35/dCMDrAvTKz44HTDJTglSA2Ls8sSLpG2agC4enxKBNHcwNCr+lem8rYaPWA9N6MtTMx0qaeggjb7qHxn8BWjbmKfd8qS/Yx7VRSxcyd4u5LXPpeJ8Cx2EwPkVIBNZqUL12FArmSJTq/km7P2p3sUxo53hdCt/sWVYFkTRy2byb7XgaAbvG4Kzlwq+hxSBYf8OhW+j7SIyBUqwe/tAtDJ11cV5KYK6JVfOydlT2VGjY+ljXzDDPV4B1QfkG/2y4oyDcxHPTks3Xx1KxvfS5E5xa/8GQLYIgK3Nt1o5erZDIFoV2gKnfhCFXN6BAWfBKz4nYQoXnuli3SVv5GxaJjyjmi7rVktgIBxdsle/wTQvNLZrM2p/CSFv91dz6cLfpcYMdH4NkYjzTl9wKi9g9GApvVPCVBbgt+ebXuo7VX4fnVh7JWlk544V3U7M+bG2UFn593R8bG9aLt9znkfs88sPzK150l8aXmWLiPo2WlCfCL+YbXvVWa3IKqbW/S7r2gnDBtI0WNOFUxivh7J/1YArCFYa66xXp65pMG6B2ksTZ3YhNRMp7sHzX8UEIBsR4DPvqjddQLVDBvEp+OqWeHAJLDixZOKahCYQhGkbgVGXwjLaVIgzXJbi8UerL7liP12erATsK7P0c2999A4U1OcoTqR6n7l5XcdLKOViJS/ZBIalZD0ksZ8ItEAP/tqXQqruiljPx5J5xofN8kpUvviLXC3Z7vpmra26FFxhxV8vvbRtqXftgVDfqMfrOygh1d8A376AZL6vm87XlXBjxKlaMZdO4p274phyeCRn72D7rUAlMr7uH9rdxQ4L78nzD15c994MxPiOFuXQMMXNebBMaIarQ99mSmKr71mutR7GpIdko1FUxDLV571UyFCteY+34nLgkV60N7ZorC9KF4dYV9HIEXczfWTh8EIJBUF4Np9bn0HsPLiVwn0fmYgU17ruRd/PE+/No1GgvyBJp9GgvMOITOdXUYSTwNu8b6LYFY7yEmA8373z4Rh+rz+xaygWBt4qH1TKxaMnc0OnH1YziZq+uxCVzDbIWEoWkC2eNvK39Gb3oib98tzrAMQdtXIb2HbqWfnMT1RK6Vh7BVc7CVZRj7bUIpvmc3HQIl4YbDOIkn9A2OlWbU6CI1PnxWhVri7VZI2cduCuTFRipVZS2Atj5CBElxpOFHMtEkkiRNSIyVLBw668XwtW/nQi0bd7iFtRNPHOfLW/iXHq+CvtKhw983r8HLLLys/mGgHJcBt7OO5YavfpsuG8vNFUSVyGQqrLYYlVggm4S69+5XXsZrCnSZg687F2SSAzXLnHEFJc/t1lF0jKP+1h4HHn6ZeODEAzORTh0F153R26jlQ67lJ5dtvZonnK7aOC8dFyStPBmF0oVKEttiXZZgZ4aQnLfNCMV8MtP6KWWpGIL2sqPr4vOaxNY6CnjXaua2b3JnPSxTbl2zCPI05fkm7oGrfB4bYbmduQNS65/dI0MDYTMV9d8Os89mW3fA4C4SQm36z2tBLW1tDzF19maLoi9XqMBZqG5emuL+k1OhPvbYhWYVqygMkhrtyXHNTyTMgCEuXWvKhJwER42Qu//o+5dQm1bsuywMSPWWnud330382VVVlZWoVRDAsluuGV3C9wwFobCDRu74y+UG1LPDcstGdRRwx8MAkEZhFwNS1bPwrhlg1FHwhjjhlwgKHDJSlVWfl6+d+85Z5+9V6yIqcb8xFxr7/PefeWs4r64XM69++zP2rEiZsw55phjOhMT280VsxgG2JnBnWlFS4tmhYTu/St3712l2gBVqf0YMaQqfU5qxjMdPJV9SNu0rBwMfe7ci1A+w4Tqad8o6edzRYJRmHdkTXr6velclHifrI1iBMw/dHwchgGMt+MR3xqP+M7w6JZ1SYOeysI4W3NBA+FllW6+Vl8hcm3CurvNi5+qccSNZEDXPrsAbDMMVldgHsVBF6vFf0AnOsUT2Fz1DiZZ7CgKv70Euo+4kQ3HHh1YbJj08kcSDUIbtdM8vKik2eNhRAbhhIqFEAxfd1/lPWnTWq0TjnrMO6ces9t3LyxNUiauHhPvsyxQA2RMRrk3sinOyvYr+ewTEVmRkeQkpy152tSuKfIh7D4A8FoH69WQ0NDF7HqIc0gCYqY2uUGY0iqNaVQJXJ4rRWOnOuLURunjwAlD0iyW7k9JY8od9ZSsufpyPPX7TjK73t4uXJsQnuR7TIpfbTNEw6ZWIxLzrB7k64yPwjAAcJHPWTUTs6K5yD1WOuQVDYS7cfGS6CmtOGhzVeuQbDiBT1iI7SJHHQgIMXrNw7mNeLfedNFQGvFmOKGC8JAUoAwbz2rgr4Ft8nvd3OgKyrb5M+DufybaGIgMoKB3tLYNtTFygBdMxbH3GAywiizG2CXbMxLQtnfcQayiGzaWGYtBkNdaBeSiCd6Czvfwbs9sNRFbnQNTuaqccKyTtK5vSkLiVUR8Hdnvxmui3gfTPIa9mrbNvb2/YTUyP0kAXPXo5P+9I5gpLRup7cwDSu2MTSFrJUxpxWOZ8bJKYyNj4H5nfEJOjENYiwLZXjJfTzxKdylLWxM7fmJrtoZ0pBdlBeOwaFgmDM/WW9ZR86bMX2d8FIaBSFqgG/gXCTOtJXfjE8QQvBlFTuy8Dt55+jaJUTikHu8vmqu3CY0CIR0EC70NCVJDwRLvvtQJKyfc5QXHto0R7cQEOk3YhhN1/HOs+7aRjV4flbmDkRCc4EDVMYW9i99fB8Eo4mPoJ4WnILlnKUyp2j8LHbgtIQVreoRC2qkburF9d8m2rMjc8KwXYYYhzru9pwCTnRA0p7JpJWenpIViMRwz3MDSgntNA5mP5O9vw0MHyyhtgGR7b/GWblX0x9bB03pwt9yqdA+pYs0rlrbivM5iOGqWln1ZtCgSNz/kKkl6NnoNGx0G6lgKIPcyZlwSGgofpJIU2k8z0Nv3I5E0fv7GZiVYS5ct92zpP2H2FUfAAVngd4O4ml/wjci6k3WmEuAqZhbcOKCTPswoxMUGKOgICSFMxKNqTntIFac24j535aBEffE6vZkub0IkOsl3iL/rIYV5DhFATID3bPTXByxAZxBKSNwMS2eNIXypoIA1kMvbmVGwE/Zaf4s4bHGbMpU1QQEgTVwUFyjIPWTTUxosuIZ5VKa+ZKGbnaijPsdBNfsOJP0dK5JuEnI3Xb73YNOCBd1LsPf02hWbYxK1KDSr+2iYacU5FbQs5cvv1xvHh0qTdgWjlv0P1HAYVrw7zXg8HXCuGT+4/Qxzytp6riLHawjpbZtv2+RxxNR7v34RZBFDkzbPM3FdyxJlSDPiQ56v3sMvGx+FYQC0T6Mj0mvnHGiK0HpSWmu6+8NZMhCp4iYvOISF40rE3DBydXFTo8hO+llWxWijQIhJiRiflTsUynJaU/NmNgYAJTJQUeNQXXC2UDN4U4dv3ZxFzcjSgbuQYvcTsHQmMEPASMMPzAiYjLxw5RnbbMdlqlLes6dRCy5PGyvXLixcisa95qDzO0RS3dKm1uzGADiL7Z/b5JV/cr162pNsPscBhi1uYZme23R2cM420cRwIpALk2gmyWJxoJeYz1Tc0zEcRFilkdTVRNE6eC2f5Bf30H54ehvuitDPV0iWwsCKdUo4nid89u4Ov3v7Pfzq7Ts8DCc85JN4tJMcaNHDzHryAxJSOCnvwvh3mX3rj5m5l+q/SScJJYJBL5zx7emIBMbT2sVwP2R8FIbBCqNsQb1WXbiyxmmaiXgzvHhG49JV1ckNTVWAXnhlMWtUK84sVvgunXGvANjKIih6GFbPnUvKc0t1daFX3ToWGVt7s+yxJm1Q/z0A2cK/bcF4aGGehLJA96Pt3s94CP65TBtDYBkOY46aStWeRQiSdGrkZ3jRVATP9DvNVFGoYE6LeB8teTrzGr/A7gNSzyaZAY+hmD0XitfIXFSV3u905AuPwD9TwL6M5OSzzXXQtpANkJCigfBmOKMoKGgNewFhQI7UcD+esXLC82FBKTf48fEek5aWW+PkUxtdmcvmEeiaF8YBkboKXJ2rKCNnGZxJU5WAGI+iGZZzKrhJC17yuCkO+5DxcRgGGEkoLrLLrsLWAWptGdYnwEqrLT/u9GTn2PfKs2tDQMHOUjPX9VbFTG3xrJxxbqIJmCFHvhmYOKJOwZ4rb8NIV3vvQH7XQ409OGkeg38WOu35NQakbVZTcgJ6vUcMcaIuhBmFLYVXTmDzisSz23732B7vjgpO1NH5Yzv4Yh71ei1dB/Scfr/upoahp4VjZgWQ0LCa0CknfY52eeJe/h6zGQLw9kIt8xbsPkpD3KEDrCzci1PTWhz9Qs/roUsJalgx54JP5hPKmvF0OuAP6BMAwHmQ0u4TT5hZsamQ9ZF70o2Rkcb6vb1sdONzHqjkPQVsadvquNY3tq9EpDALMm78d7nEm7S4HFtTwNBSk2YUxOJaVV6vZbCxz8W7BJjTg7MvCCNZFW445NUNEgCceMDIXYbNvI9egmsIcrowHPJ726wCGHYDAH8cgC+MiQhLwB1iKrKDWJfD8QJ09zIK5Jpx6L9LOCFvFmIEKKNRuGaIUsiYjOhubqEBJ3eRm7v10cuz99ynfT2FGq7TfhcXu2AnmoLckYM8BNFsUiZGJdqEDRHotE1pczLyiuMwYU6jZ2jWlPFSR+9RkqniYdQ06z3wT376Lfy83uIwrMAsRux704Q36UU5B+JX7jklnkFBwohOzPJ7FLIqxe9VqJ+hwL4MKfmvOz4Kw8Do/AGzdpLeAu6UFfat8YiK5IhwY4J1f4rpn6ppp8u01aXVfS1kSWZcoAxJXUwN5K3TLOzZMwgdYwigZgrXaCkp4DJ0sJ8OJJrhCPyE/j1JQ5fd47x9zrV/X3xf/Y4OHvLQaxNgTXC2MvgxtXktU2L9IE316UQjipKiNtJnwMawRmPgqcadpyAyet3TiVwJwy8uwgR9rSH6yZiauzUxUwFIANERogeRqOGT/IKzamg81VnSkmnCWuX11j39k/GEKVX8E3wL65rw06c7Fy42pqcVRe0BUMOvjGHaszfGE+kZGDn0hs166gD4pdrY1x0fh2HQrISV5YoAJ0vs1YCUmqcjAfjJ3V3jjmTL5IoIxj5f3Kstm3cOjgCWxXiAiWlIB2h7XQcwA1CKugEc5XN6nNw/eytbbyOGDgA2G90Mxz7UKJaGpF53EUdDxxS2TXavbbKtwTA3NvaTsOfXXXYkLsT9yCT4w8QVhVbvZm1pU78HGnrFFLV9Xrw3ALwQbEF2QxZPXDMsGdWZorHgyjafGTbDgMxziAbLGtMKYKl9MpNIvJm+hIVNY6q6+buAz/3tGY9PN1iWAU/LAbfa0uDEE6a2ZZqOtG6aH8l9GKRUntrGqMemNqZw5WEPKgo1bxYEQJmaCxLdXNyjLxsfhWEApEnI5+sdfjD+1MuSn3lATpKa+u74DrfpjMc2i5pOnVBaxgkjUu62V2okJmQ6h5tclEWnoigKbPXOTIP/PPHodRJSSssbP/074yPe5iPe5mc8pNMGMbfPB2QhzsEwiLCsWvPwvZ2gFP+vw57nQi/BKBQkFJbiHGtZ92UjVuJdMwqyUdil0s37sdQdwnUWzpoy3OsM9L4WBkzOOsdI4hkc28EXsbFCa9Cb9BLqcO1eDYsugPqM7Nfp1GwYZtTDj5Omq53hadmNsOFc5dkB6o5JCEW9ZwRs0x1UDxR1xEDCpTG90Zu84FceHsFMeDmPOC4j3g03TrJaOOO5HTbp2mgozdA9t0nncuzhLrZ4hFVUzih6jQVJDdg7vnGt0+GbWETVQNq1SL7QM0/IOGNSALL4idD57ACEKbdbnDYqE0a6ZMPti5mujUUp0fb82LB2poKH9II7EoZlZDPG8mRzdY0bkF45WS1tCUA3upY3X7nGpt8L6Jv6DFMRav05DkL1XgQ2R9didfv/q6lNew4FPr/iOZnYDYKNSKkGrPO0yLcdodRqMu3DnumInxm7SPv7Bt6FnKiDn7zGm4h8jA5IXnkf/RyrGhXbpV5C8CxNdRromQBA7ysxlpYxtIyxDZsDZEpVWtCrTIBlfKJXasO8A6RFjZFmWJBEbId6aHyNzGSejcy1GGLzil4DLb9qfBSGAeh05BOPGkPSRYxplt+GoNLJnw9cR+dl0l5Pg0YBVBuCcciGnlNx9uXbfMRdOrsEWkTmnYmmhBWgZwVe007Yhwrb6+7/jthB2WVrGrZU6mujcN4AeTH8iuy5Gk7PhftC65hCuCYQUtisNix7JNJp7BkCA8UqyEk6wPYzgO7Sm5cQy5f999w2oZrPmXoqxqkA1g1+sh+dPt+vxb/fBhRM7rnELmeAhLalZQzqAaw612OuGHMVFTJiPNYZ9/l0cQ1uBBuAUJEbDzS7XxE/iViaMT1774sVtypptw+pP2R8RIZBK8faiPc0O8gFoPPclbdvmxbYAogdYFovTucLJSCI4lBW9Lwo3ffETW9JBzd785EFswKPsxqFEUIqKiGT0E8Y0uu57urvw4YL5tuV58lckW/0PfDUwanrwGN8jpxe21jdkG+Zny4vt9dVsCyOfOfLubXrjNL1woKU50aPoC/cLfHHKgYRTm17r8hZiCniPnfshilZejJcX0Vn2gqLUj01quEzophNz2AdWejRA1VM2mB55YSXKi3wKhOG1HAzFJSxk47ObcC5iabIZr6YuqfSJjdOgl0l39x273y+PIuVpJgsfEfHXVJFauzg6IeOj8Yw2Jc9tgPmVPDcDniTxLo2iMGwRqviZpG7fNfex//vabLARKTONrNhGo+mpQDIKTvDiCOKS8DKZLd6CP2W8GaTXgsfnLWIHVUaW26BZSjia+x1vmF08dr38VBjFzIAlvPumRIA3UMLaLelzEDw095qLOQ9La1sm0jTmaFa1aTqMnfdBfPu7NQrEGZia8k7Pu0Vp/e6kr0CdCuflxUsXiCHxhjuuxWheV+NEC5I49jLmpGqp6+FItfoyYnYS7BLy0iJvfBuUl4DDt1ALm0IuhVCD4+p4MiU9Wwb92uyn3bIxdfba2KnMdM8vU2Lt+X70PFRGIYEdkluA/9OJB2ARrr8QuYxRJk1K0KJTV+c6grL53fRDo8dSbjrnp6EuKgJrK5ZVtd3uEiB2hABVuUloNOZTFJc7l3nG2xOdYI2x+mKS5EuvfneiJwEyV90slDIPmDXVwL9lJx4azz3TWiiiIkv4pY2BKR9GBLBTDNa3ftgp/BG3oCknvOmME3es5/u0WuIGQtAC7T8eb0asSlS0GibRo1GwT4/lsOnnQG1NKFdc9yQ1h8jQzgugIC7JthqbNxxqrhnZdC2LNWail/VdPbMmM27XJelx7cGPA5bh2I4ho3HZPRz+34HpZm/Vnz32vgoDAOgdOgkNNVjE173Y7vBQ3rBRNJo1U71liSDMMc0o7mq3HCXrN/E4kjvCDhQaXTlE0vlpakATRBlokwi9/b5eieiLVX0II404S4tQvLhVU/F5kbB3PLC0gQECEQc6jJztml7OzloauqSIm1jY3DAF9mQOCr6ie1oPnrZ8YieKtvH70AH3SxLY67sm3QKHhgpMGu1/90o2Cg8aDOUjvaXkMdfeECpsrAf1DucqaDqvTCvIZ7SxlKtIKWGb41DD1FWzwRZ387tad8NWwrGzQ6CmKkSME8M45IGoN7oa5OkKDPjAFEnB8RAHvIqkgCa7o5cghMPeG4Hr3q09W432kVmwqEYwwTzruTx4uvZntf5KHLv5/QNlY8nCkgyZEEdG7xCrGkxTVQKMgZa4cFPlZ4Db+5mxiyE0W8XRmhRLlkIQ/VHj+u0FwNnEfNshIIBz3nCXRpx4phQa+4J2Mnt9QfgDUBpacMNDmEehRqHDbMxwA6Goi+IeX8zLq+EDoBvCo9RkZARNRHYpdqTBjB1830y4NqOqzMIKyX1trbYyIYDoeGLxOwFzzh0o8DZT1/TXzABlYXECEyomlq8XmexJzOJUcyqfXCZ3vP58YwIX/zOamKih2NNfo3H8FhnB8wNvB5yL/ZL6M2QDrRuTnU5/CYY6c4Mg7UeiFCTg4voxnVz3RpunHjsHo1yeeIcfV0A8qMwDOwxaie5GDqeuWGpGZ8OT2hoXkKcIajvqY2uOBxPQaAz6qLBEH2GvMEczABZiqegCWeCZl+8UqYTuA67dNqk116wRawlzosuqFp0Pc32I1ZOpvCY/z5sZvu7B/u2VOEOKtponFT4RdmZhE0fhzhiaHHi/llzWITSdo8vAM/te69e0SiqydvU3WOdUZnwNh837+/GZXcvK+etAQIuAEbPDFFzoDTO136YgcxMrrHpQrPo9Qf295Hlmg/6/Z3pmhiDgpLmCUfJfBMDMpXrY5tgmpOGx4xBZXpz7zn5epXvPKCipzy37Mqgb/FN9BgAhIUiMbvp5v18vcchFdy1ZRsaqKubkzC9zHvYvKee4/uTJu9OGev1YBvYPmPWa2gwvQjxJkw7MFHDjPXqaQ10vUb3hHbP8cwHOukpApPmRcg1X6+olIUW1IKNPh0WoRFoLl+btBZCPyPgKiYVZs9rTKLziOZt40RibCtDH8MJYyEuhItwZaPyjC2t+U0++XtPaKjESNgKknSAsPcijQeA1c1Ymtjk7AC4AanqOcV7Z16Zfw51rMr6hYiA64KndMBjEa2Dm6z9SFBRreIXofEvpNR9hPQOOTfB0qyfqmFsLYla2cQdBMfuevxeAars1JWlTk18pXMb8VhnD8+/sdJu5hJ76TVMOz/jscx4SC89dQiZNEkvjkgcKxqTk2kqmrvOQD8hTWvPypmtUtCQ7ILkIqe97ZyJx2yNygIBnHomgnt9BRgzGUzYhxGhohW/CCH8ZNMHAzq9Td1tAb9IcIqj6zPG4qK+kQzV32/g6MoaAOtpZP1cz4j4Rt9mLaJmoXEQzHOwrMNTm3APFWVloU97UVp0m3cjzoUVfBk2YRT0kYwTIZvcMhR2zf0U7mtwTgVovbjKQL0M0aGog4QDj2VG4yQ0/SSvHZQCbuGEz6VtatYwFYRjm1zro+Q+HxkND/lFqoNfAQ6TejcixS9q20dtbWeG58E+85voMRCxl07b6EUxFUjAu3onoF8uQf6tYeSOHH9R71BawmOVEOA2nd0f38iRa/GTfcadZiFs8mZUuSkay79bbwGIF3OXznjIL64UZfTboukn8x4SiVHwng8MX6RgSatJKtDowz1rAXTqcwVhotbrI2D1Aj08sFPXKMJAP+n2RiJmFKI4q/62u64JyC6BvjqDsqmKduZBT8fkFGkb+4I1m/uYjvONRwnv6o3T0GNabqKK5IQf2cBGcTb8IaYajbqdtE7DSWUM3KYSQN+mZeQhPagG3dq+zVQwpSqdwDjj2A4Qcd8F0ixGtKF+Xu7wvsx4XidvX3A3nP1QsfeOrE2Z+4xjnfBSRzyWGSsnrE28hU8Pz/jVwzv8qcPPtKirz21VcL5RwYziRjAT42md8bNy78bGNDu/sUVUNiJAYgwzK6mOWgtRQ8GFSCljpgVnSPxvMZo1iAXQy5gV0bcsRVWXvrJswgpWr2T108NoplY8Fem3fs27BGMFIYVy6UmBoqin+Foaab+pozKPjT24aJV5hsFsFaS02GhXsCNS7+1iM8vnb1mmNp8LTLIsu4x8FNmVzb1Nt9lGz7AOWhUpNaU1V+f8dwCwhySmRBUBwZR66XcKxthA48lxpu2cxrk1oVpZW1E8h92zQOAF+PtQQ2EtUEoL3mOWjukqWLy2hJr62u1zuPU2bY2f6oBTHaWHBaS8/yYX/NqULkR+rahqG7L1uTZZwnOVeXqPGzTQpg3fh4yPwjDYd38NdT4ENtyeoQY9raJgaNE4S7QbJGxI6gZuF28fFu/7ZOt7Wtl3lIbrTEjrMsW+2fdkn4JeHm0jEpl6duJyXmLHobZfIejcBMME4uOwDeShgHEnxo2c3aSg2FeXYdl7J8mdM6NwTylG/oGh9HGIIRZ03qv/KLnWY6xctTmynyK00zYG1X6XiL0+JRoF44JI9aooWRmV3Od9E1Zt+y9EALddGMyevrQmSUsT+fhzHZwabbhUZFPa9RtWIJoOGbUlPJVJG9wQfnJ6wHE+aFi9nU3n4qjnBMBDX7v+cxuwcsLjesCpjpjSJX38y8ZHYRgYuLTK6AUt1mHoKtmDpOdCUW74czug0LbZB9CByNdG9BqaG58VM0kdxkQ9DrUceww/Cm+NgpGOAGwITjngD/65AW/40JKXpuDroulUG70s12JL8sedBKMo+0TVpef8fbHNYNiIRluwCHIcQDwRefU1JmccVulaWaXaE1D47H0bBDva1rVEQzpTUY2InuLOoZw9GoU4YtjVgE0J+bj7rARGoYTMcq9dZIYiliSH00M+4f0wu2hLYelK1XTzm7ebEesYquMRgKidryxd1V4W8RzGVDVb0UsDYg8PMzqAYg36GXOSlPCYKk5lwE+OD3gpI+4PvdvVh4yPwjAAPRWYFCCbaEVia+7R69eNVluQezmwhgV2wkdRTcBu9tal29ORGxMKJQfzbFjz0bgwk6WnAvfCWJTXvpcRnMxY9MKiS5GNSIO28RqibGxOK3pKRZcDjwAAIABJREFUioLL9cvmsRqOYpkUjb2NxZiJNiXHse28MD5N5i15Kzr5XqkbJ1RkkKoe9fZq8bsZj6IzS+UzR4hBP6aDzqPRdbZGZiKVdteiKMFlth25xHvRilXoMUBKHtNsQ+QxTCr8Gz/HU8vcsxMTVRy5z43MWwMgvVDeDCeXejOy3baBbe/lEdfWbV5wN5zxU9x3DIcJL8so7RhVmdpTpFCeA856jdZvQj7LOBOHVPBFucGpjvjp+3ss5wHrw4ceOTK+0jAQ0d8E8G8A+Akz/4v62LcB/I8AfgDg9wH828z8ORERgP8WwF8AcATwHzDz//VVn8FMruR80Inf9y8E4KWoMVVop8kEERO1WndD2b9sdAS4lw676rAu5OyurS7lV4gily3denzcDDiMxJUQJsgkYAM0AnCm5OZ5cT7AOAXjUHVurCgnGsNjO2wq70oS173XVvTFHE9rIyLtMQgT1TGsoYulVOzZhGbw5PvV3iCHgYkAJOA+nyAdvbusfNpxPRy8xboxzPEum1HA5qdkixw70HzJrNJp5vlt60HksyTromD1Ll0Kksaz3x6e8TKOAG5VJ1RTyE5o2haTxczJm+GEMVe8aJNcIkatCY+nA350Fs3I+3zGqL1TKhu3gmEecF9rTSuB5XutLeF8HIGnEc/5F09w+lsA/jqA3wmP/WUA/xsz/zUi+sv6//8MwL8O4M/o338FwN/Qn186GIS1Sfqngjb0WCNszNRP/QoxJHPe8uwnFhntYzpI7j5sjBhKlHAKRg69lX1vYlg0J9v0vgWd+mq028lAIa+7j+GBhRt9YUXPpINgnZxkBsXAyr2+oTE9R64oGJzk9VhvPGYHoN4A4bFe9haI/TSiFuSQmqdnfX7B+Pbw7K4sACwYkFpTim7deBMAHADcMwqNRyJGt+KWzhrWmLaEZH8mVEQFqtjN2cVgviI9P5GBjFuPw+dQ32smBoidYGaENRszSR+LOCcjgLf56AfGkBoe13lTyHZuo3dJj8Y9UcN9PuGQCn4232Nt0pOVIMZhKQP+0c++hx/fPeAH9z/Hd6f3QpKC8ndItEtszswo3KYzDmmWOiJi8MuAw2cJ5yE25Pvq8ZWGgZn/PhH9YPfwbwL4Df33fw/gf4cYht8E8DvMzAD+IRG9JaLvMfOPvvQzIIvGUlaj8hMAU7eRE903qG+0bY53gRoRTYUl6pwFfy90QU3AsIXkRCBDvq33ZCSU7LMmduqN4TmvsurCpt9TiBckT6vZy/fvs8lQbOLl5EVkwgIdUNogLdVa9k3/fp1hOgLmwdj/zSANyWJZKdN1lSIdI1V8Mhz7dTCQrywhm9PX5sKo1+bVZBKVrqi41VW/aZPL3wvemDRe9Bo2fiZ37KYL6ojkvHEw+ntDy+ahBXGMEjyN2Fqvd7MSzsLcCh7ySUA/nXfX9KAGI2jFObHv+e3xGZ/lO0yq3dCYUArhi/ciJvvFdIO3w3GT0ejr4NIrNnGh2pJkoFcC/oQITt+1zc7MPyKiX9bHvw/gn4bn/VAf+1LDYJdsbdONDWh161lTgxEr6KFCc/fQTsmRVge4AAQXmd2YGPgGBJYgtrFhC6ivAUhA51jIaVwBrJ6hyAE/2KTbQroxFh3tKxV3EYmfmHvOggGPxsJclKp9rAec26Ddusm5+8+rtNtb6oBFhUUsb279DaPHMqQmnJG8umtq9yjK9dsp5m5tTMepM28b3DAJG65V6Nmd/v/IEWnc+2HMJLUZs2/SrUGV9+mjYk8z540h32enjIzm5fTMfijEehO51/KYNTAySvOx9tPZDhZTBKuctFWCaYBWfGs84rvzozRrBrCsGWBCXTLevcw43mxPe2MAxzm272M0bEA8DypJki9fk8rwiwYfr5mlq5dERL8F4LcA4PZX7mCsROOOGwJsArBdFDM5EAnAjUMEfEZUleraurYnTxc1TCQ/O77Q3Vy7YnMRY9MTsdrNgdKYU7aF1pHrPh2xt0Xhy9M0g4W8c4WYtC8ZBuDZCAshrDv3Uz3guR5cI8A8hOM64byqUagZS82oTRd4UoQ/ibNdWTqJn4cBp7x6N/GiHsZ9loY8TqJSbsNEcE8ADCyk4VxI0+1B38g2tA24l8yLacrCyWP/10YEcBu6UXCAmDtoaWK60ZiIEpcZCNV24F1NxibMak6Tbjl5GBjbGsh9l+Iqy+KYt3abFnx7esbjfABwh2Ud8EIMPmW8TBNe1t7PIqFrQ8a1IfibZPBu04KH8YREb0BVLpXan4zH8GMLEYjoewB+oo//EMCvh+f9GoA/uPYGzPzbAH4bAD79c9/xlXJuAxoRgLnXJ8SiJLxSGKXDK8/CKWJhgjx/y2WIZccOZIb0kBkFO72E9JJc0ttGBArNCHQMYesJRApzfMyute0eM+8gjoKMk7Z/sxDiqEbhfZlxqoN7DADwXCac1gHLOmCtwrKrNYGZkHOTmpMkngMzYSHGUjPOecDNUEA04LhOQp4ZhX34kE/+XYw+Y0bYWqhB05jxHrkR3bEl7d6ZUYjA4z6Umq+cN8YFsdO+6WMbA21AY8CRMnXjsacBGXBpeBcgWQrzUMXLISXBdazGDjcDKc1wA6a/0Qv9TEzl29MRT+WAIVdY87F6znguE57XAzDK2pqCwYl1HbE0/JBWweC+XjLCxx/VMPw9AP8+gL+mP/+n8PhfIqK/AwEd330VvhBHIsbaMlZkBe92JbWKxiYqm0YoVsU3bxaaAXdyM9yFpXXD7d8i2nLWRJe2n2S66Zm11PVyxq+dhnFxR5whjo2MGHqo4mrBLAYgZhmsyvO5HXBsB5y1IOdpnXBcRxzXSVO7ulDLiGUVb6FWEV8py4C2EtLASLliHCuyotcESGu5JpwFa60m1yVGcCRpJGwnYPSUonCqnI7bdGj/dyc0xVLyCOiO6nV0I9RRoj1fIRoFvy9hzt14g3Gg7fMsINprcGZiTG7kOijtilh6CM2UgCzreE+3HqluQowGEu5N7v0sb9KC22HB+zTjdl6wTBMoMY7LiOfwWvNiPUWNfrCZgXrIJ8x5BWcW5+YXHUoQ0d+GAI3fIaIfAvgrEIPwd4noPwbw/wH4t/Tp/wskVfl7kHTlf/ghF0HQnL65R4rmDq1t0HW5MTGVFYU69voC8t7iam0pvxYTx3bovSy2+WNGZLKxAaB2qVT3CmzxqBHw2okvuTM9JOkup/wMNSBWyAWhOh/bwTGFY5vwfp3xUiec6ohTHXGuA17KiHOR91hbQikZ65rRlgyuBKwEWhIaAy0DZWjAyEhTP7FSrhiGhnGoyKnhPR3w83yLd4cbvL+Z8f2bjE/yS5jH3vvR+CQxDWphhhkEwVUSUurhg1Wl2pxb49yY1cmEYOB10BZPsPffd+Iyo5MhWYvYArAw0BTMNGWuxIzbtOLA2/eCcmYsYzGTSBKOtEoRk4q89PJwkX570U0+pIr7NmkaUvCeb00C7r45TBiHiuN5Qk6M57UXSHVCWWeuyiHXy8Pv8wkP4wmcLXx6dfldHR+Slfh3X/nVv3rluQzgL369S+jDNk/y00OJSLR1O21izE2LaK+dPNMO4e+Sb4FxCDE43jPRwgTEVJ4Sj5SHsCE0ob+/pb6+bFTePsfcwegVxZO1hxLZeQMSQox4bLO7py91xEudBGBUQBEAaksoNXsv3NYIXMmNAhqBKomNK0AbE7gK54ByA5jADfKTCSk1EAGtVTylCYdhxptxxrFNmJym20Mx+x4gbGjY9jwgSLUHT8Hnx5+LK6/dnu5+PusmMC5IfI3dw/4YUJkvPIQa1o5lKhr3bMii92hCQ/E1ZV6rKCqNtDonZqNhSb1Z0tIGPOlnmtjxSA2fjCd8Mgqv42lYkZMQ6o51wqJh5YSOeYxYUTn6UTIOSUD4r5mQAPARMR8BOVE8ZRYWjPSI2Mb0Lbihe3kyMxhZF0LlBKTlwh2PJKB4czfXdCU0iNkGYIsl7FHu7ffbk2i2HxaNgmkkGptRBFYFNzBPwfgKAj5mHDXzsKpBqI1QtYVazrKpyU6QRqAiXg6t+n0agRujJYCTPMaJwE06VuehKgYBEI14zAf8fLjF8XDoNF/dxhNVz7eLh9C7W8XeDYLSlyvzfH2YAY6bN27sHg7sAVwT0IW2oEt6unbaOHirxRDfc6SOYViIA8D1L4wmLTKCqzfHRTAO5pkmajjXycO8kla9voQhSXiWiPFyGDGlilXnzbQbYhbIwmhL75tx8LqJQdf018QaPgrDYEvCBCsiN3xPNvrQIWw72eyRbZc3J1LnKZhSkIUke96Cdw0OruRWbDSwFD0Gb18pkBHVhy02N6MgvAsLLwZ/L/MUxCCIayrAoBTjrCxXZspYzMC6JrSaxFvgPunUZF5IqIniOp8SeGQwMUAEXhPq0MAzQATUVQBKIsaQGj6/uXWgFgnOEuyhoehrmpGTISXf+/X6VboBFaJl0dQzsKbAQA8LGnVRG2OH1nDfwEAj6cVhdOn++bvr0Z+WwkywVgGsylXtEk9AdfwFDajq7d3mMxoIT3SQFCwntDriJi2bKl3r4v6dwxNucsHnyw1MZvDcRvdQZ2j9EJuHCS8ZkGtpQGZwhtzLrzE+EsPQ+0W4xNqOzLTfxHtySsMluWiv/hsP86jBVyEFPebaxwo/c3ENzd5W3V1u+r0h2LP/5Pq3noO8l/1UerPdYOVrVJ2jogU6hYWH8FJHnFt2XOG8DmAApcrvm3oNBDEUzD1DIsQX9L8JElo0gFf1GggSp656xmcGJQYwgIjxNFR8UW43Bl3unzZy2aRmd4KsSBeGIGZ3/F6hC+BkjipRUgvyGvvR9CyucUbAybknMSQxHkO8Jxm7dKaBlrEDuaUTqSGDpNUfRi/vj3NzSCteUsV5HRBVuUw01rgI91kaxqychHfCGe/qTaBEm3fcRY73aW0iVbX7muHEx2EYWOrFTQ7LMgLmfgHYEGFsbLgD1JHrBMKkkyypxetVlR0R7/0MovJyNApGUrrGQASwsfg2tm3mep+Jc8gpO4sTfRMJ7bWz56JStcnMrU3ku5Y2uFGwdGRjePaBm4UChsZ2bCGpYaAKUDQOBeBMEpsSxGtIjEYZPDRwZlSS9vbPecK7MuMmL058Mgm06OKfmsTAizaItXtalbtywiAyeRSpyjEFyQALrVk8K1Kuit2r7TDQsaIrZqedkY5e32t1t6/5qMZ9EJzBKPLdjbdKSAFgux7kIRWtZxjxQl338UbDB6uzMLC5JcJdXvCMCe/LjJu04NgOuEtnX6vGYyhtgMm4uQFODE4M5G+kxwDJ00IKRg5pxSf5RSaRFtwpyQno2MN+GK6wlyYTyTZhzJ34soEJ0ElSdqMjgWbcfV6MXY2gsscMjLl5bURk3bwA84BiCGE6C6LYM+GsfIVzG7C0AS91xPM64f0y41gmPJ0nIS4tcktZPQVexThgjahb9xTGRzUOLCAVcefuuGEA0AbCujJ4JLQB4LWhHOSXP32595MudqeGa1kMOPHkIdCZRoy04pbPXiaM/KTusTYaBrvwb7+/0FSlMCEzKkxyP2sYkYlQ9CQvnPCeZV2ZOM9kgPYr92e/OgpD+1uKp2BVmxliY0eSYj5TsfLOURrCmudgNHvjLNRB1sEX5QbnNuAmi8GIGZxDEpr1bV7webnFT5d7fFFu8UW9VX5PknYH6MS6RTEIU7qixGgTvpmGwcbaMmpOyhpbvfvTXsh0nxHYPB7KfSO/3oFB6n0KzO3y5jQIfAV7j4Ab2HvZa01CDIjl1Zepyb0pM4Ng1OZNSbhEs56REUQ8uZdgRuGxHPBUDjitI17KuPUQmLYZCDMMOlW0kocMeYFTZi0tn9ZuFFRrVeL5ImEfmNESgVNCHaT45/0y4yaXHgoqCr835HLaSxrPfiVU9cHvgVxk8wuK9zN2XxIPTpr1mFtfebs2PGNF0ExVaEIcMk3GdpTPsdf399nwUfxebSnWycBNyMY2nQSwYRvDBoiU8KvTvoHubSSws37NCBim9NhmzE17SzRrZWcZrKGHnEzIQ5X7l77BhuGQtTkHepx1qw1gAMtEdJzBQRa65BsYWGhVeEantefLAu0CoePuPSKfHsA2Rrsyx4VFx3HipszNAGTxlqvfXzO4mK3937AFkwC3cML6Hp6rGIancvDwYa0dS2hMmmaknqeyzIMCdmYUaJVD3UshDNBbub90EA8BAFKRSkUCgQYAJaHljON5wpgaplS9+EoW9nnDa7B7CMgpLDRwFpXvNPp9tSlq6PewsyV7SOHvicuTPt67eApHD8+4J2ZczPPo95kV5Ly84ZnEQE7U4Hob3Pt+RiNn4ULbrC8FKPMq2FAb3FhIl7WO2TQ0POQTGgjHOuFYD/g5hIpec5e3NyHewlJEBwDD0HCeGGn4cOAe+EgMA0Gabw5UMaSG23zGTIt3oYrl1tYsxYZTS6kLedqm3jaT7UKse5Cy55kvPZP4endBCU6UATQnje6FgNPmJAEMvOrAmVWCxv4UVl1q7qAN80aE+DXgtI44lgmlJZSqMWVLsn6Ne6DYAhptjAETkFYSo7ASUgGyyTix8erlprQMCDWFHKpJiSRurQATASVhKQOe0oQxV0yh6Oo2zyps00FbB5lJMhhJNSNPbZT/N3kcDFQS5e6FtyClGYUGEr4C940dsQI7nQv35i5F09zx3hXQRox3z4aMy8UMhzwump5u7Ane6MarRqngRD2LZGutqZEaqOKMQYrdOPf2CCZWpOlIK9s2BuW53bvneZvOzng8e4WtNhnKDe3QMHwTQwkidu08qayTPPCddp8CsCnJNnLSXr+xu3Qm97U9tfe9DG04UBle689DB5r668WltVE4NHVRz0NAwmuUXRYPgnvJdAV5X4yunmS0aAPKDFQSA1FawrkMXvNQa/cQJJzQi6/islOTE1+8ByBVQlrEKOQFoCbXZVXWTEBKQD0QmBiJCGk0ViAJ0q0hxXKWDMX7FErLQTikFSX3e9h5DISzsv+y3BigqYp0htdYTAxUigBhD+eiRkWmuvEa9vqZwppUyTu6TkZrkHSnlOVvw4Zr3ki/n3DcIhmBiqXisqFIty6ytKKxFnXdBkC9tIySsmekgM4kBSB9NgA8kihBN5ZDoowDMHSOjKU0hfOQMA0rMDakPwahlj/2kcC4y2d8Mrzgk/yC745fuFGYSbn4yfQXFOBhdmm3WHCTdxiBvL/ku6X+TBrUOrhIvXP1Ra1/AKlaeC9AvJETZxQWoGex5cMCYy4wVlrDCGETLpz09O9GqXDGZ/UePysPaCDc59Om0tDGSBULdXrzsmYs64BaE9YiLfTAANcANmr2AauQmdKZkFYgLRI+5BdgfGbkwlqFx0irfF9OhDYQcgHWmVCVSloPEJ3ZRqBJQpR1HPFSEpZlwNN4wPvTAT85PODlzYi7vGBIgjUMSZv4WB8Fzig549BGtHzCSF105C6d0ZCQNGyIgK5UKNoGJ2UiNsV3EOZMmuI0Kn4vC2ehWKs4ixmRzrLcYhWZej1N08ezciDi+oKHOXAGpBU1zWnBXb3BZ/W+g9+WKRuAlzahsjAhX8zTYMFrDMBMkD4Tb9uEmQp+9/ir+Gent/jpco8/fTtgpM/cIzm1EV8UUZN6e3PC6Vsjhm+iYQCM1FHwkF9wl85uECwVNMEavthRiIsNLbJX1/LgMkbSkmtu7qpvuQTkLD0vBgIFUKq7m0Uxi2gU3AvQeLNS7afIlWHgp5VLv9QJdUy4TZL6g2UslPlYmVw0dK1iDFojuSoOnoICjrSqt2Clt5aarEA+AfksRiGtjFTYQwnJTjCSApKpiKGgKgalkeISmsakhcAW4DHhCElBv5tn1FFovoe8IjW+WHGnNiKl3lPTFB+t54W302MhrO3DPel1IcZhpOZEpBZ0FKIG9gZHgh0YWyLT4mzGV+6bGoduPHCBO40kYjCgFTMINSWto6geVhjOMFBFCibNvMhYaj0pkDumipIzfn3+OX52vsdTOeDzcotfHt9vPlso1xl344KHm68nBAt8JIaBFGicacVDOnnPBgEIJYR4SKdQx8AbXCAChJE4tGkIq2BRBeNg4KPu2Gupq4gJxCEEQXKw0Xo6WJfmSC7pArM9Du3GpsfchjY/10m8hAH9lKQt0La5FsURuJF4CgygJKBSzzwUBRpNrEPDBaoSQiQ1DKSeAlUGJVJKNEtooTzhZjuJfM0jZSAtcoIyhIbdqhisnx3usbQBcy6YatV+jk3DwSShFjccMeGWzxfZpsRJQ4kYRtDFPbFRNCrJtD8EtipU+3seS60r8zaPwhJGyWeHTMUVQLITosSIGbcmg13M5dRGPyiMfn1Iay/JDnVBLiGIhERF3oukGdJ3+R2+f/MF/unLt/C4Sr3KrPUZI1VMacWZBtwOC97OL95n4kPHR2EYgJ63na3zkHISjCCyl3Hz1ylA6DeFOmC4j/Ezkbr1sliKVrRFIMrSmiN16qsRagwdr0waRmyZfHI9HfewLIWn9WGeSII1HTXRWQA41wFTGlRtmDCmigNWT1WZFBsAyUTUhLomtDX1lKQVRq1mHNRT0NOfB7xaVEOVtYqY9ZL19QnqVYiHABC0IxtQFMjUk5eVjr1SxvMyCvA4EloumCD6EJEMJspG4h1WVawWHQ69h9BqTEhl65d1Vtrf93F3CMSDRB7jLWCptRIWXpjKdGQ+GvhohmFf6BUVvOJyte5nFhKLKlX2TIQt4DEAo/I5Mi9GwDO5gbu04HvTO/z4/IDHcsC79VbDsYoC6aW5NKHRvz284Kkcrt/0V8ZHYxhk07RQXt0pyYDmZ2EZhCsNTQAgZBGiQRAXV9y/jH6a2PCiGMtBR4+D5fVObQ4yY0DMimg/gx1QZu+7D2+WgM7b4rCOQfG5KbE2D8leTLOy6CnUqkZhVS/BiEtNYn9LQzJBSC5NMir1QJJ6HBic5bRPzdiPWr+vbr9lKYhIwg3NAuQkIRZpWNFAXqrNJHmkFy0ZBtTl5xXnLA1ZrDt05Ef2dFvGGKjpfR6zhmm82eSx85TdN8smWAWk0eejandzPCqsE7sWbOsiJvVc7BByQpU9n0J9BrGHMk3XRuXsVafWQiCrYMucijIXM8bUeTuRTh4B9xHSJkGk68/42UmIT5maqEhBVLak6lZwnrV9vSqqj8Yw9BSU5PDvtOmIkYAsrZfY4i2+QJjtpu2noGHLbbcNHDMLniMPr/dUFbOX8VrqsAOTmho1sFGvOXNPYQJbIpZVyHX3seE2C5mlcVLPQdzCxKyEldyzEkyoRl7SvxY2eFR1BWtqE4MH8QLqjWAHdSSJDlqfO1q54yIafritsn+Hx9JK8p9Vi5s0lbksA06q45CTgI+WwUm6SaySMM7Nl8n+G6q/NwqbDFS4fyNJL1Lz+PZVl9eGsSnNczDQEUS75205D9UnTEDMSKSyUmwAaJRwYj3k2BrvVozZtC+tNicaiOSpcv98NHw6PeH36dv4w9ODzOUgr8vKJzFPY0jfQPDR+kqcWDr0jlkazt7RgpmEIvvYZj9hI1q/79dg/Sdj6spxAfQUZkL3FKrmsfdqw5aqGoNxMH5C0pDEXh+BUMBCmo6eS7aiX6hzM/RkvM8n/PLhsaf0Am9hbZr/Jgl9mAmtZnBN28zDKnwFQE5xWsk5CXVigMVLqDcQDcA3mqUohOHEGM4NOAO5KhBZWTwCANyANCnZiTuQmVbBHqiq99CER9GGjErA05qwzAPO84Lj2Ful3Q8LHoYTrHCo6gLetLpX79DpvorniMZivTAK18xJAlwa3v7f0LMRcdj97nqP8rg5ShFXMDKUf6aGqWe+ZONalsmrdnWTnnj0jFus0TEcZl9N7IV0LL1T7GD5c2/+EP/gJ38apzri+7df4NPxGYe04s1wEsIUMR7LZfuALxsfhWEAOggXC4cqSTFUBiu4J7dhgnXm6aKeNvFWOGPNZGPzlopuNOxUAEd9hS0SHW98hbiHEnMykgKLCb134v4kMgtvdGpvYGK011068j6fYbqB+yrOjcGwogagewgMgMmzDqlQP9n1BOfEvns4yd96kHiprUCrtJWyNGzC3qcBqTLaQGo0CJQs80aSxcjswGldCVwSShrwQpJmfZ4OGFLDIVXFUYJyFyek1EvVJb6WrWzUddCWg2JVlnu+gW1uu69x2GGRw3Nc8EVBaoTfARZS9Pdraihm2q6dCFobn8WGhUa2BioSDqmgVGu519xQGlZl/Vf7dQweYsyp4DvDEx5pxs1YcK4Dfnq6x432WxlTdU/063a8/mgMA9CpowLMSW+J2ZrPYMXJuh+pdY0t4my4VLuBV9yNBQAlOjUHt8wbsBHdQ7nZPXMxEpAUtY6fbc1S7ZSPNRSAUnspCpBeunUj9Zt4coJK7u4sb3tBbEbINuTFPAf9nQGfi/yn6XM5AW0ApA8N9dCDAaoJST+YkxzFUtOvb6nvYf+2VCirV5SI0Ug8GV4JjTIWneZzHbDUAevQv8dI1SXVsxp7Q5G6onL/mZyG3Keg8uvl1zLn+rzwmIGUFmruD4Y4Yn0GIKHjwbEwBTLJ1L76a2ow6qbTOKKiqic8UQ5EvS450I2IHpSUFasSuX7zRG7TGRWEbx2O+Ox0h+dyEO9gBG6xeBerm3xNgub18dEYhhZOCes+ldqIExXc0ercequQsxHJJzF+tLqIWI8PbEHAWAzjHkR8DOSpqhlJ/OmAUrtLquCSYQpWL5Fg8WOndAMdVLU40nQoDqnfvMJ5g2onYjRtkw4AXgqpYQrp6qYiBiIV2fh2uArzUZ4ujwve0Eb5Xa0ShrQB8jgCrqDDPRADJE3YZUUvtGpQMRhGKwQkDbpaxrImPN9OyKlhyivu8uI1MbbQ7btLw1zdDMGji/09NusHvQpy4znoFLXw/6Lg40zNjUNlMxpbb8EzFIHRmcBOqppIXpPVczGdyH2zYAAbMNXYFSdY2t26qK966PTU94kW5z1KAAAgAElEQVRHZJW8N9B8TgtqI2QNS/7U7c+x1AHvlhmfne/E6xgaPhleMNO2N8iHjI/CMHDYvBZONAi5qWlKcd9LILpGlj2wm1kgnPjN+3EHf7ZNR3osmen6kWMNW0dF5CVLYeGDhBgt1MBXYqQNKMoOWspryONHIa+sDi4laigYMKeyCSmil5CIpZkIYVMa7af32g1BnftJz0n+bWEEEjsOQZVQG4MaIZcQIvjzpV5i81nMQBOcouXoNWgoUwGuLLlMCJZ0PI+YhlX6XIyD9qvIThuOPR6FIr51q2NPkb2U3lW5N9pSpAVIFM/upDUY7Qonwd/vyijoKeaqPAcDsnv5t7yvZdLkmthdOSuRntQYGH/BAXDJG8Oo8Na7w9OeWo0qQOyK703v8LPDPd4tM57XCVORNnVGr44dxT5kfCSGAc7xNpKGtPeSyThz3hCZYien0TAGNQ6ZGI9aQLIgu0CI1PqTb8CJxRWcIoAUhp1SHVcggKSdSsxuVP9pxKWedy8UFpD/ntzwdWyjF8oAwJhWtKZxKCWgySZYubeUIwPUSE//RFt+Aov7D6Ar+OhfzgCS1DmkArQK0CiAZFsFQ5CMDEuhFHXPwz2GaiC9YA1JQU5OAA1Aa+pJSG5YZoGB83nEe01hzlk6hx/SqgBwB2QXZCRuvSu3nqp7qngsdGv6GWlnHGKX6xhaCii87ZhdOB4Ue6OyJVctnKQJs6bCLZ2Z0bNeM1U8h+5bRn4y0lNLBbd8Dr9v4Xny/wIB5s27vEsmOT+iaA3RIRV8f/4CPzq+weNyQMIN5lxwngaM/PW3+UdhGKwg5FgnHFLBQWsZRmiXZsp4SKdNsZO/Fp25Fuvy37dZshwq7z1RxXMTkseJRiw44oEKZmp4SOSGAP5e5CeJGQdAvQYFnio6yGXYiNX+g7WbsiPbQp+OreVsyOnST5YZq6euCmc8YkZp0j2qsTQApsR6/APEffO20bwBfXPdyPWGpVoyQ/QcBzEsbSDkM6GdCXUC2kBegWk4g4OQZlwsI8EAVmFPmhfCicCDGAd+ki+/3gKmB9CWjBdMaE3Sh6sa8U/yC3JmHKhswUj0BkN7ZqRZqz2PIQKKGZ2patR588QWJPEu0RzIvlCpDgeOVe9GQtxZ8dwRndNgRDrrsD2jblPcIaRI6AVOsX2febcVhFsSQ1AViLQwdKQVz5Au5o0T5lTw63df4B+XX8ZzmfCj4yf4pekJCYyXum+l8+XjozAMgCg43eezCEwkwwN6WmfkikwLRrSNChJwiT57Dpj7v2ctpBGilPRlQAIaVoxctb1axxUsfACgocKWGruEz7bYFwx33UC9zbpJdVXlaOzpWVsxUWVK7gg++1CCQ7YgYhGy4QlNcQBb2GgAJfMu1DBkBpACICcVoakQeGW0Sm4UHMfYR1thnUsoocAjBc+i6SUSHLhoLWGpGS+rKFF5/QB3Y2pNiYFLwZc+d71lnaWcbUTnOXYP73Palcbtd6MWVhkoacahKZ1+m3IWrGGi1g3EDsx+bexlCh1sDT83mNSOmBKBbGtTCABvxyN++fYRf/j8BqV2FfE9Q/erxkdhGBiEUx3wVA94M5wCXbgAO7fxGjnF3T3PNIghKDR4DDdqMQugAikKSjRKSDhjIslUjGCMIGTKjjbbMOMQGW8ZUC0AO9XIac5C8e36C0Zsiu3ybCEYABmHleruBWYrE7gpDVq9BmhmoA2yYDmFdCXEA6hZMAGQGoWhiXYDEppt+kZIB3jRFDF7xsN3Wlj3lpFoNv8MwT8SKQgq4QpPBKYGyuyhUG0Ji8reH9sEU1Y2TsPmOyvcu/cabE0kJszJ4n5cGrDdECp8J6P5IYDX+RBRDDiK/FqfiWwgi4cyWw0Pu/Re26O4im7shObyflUxllGJfh0fu+zb2Vjk/yonHNKKX5kf8VQOOJYJL3X62hkJ4GMxDAwsbcC7coNPx2cc24QHrZN4Q9If0QQ/bAhYEzYt4ABkdLUAeIGW1ygA3vJ+0QrGGRUzpLDgWsrKDETC1psAxDhI92Xl/OvjhbNXXpqeBACPFeX9eqWn5+rtO6n7u7as+IK2jFP1Z/cWGm3bkHEXZYHOi2UpEiBCr9JQARgZbCfzCaADUIvoLaQ1vFdDr8CMHoS+v4caBn3Y9cS/BOmTOTTvlyn3TsKjllWKjdJFD8l+HySE2Kt2yXz3TV348rX2WV1Advska3C7H9FrsC22LcmPoj07nAJdjs6wsSgqJM9pwTtNjolIQWBCBUMUtc0YXXqcpzZKRS6AN8MLvn044nE54EcvbwDA6fQfOj4KwwAAS81IGOXkaNVLVO0GTGgbd9o2f/y6PaRQiqluUamNb7jDAiSgNvK4rCLhi3qLN+kkkmwmrYXmoUNVICh+1kjbiZba/4ZCq3sEnanW9R8y2Ltxv7Z4bVjJtVQl9kW44TH4RpTH7AT3oZs1FSBnQhsZXIRfwCMk9k8QYtIEkBqFBsEjrCozVYiRUHaQGQKJDtQTkShGaNE5hBIVsFJwANrRSrwG+162uA+pKO09fZD7G+dFfoZNReZBXLoPSYHBuMEvn3OZwo6SfwY+++e7pyKY0Z4BGRmy9qteDdw0E6dYgn53O7hEBENSmQbOy/dvuE1nvKObkN1hPIwnTLnis5dbNCa8mU5fOZdxfBSGgUEoTW7BY5lFviqLIEVJA0Y6O7kpajMa/yAix5H9aHTamar3EJhR0CjhGRJSGNYwsbAprbimgjcuc8cvCF3eSx4tqBKCKPcdgObhBUUv/lqG1dabfqVpGNop0H92nCWHxb+2hGqybUwCPOomTSwhQ4xI4uaU4j6xFJxJlIrH+GQxBm0UPIKUHmjr0kBJSYdKSTZnSUMaMOkkKE19OufBkjmaZiUobdyp48b47OrYlS5l+GxurnkUJgFnmYcNLhNDQgRqe3iOhQdGr75gQyp+0Px9LAUZ0pHYGoSYBdmHwpFf06+zbTyDwoKHGUaW9WYYXgbInjCPtJLM40gNN0PBz59vsawZ9fYb6jGUmnFeB3yW7zCkugFnahZOQ4YIjN7RslkYex6DjZlEhfeBVswETYWueJsWfNEmfFFvUSnhxKMYBpbnF24oYG9uGolOCQkjZSQ/ocSA3KWG1ipOylyT+FCp27z6hjd228wFzyzyZpUTnptmYNTbsPJZE/IYtD7D+1FqRSXMZbfUog5z96VmAiLhdrLsBYEHwnqTcPpO03Qmex9LHlhVmux9BK9oaiSsvqJVDS90RL4EAK+lyGfJVHBKUuPBEZVnDEmERcxrkHlSTyIJHmTFczI3HbH3/U5GZuub0zIP5l3G0mursYjpbkExQspRU5AZhNG8RzI1LlZOhLzHiM5hKGCclKkar8UMhDVHMlakdUGzAyShiRgsS1+KYzvgfbvx/qATddVoW1eP64w/fHnAY5nx6fyMBMbtsODlNGJdbvBy7t2yP2R8FIaBwBJvti3oVNqAUxo1K7Eim/oNcu8ipPGcqPbI6+ICkDSUPD5rGqmA8UAFRbsVl3ZwfkExXQAIkJQoZCnQSVCZEiqLsgMgacyRDNvoGoejHtcjqmtKTGioxHigk9NdTU4+CpJEF9eb4aRm3mrfFMo6FJozI62acgynNTWIglKM90/y4jYB9SDK0Mag1BujuIGECmReQYY2u1W8IkqTxwOQdx5LlXqMWpOoNulmWVt2zcNz66egMSD1KnXepeNoFFZ9bUSB3ngyXxwqu8ficNal15R3wPmkXoQZhaTPy/rEolkLq5lY/HDY3lf/qQ+bALJRwwt1opINW6+m8wgAn59v8XQ+IFPD/XjGnFccDivW89AbDn3g+DgMAwE3gxSBTLm6O2nFQxIvdoNheelGlwW6sWDJ2qfH55gmQ6WGh7Sg4ogTj5vCrRNnjMpBiAsm+R9CCaeeLAopIpIGJGUTH2/asim7L+lOnlMR+XSaNs+91nJPXs8O2m2APblAMQ6ZNwvZKiHtBE8qyGIeQBsIdRZ6tNOoyX6qlDxJpsOMipOnNBNCweDITQ0X7Z/f1Z2sXZ6dpCtLzwzDhrqhHmDiNoBK/EHUkYxYtunN4CAeXcy9XFdzTsNdahvvYAThxFHvCcpFCCFJEGqxXpnRszD+S3UcQVSuTbhFwNUt+GlY1LVhbe1tETvgyb1sX3qTMKZUcV4zjmXCnFesqeF+PuPlZcLwzZSPly9laK0x/MxKTrpYTPMxWlzLRgDYnQrGQe+fYzev6uN3WFFoESk5df9dMcoKYiBodKRLN7B7CjYyCBORg5CNimQkdiCUFANVR6UnaFEM9TZ5Z4sXvaIy+d89wYtidiIMoz97mowtDFCNx1U2hHixqrI0AnUk1BvZ7FY34ZaVZBIZcA6FkZr2uXtnTHpmRIFLJhWZ4Z5hYcGYBmooSbyHljoA14HbhIrmgFxmdvc6DgshXLkb21NaSvDF4zOWpGNIiie8Ro+3YbURZijMKIzIaCShaGU5lCb1Bk7oBVNRu7KqJ1mpYUKvs7Brn6mIBirSNoul4LmAmoIpvJnPaCzp/0QN35pfcL4fvrlisEOqGFClJ2MdcM4jGs7dhaZebBJ7EL7mSkq36e7m2bC040QS6wkw2ScdAE484JZXFN9R21RWQ0PhqimkCHABt6SBfeul3oWHwJe/5KzvwydvaqsLo4SKyrUl72JtwKNXYCfVa4SFFT1tmJV1a5eXlwa0/ngbSdScBkK5IQ0vRBW6HuS9bKNT+AkHHe1aFOuIhon6Z6PBe2k2hjM57UAwr6F36hJlcL/n3HtM7LkOZjQ34J+773UTeoiIrAGI4T2grNfwmAHRZgTcI9h8NnVgmpPQu0kPEZb1OLGFDVeqgqlhYojwra6Z2DF8DOvmsd2oQbBq5IybXPDp4bmnfnWN3I9nfHqXLytyv2J8FIaBAFeYOa0jTHvR0o6m2LQVs4jl1dtJvqNVlHvUPUyvWP+EBqSKT/PzJuvROOGsoUx1dK+hYasO3NAQuxXPJATcDJY6gMY4thE5yJdvRGZAF25k7FG5towhVSxt8Dy010kog5HQ282xbdgEtEkMWjsoyUhVllaQ8xHyuSGfVqQiKQNOBM4JPCQ3EnXOKHcJywPh5ZcIVz3e18L8WHTVBARFSeCxgTNhrRmlVZSWcaqD33O77zYe0gl36Sz5fDewKupCzbkBGbxhxYpXISGJl+qDNwYkxWxDAJsBRnFVJmwISjF9HUlwgn0YQJo0EyKgcWHGrVY4WnhrqdW3tAhWQMm7kom3mGFaJB1w1TqbRgAO0te0jRip4u14xN0glj72OP3uzSOW9mXdMS7HR2EYGMCgN3zKKx6Gsyv9xGFS4hMDBVsdfwCeux6D9wt0ACnGi3YzreFtRK3rzl2XklzGGIyC/w5bujTQ3dERzRWoEmj3usBx4N54BpCb+lJHMQyctKuQnVYkHIDc28hBAUMCNqED5/6zaS8INKmPEGYkI5UGWlaApRgKRODB0gqE9pKRygBqGee3nX5lICVpPwqEzyZ0OVkzVl6ZyeoxsHg21b2FjIEl/j+gKxUVEwTmDj4ae2DTvVrXQQQarS7BNCKjUejkJPTXqvCKbXz4J8ENh43kv+9rqqBu8IgMwx5s6rdNkKSWzbIinfRk/SSiF9t7rAqrd6HeXrGGQ8M8i8Ow4qVOKE6O+3oew1cmN4nobxLRT4joH4XH/gsi+mdE9H/r378QfvefE9HvEdE/JqJ/7WtdDDV8Mooc1W1acJvPsM7BMb3jlXIaK1rjl9eGl8RuMIiMkSQUGKld/P0yxRsBm/bsM0lzxjFRw4Gq4A7ofy1/7d8pvJc0r81u7RdtTmo3VrpKQ4qoEoMzX9xFTgCPmj0YgDYKeYlNSMLjfoCWFVQq6LzKv88F6aUgHRek44L8eML0bsHh8xWHzxnjEzAcJaMxvADDCyOfoVL0Cm6uAK3sCtVABydJiU4STkhY5MahyXc/VxOpSSgqWPPcDrBak22oEPGbuCm7AZi9KjMYCHAP9UCu6lRYTvdFvYXXIvOYvHF1L/23MRUlta1rDMCB4OGt3bJRH0uQcNWyWtYPw76jlaEDgjncpTMe8gush4RwF6p6R/Jd74YzxiQSeOsfg8fwtwD8dQC/s3v8v2Hm/zI+QER/HsC/A+BfAPCrAP5XIvqzHBPXVwZB5K6tBPeQVtznszdeiaKYoLbJFPhioD2JRRuHEHCIqLLFnKSTCVJKaie7bDsbs6ZDtxZXXh+uA7jwJqTiWPUagjGT644A0jadJi3osv/sYUTDoCnLnBva2EQhaWCkRp3xaHhk7u5UHcWVb6Wf3mSVWPa3sgIHuuTXCmoJ6aVgSITD44is1GrAshzce11qZSYnIVs5F6Jn4kTVSYuzIs4QRyQnGb8hg10P1J/HxsLq92oDQAfikYdy2BZCmZdpnoNWifs9tZ853N8oCBPZsRscQs978xoQJOOAQJ5C/7xM4mUKG7I5db4Gmn2ihpkW3GFBSScc8wuObcK7duPvbeI3I1XcqSrY+1+05iMz/30i+sEHvt9vAvg7zHwG8P8S0e8B+JcB/IMve1EixiGtuMkLDkn6Vn4yHL0j1ahxe2yJbqPn/Xur+1hUtbCwDUFyk+yGyR/rqo0ea+pwCitez3EnjRSbItBl8zvlWKA/bkh5IvOAkmZDrvfMWFtCI1OGlteRpitTloIkztz1Frj/dSm2LJkAHqB/qZdfX8NeWnMqKTUGtwZqgq1M70bkc0IbFPzULAcgRqENFqbo/0dGquTUahd/WQmtZKxjRRkyxtw2hqCGsCkyXbcpa5n7hO4dRFXueB8Nt/DaGv0RW985i9amwT5Hn2tNc+3eWk0EFHe6hmU5x4WSeJMhvRkPkWteSfzOsRy7KfiaacVDesFzOuA5H/BY5zBfPey6G84onK6G5l82/v9gDH+JiP49AP8ngP+UmT8H8H0A/zA854f62MUgot8C8FsAcPcrd7jJC27zIiFEWvA2H73vnwmnyiLYIssxbjT3cE7bSbeUI6iHEzHdOBPhFNxBeS8AwVuIGYmMrVuWFJEHev1DXKLmgRimYC5i497ANF8xevYdFz2iW0D+UmJQYldismpKWrHRXzQ9F9NLsDRky5KFQNLjvTHQGqi27kGgn/Z0LhjfF+RzFlIT9fCAE5CGhDaqpsPYu2PD2JOw1CqDWJSkuUl/jOgxDCGUG5UBa5JnCds52qeuo8cQQ4a9NqjR5uOINOhrtYi2jhxbsNAD26zGXtcjPr4Hr6OSubXVMw0HkzJctOoyhpsWEt2h4G1+xolHfJ7uBMRucDUoAJhTwdsRf2Iq0X8DwF+FrJm/CuC/AvAfAVdn5Wqwzsy/DeC3AeCX/vynfJ/PeMgnfHt4wm064206ShjhVveyms4maN/P0H+PHlJUMGYkmDCrxPkGQJKHIraxs8aCsaehImdXPysT4dR4U4Lbrb7FtMkl8k2ZqJflShrWKNDSSUhAx/PaEXt7zKTdJP+oP5mQKgGr3IlUBKCU+geWGgsSbKAeCOtNxjAPovZMBDovQK3AUuwmAQAoZ+C0YCgrkDMwhK3ADB4zeMxoU0Y7ZKw3GfWQwDlJJoTUi2A1Do2BVSpEa03SVYt7hylzhS07cUgFE61CBYaVtTedr7Z5rSl2mxzgZQVlb0gsVbsVt8H470NC4zmc9SHDCEDy2ElLSkUzt2NPBcZ+7exZKHbha8bXjlyXhZ5SxFeVB2MUeS27DsQ9WfdHjGPFY53xrt7ihK70lMC4TQsOtOKLsYcaHzL+SIaBmX9s/yai/w7A/6z//SGAXw9P/TUAf/Ah72kt6qzD9ZwK7mjxVF48Ha4xAvcjntgRJALkxrfg+iWStJWhx4W3p8C2w3E3Dp67NvRZ3XkLaUzEw659QsVR3/PEI6ygKulZNFNByWenBZ8VhAS2rD5APAaTdtuM5jYCpDUO3KACLvqULOGEcBcSKDc5+nQ+uOmMGYiiGQs6EzA0oKziodjvSwKNA2gd+mMMpFtCy4S0hNDGwh34WwsA6UVUPYzYn+o1EHys4zmobg6Igm048Rokna4YDEtTTnSZffJr4G4szKsUgdlt1aR8BwMiCdd0JR03CI/5OqeG1gKPQdO3xvw0sPJAFXdY8JBPzuC19zaNCyPPfZ3xRzIMRPQ9Zv6R/vffBGAZi78H4H8gov8aAj7+GQD/x1e+H7BxGY3pZWyw176UuYr27/0Q6y//rjAg0ZqkdrBof9MMH9isS0ed4flucM9QZEi7+4mEArvvuu2l4LS6xJxhDZK1qHibj6hIOOcRFQkvVWjSOTUsq/Y7rNnpxJaeZFLQkNFrI/T6LIyIKT1ilWLLlkok0JCAqvPcgmfEilHkDGpNvJFEHhYAALUms58SaG2g0pDGJNmJJlgF1S046lWWvsn4S+7zpZdmqeuR68ZrHKn3pfDXU998DiRCtRFgNHk5KITuvsUDABHwsfDBOQzq/PSQgpGCp2AH0T5bFUOS/h37SBoSmehrrBT2prz6FWc0ACs+zU8AgJ9CQtbjOkkP1ESqp/oLzkoQ0d8G8BsAvkNEPwTwVwD8BhH9S5Cl9vsA/hMAYOb/h4j+LoDfhSyhv/hVGQkbiUQtWTav6fxJf78CwryLJV99nyuPGZHEdPnsxsTS6hZe3zsay78jrdr6YMKLZYxnL4IapvUnr5dT3gxGQ3X6tbyuuREEgIf84my2ygkveUSiAUPSbk1NFYNjH0I7oAlbABIdAxAcQMqgzRBwwCV8xJ3K3D2G9M+pe3sY25Jlz+sXmWutvavqnO6+H+/Ne/OlJxASmGNh4CBhgTMWSBiIQUjjgIGEwQh/pLGQxkIaaQxGQgIkkMDAASQMDDAYITDGGIRGM2/uvHv7dvfpc05V7b3WygyMiMjMtau6723pvqfTKZ1TVftj7bXXyoyM+Mc//oGFGO254YKomkGIQowpIUVJayVvSikuM+dl38jwD5ouQ5Daom6iYPyNqCAclaKRYgrLQJFR8MSzDzdchdfA43PQMIfR8IEBLLT76KxXv77hNYT7v2kHIqFaaOrjwKAcRITH52doqdFGzY5zl903Hn0RThtL047yRXpq3JgP5Y6PYqX87/c7Nk087b/j6kpV/bdfefjvfs/r/ybwN3/ISQh68BhuQaWwkrdU0tfcwduft41KixpRqd+wl0bmpTrwLdYAVnk5CNGSrPxWS6uquz03K+za23dMwy6QUb5IFmhc6kzNZiCey9zKru0L5bZegwHpfuVxwblBiFRiCv1G/7vVUqia++/GjvQDXE41wLKl4lRhNwBTNDlvoZdzBz3bThD3GpTs3aiipd9WM3vNlNQpv7ZpHFu3NV6LG5aeknyJ9LeqWP87yqPj9yirTiRKIypFcyHp6cphbtxmKYrC6uHEeA63re3s3Pt7y83jQdwLFi6Op8RmuaqFVE0NW+A+bRSeKNkA7ZNYluJaJ77dzn8qPIY/kxHYQmg0QjDXKiaMWo/8gptdYRwHgY0GKPbXfRdpJUZY4tGLGJ8bDUVBO3sNq9ufUdYBa7BzMfcy09lpkW0xEo6zL9OFx/zsxxZmObHkwloLFJhTZZfcYvM2fAOU8d9gFWXvjwcOkYqa67/XFhZYWJHR3c7g1RETvfr7ir1OUoIpW2ajmMEJnOMQTiggof34+kdsmthrNwxjF/S4dmPLt1EUNu71ayYusITknpwdS/q9pGcQbt8X4UWkLnsPifaRDew+LPTxM17BGo5tEYcMmoaE/PE91UlZIy/iLIUqGyU/uWZq5mM58avrWyOO/VlgDL/rERjDQ7ryNj03gZWmuSBdqmvMOccYLTaEax/VbnoQ14hQoO8g0irhxps406mxI1odkuSxw5ioSzWGWwMiLXR4NHWSloIC9xrSxod65lJnEx9JMMuFkxS+kAs1W++AWXae0olfzJ+zV2O07Zq47HbbVAX2ZC3vN2lNXsKrb7s0N0ZhAAhJgnpWQjPIPMH5ZHjCdUVfA+EGD0H9pwBsAjkhSwchm4EKb6bi/GDxQzkvY8hIGAsyc5W5KRy3YigvqIt8fdCiVxL3wGsew7gjj2MMMTbse1TU6hwcbwj8KYDqkxwxg7MIm+oLYtSmvW/qjHLGvJJWoHVzXUevJDlAVGTniZdgu80rDu0Uzs6wPbM1vOGpnvhH/OzVqtzfND4JwxAjuasYUmzhGo756dEo9HjMRuFoMKBzDKzOobzYA4sbhfE4na6aBmBSud3emrFwAPMFO9KNQegBdi/HvISLTka3qpWL61JG0UxBWKSw+Q0PDYeguxoAiaOL9nnBWWhhRDxelbRKl3kfXPo6Z/u+JZkHAFAmZC8wz8i+90k8YhDfNeL58XW+ozZlp0qvlxjSi0AzgJsmpttwbKAJt3vgr7GKzHT4cq95hpFZiufGnbzgMbt2vc9xdLGefKBBR4Pb19iMcU7mKfr3gEMFZ1Ft82hsqmvHGmp4fB5FWUD0wTTeQ2qvD1X0z/NT60D1o6yuHMdtSjKmRrSsH6mrHUT098rr7mMIt85+0Qv9Ro6q6KPLd/te4zg4AObPBduxWXl/Pkq614EKvZEcqY442NmPaiK41hhnqCJVbammKRnGUFNhTqVVohqwKI4ZeKZBpO8v7sKjxmkYvQfAG9TY9xOpVsZdQWYrnRZ1lC1ITzHiGkWmwgFIK+IwD+Q1LYionQjPRYu0w/bWdHbsvWZWUdMCzd2YZqnMWOxtOhadDxIsxvg7RjBT496+BggG+DeGia+HFD2/kUVeZBxuR6ha41mr2TMgEba89u44z9ZER4/fJ0r5Q016ltIM/agVauSmZy5lZt1O33uet+OTMAxK5K1fLuvbC5duHq83z2WOxqE3RDVwsFKdx9Blvps83PC+yD/He43zGDfH9f6Gc9i0l+3GZIvU5W2dxCyFc9pY1EQ8N0yU9kFWNmj0Z/tsq6g75Z3oJD2lajyG8QJKTxY0yoOYoUhVO8YwLh4K0RUAACAASURBVFTfxXXyNxeQbNWVhkuqeQ7totQbT0CM8DT+nhNMCZ1To1w3gxT4gmIZEi+k2pzghDi7c/hq1zo1+vg4egiRW6o4rnEzDtqxg+/qhD3ewxE8nJWDUejdyDqlPtiztyA3dKxgxrkVmihSCNm3PMylMfsxsmxn/7zqxqXQ5eFGJXK7uLapHsIuKm/ylbfTlcuPtRNVSKwHkNSARjmCLxG3EaAPfWHHGLtJjUVTm3aW46UZB8MvFqojztoa1o7hQSJ5/NuR7BHX2LBYctwFgmUXGYpotGvnWK31WLKbfNGZR11YKA1tB5+GokxSWdJOEusWnVKlKdiHUbD5ZwsxEc25D2FFvL7OUIpQ7jK6OTtvqw0zABDNtrhzMgNRtXkFL8ILEfQ0GwNyth4Ro0TcGEZYzlSbYEtIvMUIoZGqwsdy4lpnarocdkO7P73JrXkT+sIodKP8Os4Ax80lwkMDGI8Nh8IAv7bPj1TnMeMQzW2bt9verQ2TCgM0GoXs6XX7vGB8SitD3whZt9y+f6TlY+4kqfx8/mgpzP1H6TGEQvBCTZZ+2ki90sxTNGNjj1D37W3KOyPtu0Y0ikkaPQGkL1a13RnpqLLlqg3uj0o5xLyDcRiwmViVpj484iBjB6MYZ9fx22puFXTWp3N39abOZ4i6AYAl7awpM3lvhibYkrr73hZiakmdfq2H0yizsJ+FNImdr2CpRumaDCJ2wLYmbkItTWJhRE7oMlFPEzon6uxiL1HgFcZLpbkQnTLh3AwPkYomqFB15rnMPJWFS54PBKiFQmkpvPoilR0y8nF/RjyofX/GsLDfs25EtL0vCHK2saThGD0DkkS4qB4azthzQYk3jyS3L6+06sthHISH3VOJ/peGTc1cqvESZukt/UZJ+Rj36crP54/8Mr/lh4xPxDAYffOr8oYv8iNv5QJhCByIGvnws9SGNcSI0CAu0mtgYIQE0bW6Ro7PK9baMZuXZ6FHAs7Sc9y3rmPoQgSNd5ZjTB5p17D6XRtgI6dHHmXjojPv6/kwuS9q7t/nnr4EmFPhnHcu+8y6m/qv7brqis3H69K8BcWbxvjjbkDWNwlRmE5CviZmgTwSn6qasdhm0mU1zyAl80jysAdPZgzKeaLcJcopUWajXdeJHlY4LqK5krwjFXinpBofKexOBX+33nNKxapuxSjzOSnI2rJXwYAd+0r0e65eF2E6jOHVBdHt0JPEF/RrKUfLMlj2YtbSslK3eEUUYUVXLMMVtNfeuGcRxgYZiFU3I4zNHMdJG7NWvqoPROu6ACUjpOqNkTdIULNtLD9dnl79jO8an4ZhUHE5s7l1oAo2YMhvQ7e89ruN3AxBNw5FdHAD7UJlMqHH19hp7hFcfZmPO31MBGMtGsh0klvb7ucQoNdwfysDY1LtP3P3+kw0Gq9LdjWOuxA9C9vr0s5Z7XrUbASgU95Zpp11ytQpeT2EtJ6TmuiajCNuGHwFnAU5QQ1an0JeEmlXahY3KorsVk/R9Bz9IunUcQSSUJdEXcIoSDMKdaITsOwkzDsRS1emoVVdEJ3snpp4y7VmnurSxHrPraVwR+3t/h2l3WIEABjzYXY8KQrmRmCvZ5JsLL5rRw0N7pmOWNZ640H2Y/bU+gwNeJylp8IrPfU90qhnSQOgGKXd1jn7i/TE2N2sf2b/7pHd2tJEwXpa/pDxSRgGsEnwVBc+1DujDacjQ+e1CsrbLETcjFsCkjWaPXoQSYS5eQcv+1UG4ouGJFivmLPPtvx1nNXY1WikQsd5WqjiRm1we8M4lHTlQ717VUZ8lsLJDcUmmbu8cjdtnOad6zxR9myeg7e31+plzQmrh/DTbqCkL/i0e9wbUvDu9tfJQonAKyRXUh4MwJy6gRFx78MeLycxpell0H3I/fc66EeYUbDjbCUz59IA30nqQYviWicuTtyJRTHTF/X4cwzbmjAO2ghQob3R3jPGVwMYMxLlghbdca3e4PjW86jjHBBtRiG4LhC1IS9FZWNDqweP0ybQ2cOPL9KVTTdWDzej23WEVf196rVHifu08kPGJ2EYFLN2xvM+NzcJ3PJ5s87XSmhH7sEakmkhC65mAJrnoKNFNdDHwoSj5R0xhMg+ZGg3y1R8j4YqwK9DWkkHr0GCfOPafqIHQ3HGvnNBWGsmO+5dSC2LASYWm6qy5J3ztHOZCttUzBjMiTrb95ZoH1cdB0CPACRuHIoZBxXzRspiO5W2IFktTKCCZDMA07iQ3HvIZlDK0r0EzXStx8AY2j+1BMUNj8HupZGe8vB4GIjuWaWXRlSOsnmR78/tgsR7venPAFCGbkPcs5HEBuY5mHfS09qVbjSsUXJqx4s50edHeAu5GZl6w+aNc2vXQeQw58BC2poKmxYjOvm1uNTZz01uPteNQ/phHa8/EcNgLuNFZj7UM3PxYqOkzMl08k9ymxA68g7Cnd80NfGM2/TjLZo8sttGafAqOM25ZxmaTJwX2ITH0I41fJvAEgIgjTLegvPzb0LKCDFmb4jbe1X2tNMpbdZ9CeM3nPPGkgrzVDiddlBhm7K57aqkrXsNB7dqwBwA6zFxG+LKaEMdAHPjEKBi8xj8zabgFD+lq0rdjpvHgtpd1fpwVvFwwjMx0T4AoPVRGAz8KMwSYUSXnveFMxCjwiMZ08i3I+bV2EAmwOjvymz0NLocvJZMaToLMzcAqIe1rejuEMY49+LmHDPieJcSYWh0Zyten9KKEAlezbHl428zPgnDALQmK1UfrLcCiTq9tyfTlbeytUUaLt2Y3gmgpzj6W+jiGTG2V3QUgANgGUzI2OnHFFPIxAVSDTQjsEiPXRnSU/HaOVBoKXyoXdA0drRZrEIuUOdVM1Z8XbmoIfJ1mMwmGGsl2CLKvOyUc6LeGdGoVGjdY4F8tXMR8UazqoStbIpngu/4qXsCYiQpK6GORd+1HTUNfIg0hAwzh6xEdLmyTlcvjWMwOq2XZeWcN6ZUucsbd3njlLoXeTu6jLx5CBedG3CbUUsLt9fK4bNHhacAmYMSbxLw1iIuI5z87m9aGenyZ+kdq59cmHgkWgXdevaKzl60mhqg3c6PngXZ4vzoG+CMcJLk2JeSqmW3viyfcak2b2p4mYPs/Hd1uvqu8ckYhopwrTNJlKtOPNWF9+VsZcm6HQzAa8NSQHakPCzqW677d/WYCKMwEpfi80bj8H08t1smW8SzYWRu5edvwaNZCvfpSqoVsLxz8Ql+e2MrnU4suA3ISs2mu5givi94Q1k7QQlcZfBgD5lUsUzC+JxUB9L20HPshiBwiGYocjzvRiFhxiLwjmw4CEnbt5ehTiLqJqZUecgrp7xzl0wLNOojejfog2vj12k6eBRgHlfVxCJ7w3duJd9uN5tR1HXkwxDPaVCfA0cycHCmtoKukYkJI0kqQtl6fG4YkQlJdK2HUI46uScTcytr9w7qKx7V2M7utx2fhGFQL7WdU+GxnJqo5X1aG8PrgPK/MnprsePjAT7e9n6ATm8Ni91CBl4agACXIg9965iNhiu4C8HBiLg0MJH5hlww1oQsFKokNvb+3YfXjoh9aBlEL8skSskKWX1hKkw2iesk5KrG2hYgecs4BrxtCDN0MCLmEQzNa9VAxzitA3ZwM0biVeNXBOYATXTmtZFEWdLOXTZFrzGk2DQ7btPvxEpmlOMf3enXXOnwEoAX8+ZWI/RWwHXsDRq/ZCI96fiYjJtAZ9OOx24p8JvjBzemGa9m0HtmqxPptFXqJq2HcMtIUVPrIv7bjk/CMIBN9Gux07G0U+ZaZzadTPuOvjhHZJiw8By9gddYZq8ZB3AwkdpuTONE8P0UbHvv0S0OebhguxXkxY20opebY/jPIpWZnew9NGOMVF/orndOlVKN45FydXwg1KNDdNV2cSnQmNTBbpY+t5sjMYbAw2O3IUQjL/mkbYs/S+tpYZkIWn8LncxwSY6y6yPoaKpKZhDmVHiTr5zT1mT/xlYCry32Jq57A8DFNetl8MPz0jkMZrxf30D68UbwdQClPaQ4Uw56HEG+s3lV+0V1Dxecd+MX88CR8dCnEHOqn3tkRMJ4vCZB8FRPPNUTH8uPkPko0pWeqwqnAUEd0WWzjAzPQbSqf0kS6ay1YJn5w60wKqTkwV07sYyEYRTSPwPa7j/KzDeDNJzXazp+YJ7I4tVzxrx8eROTWH9DixF3Zsks0hu4jqHHJNVUlJMJmqgO4rDDCWr2SswJb4Xe1Ygq1tq+4Q3+L8RcGA4lehNyxGPuARyMQvudIWVJd5myhRK3aZKKdQyPHpZFt6HE+va1qak5tceCChzclYEZOXJh2sLyOTWyZ62fZWgqvtxEuupkr5U4Pt9rHPq5DlLzBF6gh+ubbgqyzFB5U5y48G4gRq+2aE+3bo4v9GOY+PDX+8OfmUr073yckqkkR+34fVo5pa2DOAPKf+QuvMxA3I5uHI5xoQ0vivFFHzJbt7qBG8mZleUQsty6oOMISx76kb201rT6wl2M14aLmbAKwii7bn0CsNTttU48TFeeptnp1OaO51yPRCJoaULLVmDS7eGy+tq0DpiWPhztCtBk4sewwfpaYvhBsBpvwoXwKLR1zVZPYTq+kI6aj+10nT4eqH5X9urI+m2lYXYP87aWYjQKrzWSHfkmsSQb4zGyEeJYwA1uUYedvmFT2udnSL7F+VVMVXr2cOAgDUivybjFuPrn9WsSfJpx7kW9zcX7WMZ1MD0LaQLDv+34JAxDQluzGYDPpyf+/PzORVsstoyelFHglPB2YsBjVecF2PNzyzjYDSjaZbtD5zGQ54o6ypt9F60ex0k7t7gpF52oItyn3QuytGciOHLvIXQFw6qHETvqB44U3rOUlkarmCIPCR7T4se3pqefT960dJmIHgLbtCOirHcTe5qomwu2AmCAZOzsOQmyQ96UbiOkeQtpYOs0gFG96a10DyDIS8Xb3zXj4capzhY+1AnqouisyLmQlkLOlWkqDR8ZRzAen8vcFnyKhSHdMwiS00VnQlY+RjcKN70oxkIrrFYmi6cU1RZzbYsXihZPW954LGqp54t2kdgRrwjjkIIA5Z+1qnKW2oRbXqNDB8i9aXqVyblJKI/38VRPvCv3XOvMKZmO6FM98c32wJfr2x9nU1uggY1g3Y1jl1w87XIrgtneR+zwFsdFavC2ZwSqjM1Bxgo6MIPQY0VAu6sZw1JhE7PWAdXu41YXcqQCNIVhNRWgdswbll4Qn6KvwMzuRtFq7q3aMnLy2oRipxBzyZU0V8qs6NoFYEmGNYjv8tFjpoUQuGeQaSKxYQTC04Bbr6CHDZGpiO9rmIQazjCbUdDJz8+xhZScyJS6l5Cldz6vmtoOWCWRk3oYFgzGRJF0MAqV5C56d6pfbTs//B4aiuMInY3AjOYIN4adfVX1vhI91LTS536Pu+zbDa4kx85VhnMdz+3isna3odToPTdWrXvbl2rG9CoGNj7VxRskf1c+7/Xx6RiGZOy+SNnde3u6YwnyEXAcNffC2lscpo2cAma1C3bBv6/8FjpuYEbA+kc2oE+VlcxFc1OY6g1zj9RZDov/6EkEsNkBzWhK2l3gipKpbOSWohtTTlMyAZdUlCUVVug9LWuhePkz6mnUqadTW1pVpPEUoIf8EhRpGbITA/ZQ8wAwBnYweBJxIOt45aDjrDBVUjbjkHNlcqMw5dJ5DFFink2UpnqpcQDSs+5USQ18DIp0v3+1Gwe/D+Fp3BqHEF2NArpYOmPLgUgVIsd0dKdCD/0vfP5ZaBOfASEz3zaTA042gqQdC4tzWcmuiF1aWGQe5ZDpihSsVK7VvEiSYQzPZeb9dv5xGgaRkC0zht9DWlkGBHqWQHlr2/1H5aU0/D4uwMhCBLAYWgtxg29rKqBTSoOCuwosAyi0aeaRBbyQp9VPeEixiLwAh+IcD+fGS55/cqJUoRsisN2g3NxYMxZDez7xSefViuVUqEWoksC1IEvCah6sSNJay4l7A4E3yIDnxc7vhiHt6uFINwoq/R8OfrZwNtnfZm3VshHSsYVItx5pzz2Ea/oDdYYEWY3slVTZdCIaqYzsx/ATNkzwxTI9cgzbBGIB3j4W9+lwTN/xx4RfVw3rJfUV1/UY7/XRFrXqTvu+R8btuMnUwZCNIGtkusaaj6A9G2U72UZUhDkV1jq1TmY/ZHwShiFG9l3SkOi9XRjLWe/WyMVn8O1Cgx7bwetEpltPoaciBzSYTlldyWx14UHWBn7FYo3dO8Clz+PGaQ8vbj2DGGPMCTR+/22tRWQjrJGp5adjotnOarvqkndSTcy5cJoKuyjbVKlLNbqxgOwWHtRkFZJps7ACockkNgbkwDWwB/yfL7DAEyJsvc1MNIPSHuuAY8q1VVSGURBoxCb7570rB6ZjcBdm7V4EdWmbh+mFbkbwaeGhaWSMmokRGh6UnjDk33Q2e3l8HTzCtinRjf3tGCt/b4Hl8edhLrxWSBWepyiLFlaxuXimstxgJv09sWZq60BFteyViFJ/jB5DjLihq7uHc7qSsV4MlhXwtmRpt+Yx0vPPrxJU2nFf4hPHzEavZhsVdaJo5+J7RfKLD3ajogflWXYq5VDDP7IkD2KedPdzVHTqu8Dxixy0C7UXB407a5xrkJ7S2A07aQcGw0VSS2Ea2DqECsN80xvDEGFEK84aBFjcI3852vPSnjcyk03UKkoROkHLPbVdM7sWA4dHDEZTCx3WG8Ns38Xu6kwZEoqWskM2SwUP3kOM8Bqqh49R63DLHYk7elumnT2stO8w4go9TAzl8JAQjB4VtzyZUZjYJOF7ujKyK3MLKWhY2K2x2DQfjOwPHZ+EYVCNNlq2AC86s2XrCv1FNoGJuFGbZi5lYpbCT9PaDEKkyKGTml4bYywJPUsx/r1q3ICd4N6DiV+c/bEAu1asG3HQVyMRsOmxUKeDVjb57DE54Aah9ZhvDFMw2Yrn7uOzkyinvJuislTr4OTdqnZvZbcpaErU3bZwUVvc1VMoJjM/9HsYQ4pxs1QPPW7K+lsYESM8hQw0gVo9vDDk3OK+Apyn44Gj8cztKK0eYmlXf/N7U0Ramje5nkeAcVUTm+z2PJWZSqg3h9HeNDVNxcC1Wvk9lrIei7UAPk+bZ6bCKxjvd+VyU6w1Ko2FWIw9fgQnbT5bb8pZuuFsKXC6wYjjz2IyQmudrGlPyu17hALYbzs+CcMAfYJc60yVxKOceEgnEyjxrEQoIV10BqU13RhThuOI/PDIKBvByxgmFmoqziM70YyALcRVs5GO2vOdsw+0BrajkYlKu3FsN7z1WOyASZWpeSy3Ml23YqiH7ylKHTIT4K55MoahVvVZGOlJ39K8dFiydtX18Bwcdxhe3cKI8Ts2b0GOvyNmFMja/gYOnz+WB5SaSPll9uC2n+V4zaunWl4NwYasllWsln49hx09DwSpJNqow5HmDCHaleQNYOyNK+lwrpH+7hcm9Dhq0ylt3sTNWLWLE8frDEPrmY72OXSZw9ETjfMMgxq9Kq81U36sDWdC8zEWjLXmElbNPNUTVRJfpC5vFmWmn+u1pW3GMV6CxnXHG8twNCINpR/IKfbTXLYzG5tkd2Onhv7e0nGvfu4Ro97moEel6HU4g0udmwtsGYTO2gtBkrg2I4knJkE7vrvC+9Dfsi28WOQKUr2/RD02mdVE9xYU+y8MxHDBSqKrTfs5Ny8hFtyAVYyVlKqg1dIcKpCSu+zeY2KviTSkLsfvlJOFAZtmkvYMjRmA/VBHE3UUMcaiq+Q4jaV9t4Mb3jQfsHqLMhiYqOC8HRvSAMWRgWv8h4ETwREDi3AidB2CbXvbDNleO4bFXUawOEBU9cjh2DVxKVGRa/oeP9qGM4E+zy0NmHqnJgCe2+vC8m8kFl4SU2eOfPbbjtZBT4YxW+D1FP6aYCIa4lv8fUZPXpSDUs5t/0RD09PBomeUdfAODhPQQ5OIixfvWRgltNYi3oEsji5nUGYjlo1vGe3fGk16AATxjIAiSPJ5O+yi7SK4gWjPZ0jF6i+kdtByzEggNBakUZ8h+ms2PPhVEE6OBntMAxLkLj0YZEtDJgsnNCjUBmCPEvx2PTp7cdPJalK8U3Ys+Fg8JYyApmZgVgxYibShnZNfe9EXczAyYdHjtPJ6c90yEKRizmwaRDht4sL9OvSwmpvfY2zVul2vXli1ldx7kfyW45MwDIrpC2wpO8nFduSnejKCU94OaRoYmGS+mOLafJ/DFBWYreHMrXIT2gtThgmzaWHDvJRaExeqt5CLZru1SWvFuY3/wihE5+Zx0QfQ2tNjqXkQAbYFsy/KZ2O3bBoN3+EmBvIvXlRFGAo3IKJHd94+1I3F7Q3yUZPN4lQEypDaDO/CMZtDbYUcjxEARlRWlprcBnUDVzWxV+VaJk9lr8ZRGb5r8VBixTAnM7IVvBnLzM5taNK+hybniByfNwPim4/sQ2rRMSWBrCHoGwVdOF+ie0fA4EXQNBZGvsutoHBwZ8Dm370D3SP3JADIMTtlYKr3H0l7K0h8VuGU/Rg3oPZvGp+GYVCj+Kbi7d5JPJWFN/nqiyYdxFFjbJobPbQOxKLRA4CeErLnh5vSqtWk0auD3z5335uz7K6rN/FluTejpVfu5epAV20LOCjcVdMhZIjzjeMEiFa90CW8JZO1s13voiaO++RS4VZ3kfriQbhWA2PD9Y7bH+3fmmFINntVbGHL0D8SOHoVx/l9+EOdAVZFkYQJv2g/hAZQacSRPqOlRyedy2CGqaqwFbtbOdX23ap7XpMmkpMjcq5sdaLKfvAIbvUGTKthojgWdChd9/etzQx347BQ2CQ3kHf0Fu0C5LZxLQpbWNFaOUuXg299TCWqertxGKt4b8lvzTO8CVV7NgPeys42ZCs2sVZ1D8kqUadU2ErmaZu55IlS04/TY4BO6aw+Ue7y1hbELDsP6a4p0owg4Ivj0AHGI135yFLsCjmvA5c9e6GH+Gz1RZ2xgqXKxpnt4C0wLPpxhEEwfb7Ubuzqxi+hoBOwU8isLn56DVowHN3rtrPmtpACX6gqB/S/jQgLxjF8P3Wv4tY43L6tfbWA1m+yEwFghohLwx4cjJAAR6WXXisM8XIZiE/hGRmwdujC5WOkNB+JY3avWjHTEGJEm7sN7TvxMKK3h6UkK13jIBEl8puHG6aZoE1V3KTghLHIKkaGFg4GpyXaCpx9A/wuTCBaM1ohnw5cCTNWp7Rx8gzEdZva/V/yjzArkQRn8VV2T7o/7ierrvQsxKXOzHn34hO7eK3aTqVp9wNNhacMXsMtiSRGxxQ6nfn2Es7UZpQirn2sXt/uC+TceigmdxmDKNNDg/ge7+vd4fgtnUZkMuz11zrzVE5mSCQ1WvRWM89lYa+Z3T2IvSZLUZXMumdKSUfDMLr18S9pW/ENexgqURuI+B33zQDLwWSE1xHOVhnxB/+c6tkEUWoVcu5KTtWNWhJ1Cu/EWXeq2gJMmq1WQq0h8Ezxz6tUSW1Xb7Ro+gK0+goHImOB01PRSHlRUXvrdQANBwogc1SRKlRHvKKXKdxWXtrr/Kd26v33aUaM28t37ftN9FV2k8FLlj9Z99y8sB8yPgnDAC7MIYW9eH0C2gDJj+XMJT9xr1dS6vX5QfQ4kFWGo44AZGQlxtd1NNjLoaH1Dyg+w4OLPpbvbl5DcdHZ4u3UiU/Fb3J1QGwEECOUuHpuPUZ4F6e0MfL843OidDY+e6+pEaQihNhqZiuZqxuFUhLqmQmtvs1ohBDtsreMhYSR8IekPT+Ahq9ZiAR6yF/211qGQ3o3rOrnglLFNCTU8YZbWnSjGbewIrG7YZi1sHnxR8a6dEXWyO7toA7t5xSGwkDfqD0xHMEWtwHLYcwD+xkZhRGSxD1qFGz/gt2bqyxCS21GCBFeaMy9qLWwe52aUYjQZlSYGoVkXjBpBwDylDYepit3kwGxW8kOSv+OVaJF5C8Bfw/4Az+nv6Oqf1tEfgr818AfAf8I+LdU9RsREeBvA/8G8AT8NVX9+9/7GdA0/a51IkRPp2YxraIuAJYFp8xSu3qN9E4/EePB0Wtolvd7vQMHIRvQacd/kJ0tPROFTY10JCMi7rGrG4PHehqo0xZCXHTm2/Kyf0QYwpDpAjMMT2Xh0fsORri1uUe1a2Kv/d9WE9tm3alKSQYyevNYqiC7/yxOagpjAWiR46KGRmy6HePmo56tONzM8TiDUQBB92Qt9VTZ8ZBlVoqnJAOULA4+7ppINbcVYQ2CJiuwqmJdqaq3aqvWd6OnfA2rWW+udZZOLAtNTaO7OxfA/5mR8BT6i5nSQxabr4AUb2ALF63NCNyW498u0ZbB0lALH5ru+GtuGZIvzqVlrSr3aeWcN0SUUhI5v6y1+U3jt/EYduA/VtW/LyJvgf9TRP4n4K8B/4uq/i0R+RvA3wD+E+BfB/4F//cvA/+5//zOIaKtsnLLtiPGLgmOuIqVIo8yaOemrXekOt96Ci9/1xZIVzmW2MYx1vbZ3TjNYgBPqT3DUFQoYmnHjMWhG9aH87EuzVuIPggWHiwHw5B8Ap9Mipnqz0UGwoxl/x4VMZWjklnr1JD9dc+U3bwIazE/AIyKGYXajcKB7ehfPOyAgYT971dD3jj0C8yC4Qk9HrRIe6gm04woJVGykJPtoEmlhRXhPRQVJqF5UFFY9RJ0TJ185nXmsfO366dpMB4dO0DseKsfP45V3MO5Hd3974a8H8cMQ5XXQ4Gotwgvw7xDwy3QY4Ol0WsY+5TUm3vStC6lsqTClCulGM7yOw8lVPWfAf/Mf/8gIv8A+AvAXwX+VX/ZfwH8r5hh+KvA31OTCfrfReQLEflDP873jvu0kvD2aNB20FEd2MhF1R+/CR2G9OPoKcRocuCUhhSjyocwAnRF5+DPX/wcqrumb/MzSapTbZ1P4a3Twg39UO94V+75dr9vExksv7xp5t12b/RlDffPi6HS3gg1ra1NmAAAIABJREFU9h2sbPbLy5sWXgENiwlSUBxrL5ZOrbt7CeMIPoG77ogwwgkvXguHVKb67j96BO3p8TGhWZEGUKqTqtIARBahFkFnQauVYasKUx4WcBI+bifOeec82T67a2LyrME1TWzJRIRPsnOfr6aYLJWsFnJeNLFJ7hmf4Rp2ebiBAq0TTfNiCOke66mFFUG7NiDcKMtVheLfe9PEV3oy6rXP15hft1TmTROXmD+aXLVr54GN1gSH3tUs+BAtLHHPOTzUJ/dSoyBte57Zk3J3+lPsRCUifwT8FeD/AP5cLHZV/Wci8vv+sr8A/JPhbX/sj32nYQjXEQJdtQs5a2lpvdtGI69Vqo36jq8p4wR/fSyUsvd1FNsag4xAkRx+ZtRIVwmymvuP2q5TdDngIr/a3racPPhEqJnHfTHQ0HGD3XtRjuBXvOdxX3je50ZpjfqI2Elbp2gGJLvREDlu9dKzDv3i0/CAeE08LMPPdqgRh7g1EIwvvvlpl95mc4AYBVQSRWDfc5uNUXVZy024NakXXvWFBTZPNsl946A0jAewwin3RqL5SnBIwNzwc1pN+GW4z71XhXsYmg4My1GANXpiVBVWPAUNzUCdpfgCPjakgU53j2rQwDrGAqzWhGi4IabzcKzOjPm3VgOhuWY0K9ftT0naTUTeAP8t8B+p6nv5jv4M/bQP48UqFpG/Dvx1gIc/eKCqHCSuW0rJgafHeuJtcqkvKeZuRl0AA1bgNe6jvsLtKIeFf6xpqG1r7BZ9a6+VVlhlr01OujEgMuLZCAFMOccakJxcdCSOa++PxdtTjwG2xd+XMnEtE9eSWxy+ZevzCDR8YZxoGotQhtvhtQtGPBIrgw6ik9A0F+JujWHESFT6Xmat9M8BnAF584bR0gyPqQOVdv5m7ESUVG3HT6JM1Rh8i+/QW3VvTqyDVSs3TgPwF/e2huFc2+OXOtuCk91SkGI8kYR3v9IJxlADDtyGghwAw4oZvCiVb8V3srHJzltXKBsrNwtiLQNIR0Yr/TUVOahO36pN9XOz7/RcZj5uJx6fT8g1QVa27U9B2k1EZswo/Jeq+t/5w7+MEEFE/hD4lT/+x8BfGt7+F4Ff3B5TVf8O8HcAfv4v/VwjBq9YhRyDNd80c6pba3h7dhLQOMI4HHs4HGdgHYpgYIj3RgsugBqnIG7OqDF428wkdPs37fFq6Pg/7qdGPrqkekDebTGnFieGAbSyYwcUNXPdJy77xFaSVUwCay6c573t6K8CSyq+Q+tNirJdWmLpR+bg4DnEtfCfEYGEARnIdwcjAhzEY2+PJVX8eGp56oL1wCjmOciwM+bk+gxuHC5lZlHL/uS8gToxTiq7ZFLRVggVXbMrwkl2avLwIEFlJ2m1Ck31TlKvcCPsfuYW3kVP1XFnjz6kI78kFvwIQn+RHzlrJ+mNCz9Gaxoz8CsqR36FGZIjZ+PquIiBpsLjfuKbyx3r48L8LKgI9ae/Y/DRswx/F/gHqvqfDU/9D8C/C/wt//nfD4//hyLyX2Gg47e/Db4Q4hsWq/Wwovhi+Xa/b223jI78utWMnPF3dZyKMWr3jemeoqa9F3Fj1GY0AtaQhoybEUBk1g5CbZp52mfTFqiJVK36cXLPoWo6CHTuNbM4fXWvXjpbMtdizLW9ZPaSjDZc0yEej522qDRWoUIH+8Y58drCx4Re2yUdAMkxwojNXgbjEC8PAqBUb0zzwt3Q/nmKeyt+xC1Rpn6SmoWsgmptQi4xWtrWZfVnDKytolzFpvNVJjJd+CUyGnPajTSXDL3ffNFfWKDCOVmoMe7IEVKcZWt1O+MCLmJCtbeFTuCFcNXo7o/1ZGxJeq3F+FpLhx/nrBWDOb/ippK3XQ+klYoXZ2t+u535eDkhT5n8ZPf/cv3dewz/CvDvAP+PiPxf/th/ihmE/0ZE/n3gHwP/pj/3P2Kpyv8XS1f+e7/NiYRVfKqLETQ8IDU0OfFUFz6WM4/pmYvOPGh0wO7kJKDVu9tl+M0Ep2OBynBjdMg8IKyDVsRjPbU6/+Lndq1zSzU+1YUP+5kP25nrbpd4dg3DVDNLKg1jUBWePUTYPZcdFOHHbWEduAm1JkoRtCa2nE1l2Y2DgHEXgNZVJhblINWmSZFITQYWCRFjtBQj/S1NTLY9134GyOCGIMXx/OCjx4Fbqzi3eM7TmHW3sMwMn1CzNBC6pETJZlD31MOmKVX2IcaPMCzRlaAATmnmTb4y69QEh0Mv8kkt/NjENoC3+Zns8y7wrd6TovbNCuNDWJpRmlbDODpjsku7GzZ1ox4V2YTh/SvuqQxZly7QMuITMmxYlsG6lJnrdSJdhOkJ64C+/o49BlX933gdNwD41155vQL/wQ86C2BE4/eaIMHJJdWq2EJ5qgvflgce0spZNn76HaSNJ7XaB5Obt8eiaOo2H5w5tozrF7qrNIUK0KqZd+WhEZTCOHwsJ95t9y1jsmlqRuFxnUkCl33iPO3MLnwa1N/NAba6y/G5aunI6zaxrhPVvYWyJ+MclIk1K2mq5Mnz/zVR43kfEulBX5Cihi8Q6tEeJmjCej0UW9gj0Gg3libAIngmsPZF/lo2bDx+Y0VCxx7y8AFFUEnUXKk1U0tiTxVd7LpUNU9iGozCSC0IqbtbGnqSyv20cZ0m3uQrJXto6ulxMIbplZmr2P0ca1YifAgN0vAgFoozYU0XNJirYUiMKVuYdXc+y9QIcYuUA241FmTFCPxqG+blpp0yfVtQuPomda0Tj9vC9rRwekws75U6C8+/a8PwZzmCwDSlejAUCWtAkyW0Drwi8eAd2Kjg8lzGLTOrHa62oWpeB2RDTJ15TCN10QvjJARmcFFnYdaZijQ+wvv9zIf91HaqUMvJqTLn2jyDzbGAu2l7leFXqtGpI2/fvlPxXTIeGwqgVBI19dhcIhXpC9GwgY6fKIpIb0YLbhNDbSl5WtQZkof1HmnMdu2G5yJ8ca+k4RmjUbh5vR2zGwwDTcWKvgBqotbKLokJGgYxOUYTI0Kp8CaieAzMYCy5EJWpk1oqk2penMkG5rbQQlczeBCRhUjOUQnVpzT8jDEu1OxpmEpqIcioCzFK/dn9T4fjmFLUMLO9VJyGbaTmVdyK+JSaYBfSDmkHze4l/oDxSRgGxVI2J/a2Y85JndgUKai9FdNEm/NOK+14QXDPK139Zgw3Ri0GsLk+Y81DNoIy20ujL3VpFvlSZz6UcyvuetxPXOvEh/3E+/XcYtpztv0gJmwSC8hD1XkSk1FXFarrfKXBUASguJc+0YOwpO4BhHFQTMBEMq0XJXAAGzs4MDyXTI/hoKHgFZNVcHakf05EJmMYkEGrHMKO7h1wFGk5gJgDMDECGI6NoHLIqpTSJ7018DWjECXaYB5mY076da8qTWB2LZlryiSx8uwL5tKH+EpoNRSkFay1LIanWBOmDRI1MGZZu4cyhgT4bIy/TWOjP2MhSh95yHJYdqQOmFf/e3V1s5m9lepvjCJC9n1zqsM9858/zC58KobBi4CciGJ6i/3ShXWdvcS5NONgrPjXumCHXkOmeMcoaTcjRFvDkzA6dR2EQLuIyjg2nXi/n3muixcyWb74Umae9xCMtYl4zvsLefSpKSN7TjxHl21pk3v8Ki191y/UAAr0v9WRfgnXPKlvr1iIMcqxBc4QRqFhA9rrHmIxi3hdsHRMwT2LMC6UOKdjWKHOB26GYBxxfMc0UDNIw4QYrkHHHYokNqcMRgPk8K5G70uHxwVYp4m1GhHJGrFsXJg5yd4Yt1kKyb3RiOttQXcSVBRVjYLAC7GoOyV5/BssdCjxeOO0dKNxS9k+8nWOIYYZj7kRovp7umFKYmGaJrwtoEA+Huc3jU/DMKhZ6gghwlMY+xWO7lvE9xfNxiwLIMcn1OJW3Srg8AIp8fSPjShd7T0tj63LOrEqBFKmtqO8385cioM8xXT7V8cKRJRTNhBxSpU5Vat0U2nsvXM2z2itmT2llrlYPR0ZYUUMh/L6QqodJ6CBgNq/jmjPDCTp4GAcKHb2OnxA29UtPmuGIoHs/XlNPfQIEoT4hVcZjhUaEL9pp3odEzZMww9aK4j44q+JmrSFZUGbVhXIIZwT9RbC5JWFe008Mw+pv40t5ZZJGkuxrcAjkxBWJ0klNVwhAEdwvoIcGzAjHLyIkcIfWaz2WqDwerbAuBEvF/PItr01KDGqCrJUyknZ7+2cNH/XhX59fBqGAXguM5O3PR8NAvSbFqWl4CXMOnHW0pq+tIITedlNOvT4ivYuQUGJHrtbA4d6B8Myuu7itU68W++4lomnbba6Cc8YRCpNZ3Nj76bNOmzljV0zi6vrTI5BnHOlpsSuhctuUqJdT4EeL6eKaLZmtBErCgbeRTxVe1iggUc0i8IR6PMKR9J3rNoRn8CMj8KLsAG6MRgdg+a2jvhC3OjRp403NSA0cBL7DoKFlakae1FE2Emw4yQoX1yDEc2pNg8CepixeoemEEltqlvaQ4kYVrm5g4a2ooeWhMLWoM2QXkq3H7gu0kVlx54X42tG76H97epREUrEGEvKI+yN5kOZrkWaslLOyn7n82D5EXoMYIYhQojRWxjl04zH4OwxEl+VNxQSm1w5SWmkp5leXQeGQWxUZsTXghmHuFQbXSau0I1CpCarCk/1xLf7PV9d7/n10wMfnk+s19n4/hEHi5LmymUubCVx93bjzXzl8/lCiM88lqWBlJ9Nz4Soa13sGJcy8XE9WbMba6zNPBeYC7UmtilT10w0cFHPQlg4IR6rK+TaQgzEshfAgFOIsYi0G5RmSGKe11jLimzxHftzPVS5wSESrat16D2294xraDQaYRTcyxB/Tvy54G84PekA2lY3EAeF7MFSXcs0EKeEixsIMxQTsyyture6t1ireK2FFVNRcdJUr0u41pnkHIdzMs8hFvcyzFvg4I3Ggu41E6llQuwcvSen/52prbrTKN1XzrrxKAuXoenOptlrbgqfvX3i3S5cFlNUn+5eKqB93/hkDMOUSmOSgVlKSyl1RZtMbRe8YG3KPtRzA50ssXjUZ6h4meug8ZgB3wgPzlqAm60oSqUBjd+WO95t93y73vHxcuLyvFAvGXabNEHsKbOiZ0PFr/cT95N18Q7Ddq0T15o5OZZSFO4crHwzX190Ja5ODc65ktzTWKOUGmj8gyooCcmlCbxKVkKKSTxzoWoLXasx4mKB24LUDv7p6G4ITA5WxmfeeAXafg/joDApMsa2DljGGIVqJdKp0iM8EW+Cm3oTXPEQLaVKdVGXKgbYhsal9fWsTaAk/p4OQrKmfGWl3v0Ub5mt8drSGFz9Na2BbNooXBr4Z/Nu9/qKfj9jXm6aW8Xt+LljyThwCFlIq4fTPc16RprRHUWLc6pMubLcbawCVGGaf5QKTmoVhmJI7uZFRfFlZ9kJ5Rx7fbegsYjNIu+Nsdgr03qPv9kJTybGog2TiPZkoegzEpieyomK8M12z/v9xIerG4XnCbkmxPtCAuBSXlUya1aet5mHyViTkbGoKrxb77mf1sZ7iAlVVXjaFp7W2ViPl7mFEgDzVGznU9BL7nFjpC+LGaexPkFaZqBLqDV5ITgamHgdgForuxYy+EsOmYTxd6GlSCPLIVM1bsSw8DTwDn9fGIc0ahJ6CJVyZZrKoc/lGPzEe2J55WSpyTAKk9iij99HSvpac6NTJ7WS/s31HKMeYvRWM0dXPHCELj/oHi7BMzgu/Eh/Gp36WEZ/Uc+EKH0usLfrdJaNrBNZNqLfReUI0MfrwDaY58XA8I/JN5XXQODvGZ+EYRCUh+na/i50NZwglxS0uV5HkDC3C5vc9/0uuvQ4kn2wge5E+Ws3Ch/qmWs1UZW9Jr5eH3i33vHx+US5ZOSaSBchbb3OIDabkhJ1zazFpdc0MftNey4z7y53fEgnpofKw3RlIjHJbv0A9omny8L6tKBeACNLBS2k1FvLySXdfBn7ProLZDMI3S13j2uqJr5aQm5OO/YQ7rv/3sDKZHRpinkkBxBTbz9f+99ZrbO1i6+0EUBhvEXsPLP3tNTh+eQ7X3ZPADosskx7w3TiaKfJQN0s5iG0bJB7Z4t7XFGpuqsBv7OWJsYLx0xAn2vphXGIEaX140LtWg4D+UNoOp4jQDl2YYvMWwjSzrJD8kpjkcbEBROuHfuPzLKTUB6mlW9S5SfnZ5ahk/gPGZ+IYXAtBlHe79aye0s9ZRS18VmGmM0BoyA7RWOaKhtJtgY+hrqT0+W5hcErXUEnXERjM555KkZt3lyJ+XFb2LcMW0J2Ie3eHNYBQc0Wb4vH/HtJXMvEY1maoXvaF37x6y+YZpvEPzk98Xa+8pBXq4i7Lly/PZO/zcwfEuWkbD/bKaJsKVsIsCfmD8l48MD+oOg0wAZZeyOpuSJzJXko0Zqbis3UoVK9NYORweMIw2IpSvcIAtgc8QE3IvG4pKGH5gHj1JaCDEOQhl1t3NWhu8WR2RlH8EKSexOnvJtBQA9e2q23EJqSRaNC01KYsYaDVh09ToKz0AlKqc1B25D69YqQt53jdxgTO47PCfdKA2gcjzl7T8DwGi51bgZuBMxDfXyWwkNeeTtf2DXzdrmw1/wiRP1N45MwDAD3eeWUNp5Lpxt/KOfBEPiOQe+Gfe/g0KYT7/XMKpmShHs2F7AIWnSXexuVomM8OsC04a4kiV9vb/iwnflmNeHWry8PfPN0x/Y8ky6JfBHS1QxD2nGPwVajTkLJics682E68WV+w2fzhV0Tv/j4Gef/+446w//3F+94+wcf+OLuwpvlyq+fHvj49T3Ln0y8/cdw/6udckp8+Vcm9i2x3hXjFKyJ86+Ehz+pTNfKdpd4/MPE9gbWSZDVjJbsIDWTduHy+zvls80WYLbmqkUS9Rp7onsi800YophXkcTSmFV6eiKsT4QgWZunIsA0l8OCj10/sghKNxCnqTDlwikX5twX4chijL+Bg0cZ3sBnyzMnx6pCOTuhTKlwl4NtarTh59apKcLT44K2btpwThs5iE3auQqBGd2nlTmZCOssO4uXcAMNWI7fUQtBbmXvI9t1rZOJIUPDQ05p5zrZ/HzQK2vqKc/gMmya+aq8sdC3LiSp/GR55pR33q137DVzP/0pCrX8aY6Q5J6Sdc651olrmvhYzkCPn0hdaSk7Zz0KVKokZ6d1oxAl2NGb8tXPHopYrLKyNB2FUEq6lsy2ZyjR4o3Wjek2lADabhqTeq2ZvWYu28TyXp2qOvEhv+H62cy705nrdUY+TkxPwvKhsnxrhmF6XCgnQSd35XcLX9KuzB88Nl0Tm3g2QAUtkAqkXZg/ADpxyUp6u5GSeliCLfo9Ic6lVwU5l+Y1aDAfI8rItXkEOrAizSA4ANiAxE7wMtZiPF78sxxLSLUZhVO2epKxEjXwF+ixfYiYTM5cnFLllAqntDOlwi4uiqJeFq2WZWBw968hu0/PakRxFf7+zUViQ29jPBewHT3cf6DhB01GIMhy2hsWB41tbBYUHs1WvcI4SXs+DNBZNtsE67kxc6Pvyrf7PR/LybMsxcB8lK+v93xcTzzMP0LDEDegIJyTWfbnsvDtftdCipzNQhcVo7TW3sE31Hov2sUx8o2nAMZlsJZgvQuQPX/km/cceG6Vjiab1heJlG4QpNBoATX6QhYh59rc3fieqsLpnTJdlLQm8mVmezvx+GBMtdPXieVbWD4Upo8r6Zo5fzWDJLZV0EktA5KxNCFmINKqyG4aijFE7RzzCukb2D7LlFMlT877cMBSron8mJAK5SzoXJsnoNXTm5HiTIrMXtE5KkurIFJbFgEiwyDBKqbU0FiI+95xgxzgYrj/qRyapASzMfnCripYDa65+skX55RcBjBVlwYcO47bIjM17t60Z+Q9ZDcIo4xel/YftCKpnNLWfh9DFStm6p5v3PuEa5v6dzpU896sh7VO7bymZBjIKdm8LyIu4WbL96KTVx+bYTglKwDbNPHV04NV5/5ATvQnYRhUhecy8zbbjXuTDYh8LKd2odtNkEp2BDlGxGWXahWQ30luegVfgFAIVp6gpZEiRg1Nxd37NMSiT0WOXkMBrZig6WbVkjHJx/OZcyEVOL3bWL4VTu8nyiLsZ2F7I+RVmT8q02NBLjvT08rbPz6RL5nLz4RyFupsVNcyQ53NoEmxkEY2QWdLF9bJDFmdIF+V/CyULbHvQ+W/syfTBvnZwo/1swTBe6hiKdki7dgqIIuDoYCWo7s/dpoa+QMJCK1NEds5izIYg2M8HnwPgJ1kWYZ274Tk1Z17jWbIvQivqgn+FFEe9xNFxNq3YfNpkkLKxmO4Ff2JGD+qKYMMVX23z+25obv4sLlEwdalzlzr1EKKU9rJLgM4cg96QV3fnEI1vIryXGau09T0I6zU/9TSnqEj+lzmZgTjPESUfc/s048QY6gIH8uJz+rFUj5pc2k0k06vmnjKJ2pKDrDs3KcrG5kZE28ZyxrqjSWOnoLWpNbSlSEfjxiFepWBeeYu3FpNKGUt1rOh7NlBR5DdXPXwTpv3sENehXrSVswTY0k798tG8dqFfN05VVvA9ZTYnhKpQL5W8tOGrBuyF+5+8cj0eOb0YeLyRWJ7I6TVPh9VZIflowOQSaiLUE7avOaolZgehfI+U5+TGY+lImuyDtjVsZKrIJeMpmSg6qT+vQxT0U0M4Jyq6z5qxwrUvBARl2W7yVZUFUpxWTq/LgFABm2ZBGuhGdOD1+C7btSWWNrRU387XKfJXGjVtqHsrrGZRFnTZIK7Da8a+k/64o1FG8V7I0EpgMgspTMOG5kuvCwL93p38tQqQeP4Yx+KeI29fiBu+Rx+Lq4DUq0fSU7amLij4RmNwoivzLlwmneW6UdIcFKFb9Z7vpif+Tw/t5jqYzpx3e641qmlM1M2hw6CLqqH1nVmQRObWvFUKDmFaGbvLWj1P4Y/2E29ZasBbGEUSkJ3y0bIKwYhhlQ3GpvHz9IR8nPe+PndI//0TlwxSUlrQTT5XLG6g3ytyFYJmeb0tDJXED2hMpP2hBSYHyt5raS1kq+V5WNm+ZDZ7oXrF+ZZgHsTBaZnqN/aezVDuVPKnUL1YhsvjkrPQnKmYzmrhS8eSkTYpM6ZaBWdfrpaE5rMmwji2S3wqGpSauEpWHl6aWi75fG7EOop75Y9kNrUu/e6UNW8iFV7/85GWsIW4uYGfq1mFM45N/7Iw3Q9tDrcNHuLgp42DGi20lOXm9dRGL9I21wMYxQjjF08NsoXxrD+GUbZDlwrPIf4TmCVvBZKJ+a0H44zlqBHfQhqsndLLuTztV2T33Z8Eoah1MQ//fg5VYV/8e0v2+Nv8pV9zvx6feAffvg9fnZ64s+f3/F5fuZtem5yW2fZuHdsohOVjF0YSk7ts+iFVOFcnd11NGrr2naBKIFWFVdOEvJqO2deR69BW4/GVveuwvPjwuO88WaZObMxT5XHfeHDH4GmheXjTL5GChDjtSvkLbHfPTA/nkjXgiZhfzOzvk1sD4myQL4OYGdV8tNGuhby80Q5Je6+Tl0o2r/ww59oMyKyV3RKvPvnTxaeTLQS7btfCadvlfnR3vjxz2fWz4T1c6Wc7DvqJVOvmfQuI5Oii3kO+6Tss5Lvd7ZcrAWdG4V5MjBxvS6kVLk/bZznzXY1l7W7lonrttiironfu3/klHfu8taBxZr51fNbPqwn3j+fWdeJ82kzmvly4fP5mVkqz2Xml5e3fPV8z8fLiTkX7habJ2+Wlb/88A1AU91GaO0AjaxkFy/CiQASAYr2ruPj8zbHfLdOxd39uE3CVaeWAQkg81qNZRu6n2vNrGXiUqxnyNNuFbuPZeHtdOEn85Ofg33elCo/P31klsLX2wNfr/d8+fwGgLfLlSXtLdz6bccnYRiqCt8+WVrw988fqZM0YPGz6Zn3+4kvn98YGSUV5lMx3oJBWu04s1ROKNdXcIbAGCo08HFpna8DnBriswCnRtAmwoUAIJ1THZ6D4ByCXUgF6mag5eZybl9dH/iT92+ZP4qFABXypRqIKDTgcLtPXH4C1Nx6OZQFysm9AIHp0QyIlIlJhLTZcbbPMnWSY/29xPmqAZJFSXtFq3L3TWU/C2URqlccn7+pnL/amZ42MzrPC09/bkZKYn+AfbMyZCmwvBPmj3D5vURZ1HQOJihroj7s1m1qM/f6erc3HgYC+pk08tIpd+/sw3Xhss5cnhfrZYny2fLM7MDijoV4X71/4PrNmfSYef9Q+PzuAtBSlE/7wrvLHb/88nP0aSK/3XherBx+3TN308ab6WrUdDHv7uT9SUOGbTQOphjdF1h0xK5ypD7HCC5EzPHIsIzzvs+1NPAvDOPaalcGv5SZpVjvkQA0w7jMsrUS8vu0ci0T/2SfeLouTG8qadJWuPfbjk/CMChweV64Xmb+4bzxz739NZ9Nl6bN8GZabacpmY/7wmWejzxzX8StiCpKaOE7hWEz3UCYAOzR5Qv59uYG36C6ooH66zGcGI1GsfO67hMJ5f164uPX9/zBH1fu/2QjP++kixfOnCfSOqOT8Pj7E8+/J1x/WqlnbbUYUrRhHHUW6pzY7oTpkgwfEChzz1SEp1CtTzCaM/laqbmHCurJBk04RmFYQ9pqO7fpaef8TWI/m8c0fzTDk1c4fVNZPlaWD4nr5wlRpZyF9fPMxVmVck2kVdCPGU72HRDluszUu+sLYtO6T1yeF/Tdwtfpgc/PF352euQurbyZrtzpxt20sa0T89cT5y+F7U3i+S/M5FR5M1+hGpns8bow/fGJ+YOwv8lcfn9nerD2be+vZ573mct84fPl4s1hCojF/2Mz4Z6B0F4Ipck8AO1tBaGHF1mqbXAVNkaQ0eTue/NeC1/3KPHmeD2oiZo6QLnXZF3L/DNOsvP59GTzNVe+mJ9YcuHL54Vvpjuep5nPTpfvW4IvxidhGFChfLBUzC/v3zCnwh/evecPz99yn1b2D+d3AAAgAElEQVSek2k1FE087Qvv9zNv85mLziRq6/WQkCaYOZbC3jIeQ80pR5s6umdQPMZrnaRvKLztUNV33sgGBOenAKLoJMglcb3MXBabVB+ez8jHibf/6JnpmyfkusG2gyrpvJC2B8p54nSXePqDTPmskB9shyu76TniqcPtMyFFWHNJRrQqll0Ql/SSgoGuntbMV2WeEnk20oVUDPichHIyjwQCf8hImUlbBZGW9iSpfdYK87Myf6zMH3fSmpmffDJPsH+VSOtEXSA/W+gjxT5neoY6w/XdmV//kVB+Irx7PrOVzOV5YXuakafM+U8y29Mdvzh9xsO0cso7p2oL4nmfqWvm/F64+1LJF+HpOjNl8xQmqTzvM0+XhbtfCfe/qlw/E+o0sU8VSZX3lxM5WePXXTOc4C6vbR5Ez5GZwonN0+J9jO3los7CFnWoSWvrcxFtDcF2+jAQY3ZtkmrYhe95cypNExSCtWlncK2TcTfYDxmO6HURodmHpzN6Z01uf8j4JAyDKsgq5KfEx4c7fplNw+BhujLPhc+mC3fT1pDo57LwVBcudeact1YrkR1jWKQX1xT/AGuB3sVgX+/N1kcYi9BGaG/T4WfwGQqkQaxERUgZ0iWxXSY+zie2OfP04cT0JEzvnpGnC1xXdDN3netKyhnZF+YPE1Izci6czis5V9Z1YrtOVGfkkYTqqUMS1I1DChUd+lNi51pPlu5Mm4u+qhmBOtFSm81YzEK5m9C5UqdEOQvbg3kqaQ99yHh/hEKeSbiA1MrdL1Ojas+PzrfYzZhoFq5fJN5vZ77+SxnmCpfM9CFzfhLyM9z9Stm/Eb59eMM/PRn34OOykEV593QHV+N8nD4UpCY+Xmeu887HfEJEuewTZc/ki7J8MDbF01VMUT/Zbn3dLBWdU2VJOz+dH638um0GnSZ/7F7WAfCR0NQqH93DiB4lsxTTkoj3j2k0cBJWNzZTKpyzgenXfWrEJxjKy2+Ymh+cDGjVvMo0VS5PCyL648xKoNaJOW1Q3s98WO749bLxs9MTzI9kqfzs9Mg313vAGGtPZXFlXDGmu6vjWF/L44huwzMda0jDc8118/s+xneqvSfkGE60Xz2ccM/OH7eFky9QnrNRWktqlYyaBgm3UswwTJORIHbboetkYhtRJ1CnStmVmoxJpdnk2VTUv4O555FZgCGF6vOn3AVNelCI9jBD3AtKOzxJYn3rRsavSTnB9lYawJqWCD0cL/BzrrO/xo/lEEn7jPnRPAwbM/e/EKgz+xtlfi8sH2D5Vi08+Xan3CVIM1+vP+Wrn77hdL8xTYWPv3zD6cvM6X1lfl9Iq1K+OvHeMwHZ3fS6y8GI2w22RTOlyrZNXNbEh+nEJJXtLpNUjfPQMKaQd4tu47lLtGnvhwK0dnizlpb2DHblGB5EXwsLSaQ9Dl4DgjYa8yg2E+HtyJsomvhmfyChfCzWGf3NfLV+oE8TF4Xn5UfoMVCF5X1ieoK0Zbb1jn/yNHPZJ/afJX7/9IG304WtZr6+3rNWO+2/fHowy5zs4ixSSFo9y+CH9p9Frb39LdlsFuEhVS6qXVbOXb3o/7ftmboloyIfejRIl2Wvnu+nk57OXydSmVjXxPpgsWadle3n90xzJj1ekGkCVfT+zPNf/pw6CU+/lylnpW6Jy3U26Xiwc9gSslllpzg9O9KjbZOKu+rsReiGLHZ6MwpKOXPIXojCfu8GI9NLuFWoS23VmDrpYDV6WDM9iYcNngJ13EIK5Ivw5hdQzl7deDIGZ17d49lhelLO7wrTYyGtBZ2Euy8r+SqU0xk4I0X5c++V6Vq5+3Ilv19ZVPmL//MbtvsT1y/OXL8Q9jfKlCyU2t4krp9bZuXNZ8/89P6509UdqNtL4u3yOT9Znvlsem5SgybUszTvAGhVnOEpxJyc3NsIwSHoIUUYmyKGE6Swxh6+Bgszjv/T5Ykvlmfup5WPrkL+uC/8dHls8/epLlb9uz0cUrtVhbtl43ET2GY+LuffvA6H8UkYBikwfYTpCbtOk8WC7z7c8cfnL1hST1clUa5l4v1610geD+n6QjQT4PaRjZ6ibDX8AdnTdR9nsaKbSCcZp58OLIZHORqZYWcGj/evHl8vgkomhFD2u4yEeGytSKnUN3dcfpIpizEgNSt6yWw4qi/a8AVZbSEaliAtbZpWGkeBwaMBNwapfwf7cDMmNZrJDsCpZjNidZJ+nGSchibC4uIuUTexr5lyl8gXxy+yvcduhpDP8KiGh6BmNCzbYpjDfgfbg7A+OyC3pP+/vbOJtS077vqv1tp7n49730d3PzvumCZxvgZmEiwrimSUIRBPDLMwIJESkQwSBSQYOMnEUkYgEgQSiuSISAkCIiRAWAgEASExIQEncvyB5cRxLCVxq+12d793P845+2MVg1q19jrn3vf6PfLS97Y4JV3de/fZZ+866+xVq1bVv/7F7l5kd8+2QNMyG5idEHKKeFpEOGmRSWnPJ8KghDEQd4Hdzt6jEfoTob8n6HrkznLHg9U5fTLwmledTilw1i/pwsQq9jmoaOq7UfC9PZrKa/Y8eawqlK7XLm0YcbKXJHJ9S8F8Df/ejKPEnsFmYc+j80dYajUUb2WTuUdDhkG3YlkMoPQVeVa5HYZhhJNXE5JgKwFtJH+ZHa+vTrjXbfi29RusolGlgdXSP5xW3IuXbFPHSeyrQpZAJ2mvgjJBjhUoLTNMGrxeIqeKstdQS4FCp2sGWaofP6R2s9graSs0reSJBgRl+2IkLYR2EWmDBTHHe4sMShL6O5C62gIdSJ7AYZK9Qq6CXpwqjEO+TKqMRQmU+v9O9lqlYjVgYK5O523H1qoRtWWPQ1ICSEzGNB+UsclUcI1aUZdYGnOahOGej2XWIVDOG0+E8TSwezEQejMgqZszJqnNWZneHvi4tRRrswiEPhnqs3XwGOX9Y46NbF9Slve3vLS65L3Lcx4NtopeNJ2lSlPIdQqGui0PBzZpxxxAnJLhHJyI1oFUXig1TIF1aEow0sULtCJG+T/jGypPhP3eJK1YtekqDqUi1LkqZ2Tngjd2a7o4pyQvho5hNDKfuAlzuf1Tyq0wDGGA06/1pDYwLVpz0UUYTxouThacnS7ZLY0s9iT29JO5bm8Oax405yzDUBihXZy5ybkYAsbt6FRnEUc/ajnfW9x7mhQc3GTbCKaqnDkjAPdWYNk/Zmm/OUMg0R7w3QuW+lt0wTAHwVzd/q4ZkPFEzYioGPFK3QuS/GEEUlSC2gdU5Up8QWWe6KE2Fq6uB08PjRr5M0wZwekWVpUwBfMkRsnbCUjLaaZwy8FQbfO2o7FybOeoRKk8MF8elW4x2p5YjcNx8JiMMnNXpjweSdi9JxJ2QnsWib15Zu25UeVPy5xl6Qzd6Vua6X073n/vjPvdJSdxV3ADDgByQNGYotHvhTBPZJ1JYZPaZ4x1yXbeFrhxqDkbHPzkC04K5jVETSRiIahtM0K2DVMhl/GKz1YmlmFgSFZa7RDuQa0j+sOdGbkxZzG2Q8M4RhuzoKR3Y8OZMCrda+ekRUvzQlM6IrUPA7tVxzdOTvjzp4ZS82htnxreGla81tyllZGTsOJ+uLQHFbkmXWkyZeNQu23A3vlFL8mkIklKlN9X1YJheMyibuk9JUVIG6GJwijmtfR3IKwMrDQuWjMGS9vbp87YfbVNZetRE6QYnoGrnsvbuYtusA6eD/cy9lCSgmOAYaREEOMo6AgymMEwQJSik5AKzRw59pEDra2iTTJjEefthyYptG7O4NQ0UymRdlVjSAU+ffjdTClwuWvZ7FqmTUN8szFUZkwW24hqNSFRiScDD+5d8N71GffbTcEBgE3IXWpYNgOPeptgKUPrG+a+EQ5ZBgoBrccEvD7B8C9twdR43wqYtxu1eGo8IYwZmBUk0UgdjExG2JIsm+L1Fwkr5R9TZNu3Bq3uG8Y+WmpbDXcC7LUtfBq5FYZBJkUeXRCWHd3ZmtSYi506YTyPPDpf8eYLK+60u+JqBVHe2J2wCGZJl2EoRVBdZvFpZc4+BPF6CcH7AKacnRh09hic368NuQltzVnoq2FOUOxxGWZ3OyclELV8f+PuCoKoFNd46kDv5z1w2A/0ySjEZCuzBwkBCzr2FheIWyk1EAW7MJgxcuN1CL/wLY/Wn4fZk1Cx+g3PMhCqLYh7Q2LBPG0ozUw07mdDZpCXeRUpBvMuoHgf4noE26psVtGYpuJcfo0oMSpNM9E1I23O0AC0GTF5utiVIqyzBwubMHnSjmP0chMWi5FlM9JUmQUvu/ZVOk2yR//WV5WXDnhzWjiv9NylWLYCdZHX+Whs4ClK8UBr6jdPX/rWxSHLXbQirygzN0TUVLIctujZViTlrc92aiz1umsYty3aZ36NbPRltG3cs8gtMQyJ9PAR8khY3lkj04r+ruXRNAS2uuQrpy/xwnrDC4tLmpxz/ub2hD9K97kYOy7XHS93D3lP84hla0GbpRuKPIM7gYukXGrkrDSrscKcrUa2mY/P6LF23O223F1tmabAZhfRSZg6NeamxjwTSVZgNO/zzYvwasMwGDeEQaClBNpSa2XT41rnDECJV+TJnQlZ1NvY5ayIb1F8tS/FXMwZAKhWf/aNRDEaOdA4Z1rqVTmXV3sGI4+hJOvipVGKQXML6QjKPQOR+2T6Z6hivXu6aYxmRJoc+Mx/D13GmreJsJho2omuG1m2I4tmZNUOBcxzf7kpe/oxIwSnjCOIIbGMA02YSiVizbnQ5S2FB5xDLtbaTG0u2orFKNRpx+3UWsOh0q9CS8bisu1oQmIVBxov2pKqcXEp0zbjYClJaz40NJHQKlMF+Z/p3iy+0YSJFRZ3O1n0NCFxlgJTRrVKkr1mQc8it8IwoArDQFIlnG9o1y2pM77DuBWai8D5xZxuudPuSqBxSJHzccGjccW9ZpP7ADjsdIZFz2jHmRXaZhGgOT3JnJ92rsBlM9I2E9smoTHYBM6rZcj7+tTYyg0gKnibN99yhEmhtwOShDRBynt+DZL3wFrqICBXLhagkpYJ5RBmuxfz5M5eyt7qDmWVL0FFqf6vA4711xHm6xcjEnS/RmTKLdCizA+ezNcvRs6Ph+pYfT8/fwQdhOCgqRZCg8V3BLQX0i7Qtw3jMpLW1nDG4rxyhfeiVGfKXMWZNLCd2sLuVPM7uLRhKpWc5uZLKfP256KuZhzzqu0IRdfAA4HLOFapSUohGFnvmkBl5q4wAzFoIGgyQ0EsXoenP5MYtPpOu+P+csOuNXIX4xVpkTHOz8CzxR5vi2Egd1makMstYbsiDC1hNNc4boXdo45HPohrq4jz7tAXdLw1rLjfLjmNW6O7cph0JgZxn3LSq7dPzC3FPUC0DNZF6qTtuVy0bLqWYeermiANxV1GLbgTUGsPX01KmZRYrexhMCIX2zrYpHLkoeZrzi46th/31dZjG9McdfdAqKfviqFwBGQ9zNUcdrkSI1GuBCk5MEalNkTIdPXz9f38PWPg1wiyZ3SojFPBVgiFnSo1OdUbKI1sUoS0iFzeiWy6xHk30mZuSY9J7Kku9v03caLNzNFezem0cDCv9HXh3CGNXDmv8kqGFItRmHJqO6nMTpMKMcRiGJZxoJVUjMOYZsZoZ7saNcDUFEDTTgwMtQiZMdqp7fNYbsTqQ+4vjOfxT/QeGxWmUSBEQv/sLsMtMQwJkqJJ0e2WcLalWbUG3+0jzQbiWWQSOM+swambyT2GZNDat4Y1D9ozq8/P7evMF1fvGr9XVz8hxDxbDMNgvP9uHJqcblo29vANjUfkstpenNQYPJm8tbB4AhxGBEsWINcwgJbUmq3C8+qv7lHkvb7HIB1VCPNEF+/VkG9XT9TrJr4ps//vrOT8Wv2i5MmrAME/h2a8gtSXnA1JUx0sxrlaIZP1rhDVaiskMJkB8K1QvRUJwSjitInoGBgmIWUsQmyygai3TXkAmikw5hjFmGyb4fGE0sSm2iL470Pa9dpbOKy+LcS1KnvnBd33MLo40elYuCXqexZyH8ltG7PxiqLW/0Kn3IYvFOOwCCPLvJ3q4sSD0wteHSJTaKuu5TyT3A7DUD0sut0RLrfEiyXxtKXZ2h68PXPM+oKHAnpiX8KQ3aVHcckbzZqH3Zqz1kq4W5lYYiPjgUcXr5LrCSzJwSGmzAu4T/DZOIV5l/aqa+tnRvNqFhxElA5WZ2sBZZMYyftxmQOZihmGvCLXCMVUTTDvCl2qIXM8A8+y1BM/24vDCV5Ukqv/X/vtHBoc9cUqT+rDQGe9TTm8bmUbfLKrCB6wdV3dUNbAMU/BxmSVpSkpTMLUW9Zj6gIhIzIFSgATFYaYjIMzGimMMk/kJqQycWpQW0JKj4qy7RDK1qI2Gk44Y59HyzEX5w6tpcvApyZMNBrK6x7IHFNgJJR0aFNlN4JMhQHKKjNnLEQXJk5WO3aLjjA0Zhj6dyGOoRbd7UhvvkUUYRFAw5o4RFJnVYTDRhh2gW/eazi9t2EcjV3JiTNHDdyJWx40j9iqoSKXua3XMjNIewZi0EiPuXJLGelIIAODRF5oLjhrl5wNSxLCneUOBc5HoQ8tzWVABqNx8/qDMMI4CKHPVZf9PpDHyVCmhVhWopmj/vvBR/YmUMqU7u5mezrRPZCQ6eLreIO9YR5XL5oqRVbsxxL8+q5LLZKq98l8vTAqYaiMuv+Zrzt1Urwq9yKKETmMQyiIyp7XM8Oy7ZrJ3KM8DmrAqDbN6EoVNCkSK6NAXoWnsNfMpo2JdTuUwOTcg8L3/2EvoLhshjKRfRK6VzCJHYv5+lGUi76dOT+Dd5eaS6edtn4Vh4xq7NlOLdup3d+2VFkRJ3p19qZJQ6Gdvxw7Hu2WudGycLldGFenGuRcHmf1HyNvaxhE5BXg14D3YXb1k6r6j0XkE8DfAr6RT/1ZVf2P+T0/A/wYZvN/WlX/89tqkgdKpwn6Ad1sCJs1zWYidQYBjhkMlNrA2DVMp6GktsYxcr7reNisuEwdZ2llOXEMitrJRAxbZtIN8N6CPXFvEjkseh16VnFg1MBJ25sFP4lskjCGhrB1rL/kOggILYTGjEQMWkX859jBuM7pyvzA+z5dm6yGr+RZJ/MYdPYeoq2UbhhSN8capMQ98vt98md3ssQf6lhC5WWoUGotSqDSK0irAj1JoIMFScNA5QVQvKHUUchfrsQtqD8/lCCou7x5C1FStn4u+bejKqMimbhWgs7pzuBpT5jyRetOV45a9MlXxw7s/2mPrMdp4zwo7Su5E88GDFdRc3eMUyy4DDcubiBcfJsAMwIyHUziEtPIiEzHYDgkekwzMdGUhLOLJeO2RfpA3FrJe9g+Z8OAQVz+rqr+jojcAX5bRH4jv/aPVPUf1ieLyAeBHwL+AvCtwH8Vke9R1SdQyLjFn42D7nrC5ZawWyFTU4JtjRi+QdtAf7ctzTpTErZ9y8NmyevDqa0EWEv0Kfj2YCQylJjCUiaLVGtLL9byy2UpPfeaS+63S4Io/dRYsCmvPLugTEtr2JK2Vh0qyly/kDH9Uk0m9xDGtaKtTyBsFYwYPZp3c/JhUYyQFSwzkBvFamGtxozEaOhMm1w6U8xV7oDjHgwgJfO9dDYU+6nT6vVR7PO4Rz2BdJTirRIPyIZFI5VXpCVOMj8ofj+dvaA6sIlvl/yz+3tMJ1mNhKilgQ5Q4gtSTXYRLbUJDpaKIdFFCyw3Mlm/y2ovb7fTAnN2CndvZgM2SYcQWOrIpbQZ85C7ZeV+GAol2OlGpMvxAKer63Jfip1YHUQbJnbTPC0dY3GFvAWKUQBYNgOLseXNaWVYhstIcxYQNR6OZw1Avq1hUNVXgVfz32ci8kXg/U94y8eAX1fVHfCHIvJl4PuA//nE+ySl9EvThPY9bHdGljrtT5YwQNgJ4xho2ommSQXU0o+Rs2HJadzRpqWx+eYBHbSh97gDs2dwoR2Tzh1+PBaxFKP+2qW21GgY1Za5h+MYGReRqW2sNkDJDWk0G7J9w+Cr47TIiLwwP+hEzQCfZBBiyK4xxGpFjFXfhpTy6jcaFj6N5jK7HiSZJ3f2ImQyI4bq9VuJqPNkrF9IoJuQ35vTs6kyDFWcwXAaSlpQ2uXNmQqPoCpk5CT+44awtiBx1kVk9gq6hTfOmSdz20xlWyBYbEjEiFh8ggKFyOS02ZUJ75PvkAKtpnV3Ul+AlLcVCaOFN3e+LZ2kgL19fxSLU6wb2zpYu/qxVFx6IDFpgLjPndBkg+H6zXBoa8w0qBPKZqqAJMWQh97K/+Pm+XsMRUTk24G/CPwW8BHgp0Tkh4FPY17Fm5jR+M3qbX/MNYZERH4c+HGAZTwt3oIbBx1G0sWlZSjudNkPJ6/EAMKwiQxNKnBa3z9eTNZ8o80xha0Y5VevkVZj4e8zw2D3dcYeMBquTia2YhyAd5uN5ZTzuYtoKLohBS53HduuZdjNQ5kyo/I0SHb5c5DNjUDuJymh6rGQA2POruy9GgvrL5TVLubJ4HnwmTeiCmAlKT/eWh53U7PBERtGKAFMm3hN1brej6uKVTIORiufcpfvqYp1eNpRGzX6/NVk1w9KSQsVZmkbi5gb37rnN9+P0teylhBsjFadkcgKc5Bv3fZ59Z9KwDCIss2Vh75d8FX4JPY0YdpDJtbl0i5TNgDea6JwMeQFxOsX+nYmVLloulLT497Iuhm4226LQfB7DSmCQINRy+2qJjUwN+X1nhmWPbP7tGEqBmgztGy3LXIRaR8F2gtoNsribKLZPls48anPFpFT4N8Af0dVH4nILwE/j9n4nwd+AfhR4DrTpFcOqH4S+CTAvfa9uneG2mZTe9tOxM0awoLUzm6uQT2zdRRl3Q1lwjSSciFMQystpzGU3pTe+NPTlEaaoZnzUTL9vHH1dzLShpGQbF/Xp4YujubyRaPdWrcDF13Ho7icASoelBqDAXSguLhgPR3b1jn7Dtu3ae7dOLuNtVvcBO95oAVs4xH0MZlhqPPp9nuecDa8+3l5d3vJX16NBSi9IlXou4ahb5h2EY0hB0Bz56rciAcFbROySCzWfenQrUoxUtZzYv68nilwIznfk3KslhgSp11fMgYup+0uT7ppb1tw0sx8oC5N5kdwg+BNixZhKNB4uFpD496kM0ZHlCkK69RzmZnF5krJ1rpN14jFHKuIWCalpnabv5OD6LDrHJIVDGY056Esm7HURGiwMvZ7bxqRTXP5Z2AYRKTFjMK/UNV/C6Cqr1Wv/zLwH/K/fwy8Ur39zwFfe+INQkAkouPoF7Tf04ReXhLPTlA5KbX1ZbwSpcX7shm5u9hyt93yYndhFjlMtGEsjUE8I3GZFgwyEjPoaSkD3m1oS8tW57bm39oaXf3DZsXrwx3eCitjo85NccD4976xPb32o7lrmdSQdgmhC+PeAz3Xf+Q6gAyAafP59Xnegq0JaY+BGCiuZU36UUA6PtmY8fx1sM3POXxfubck+tSwHVvOhgVnuwWbvkVVuLfe0GZ8gOfl25B4cXm5D+TBJ4xeufYyGiLQ2YsOda3fF0SNLyFPal9N3d2OZRyn+foyljoYPz9Kotd5CkRs4kXmsfUGytcZCG8RNzextWPOE7LVlstpsdcDotZvUGMQ98/UZu+lPrfugr0IA8u8N91qw2VYcJk6TqIZxD41LFYDm4uWMAjLb8LpV86RYWL5LS3PIk+TlRDgnwFfVNVfrI6/nOMPAH8d+Hz++1PAvxSRX8SCj98N/K8n3SMtGsLiLun8ApxgMQjSdcidU/oHa8a1MK3y/tyRnstE242sFz2n3Y677db2b/lhmTQwpIZt7vkHlGIrdwmjJJZiVtg7Z29Tm61yyg1temPpbewB8BJYfyAGjaziTJ3l+1JvymofZ44s13tVmGnAa7JQI+qY95o1G5Dr4A1V7Z5z81RvWwbsGYDDRif1teuHsKYqq/XcTC27tuG0XfDCouGst1TuSTNzYYyZut87Lh8GzPyadR8GMBLWw2Kjw+YtPqGBPSNQfler/H5RnO4ZhVoO29RbQ1uKjztVE7Pon4Fwk4Y9bIxfb2K/cKrQweVzXb+QzHPyRci/0/oc/269u7Z/xiUjxPn7DaKMy8hbpyu+PkSGbaDZBPqXlrSPeqPqewZ5Go/hI8DfBD4nIp/Jx34W+Bsi8r15CL8K/ASAqn5BRP418H+wjMZPPjkjAcOpcPHh72L12gYZJguMNYFp2XL2vgVnr0TOvnNCTyZCO5Xg3HoxsGgNQbYZ24IqO2sWZXVtJeVccc86P3wwTwx3IX0fWbf+KtBT4HxasJk6dimWVa1e0TySfFhWC/uT4jAV5VLDcT2CPbdpm4fP7+2fzY1MyY/n+/uqX6P3Du993QqekLKKXfc5+hSt3iDXCIwpcDYYX6K7/PNWYI7LuG5dDrbVnxvgoSzL39dF4OsJYO9Le+NRe1z1WMH+Hn1+//5WrR4nNxblf0lXvJjDsSpUbzqTw9Z8kF7GXaMd61LtKJqLrR4/VdqD16xxbiiZiV1qOO126AvwelAulkvefLSgvew4f+W6Kz5eRB/TGv6dFBH5BnABvH7TujyFPODdoSe8e3Q96vn85Tpdv01V3/M0b74VhgFARD6tqh++aT3eTt4tesK7R9ejns9f/rS6PmMx5lGOcpT/H+RoGI5ylKNckdtkGD550wo8pbxb9IR3j65HPZ+//Kl0vTUxhqMc5Si3R26Tx3CUoxzllsiNGwYR+asi8iUR+bKIfPym9TkUEfmqiHxORD4jIp/Ox14Ukd8Qkd/Pv1+4Ab1+RUS+LiKfr45dq5eY/JM8xp8VkQ/dAl0/ISJ/ksf1MyLy0eq1n8m6fklE/so7qOcrIvLfReSLIvIFEfnb+fitGtcn6Pn8xlRVb+wH40n7A+A7gA74XeCDN6nTNTp+FXhwcOwfAB/Pf38c+Ps3oNcPAB8CPv92egEfBf4Thun7fuC3boGunwD+3jXnfhoZ8rcAAAJWSURBVDA/BwvgA/n5iO+Qni8DH8p/3wF+L+tzq8b1CXo+tzG9aY/h+4Avq+pXVLUHfh0r277t8jHgV/Pfvwr8tXdaAVX9H8AbB4cfp9fHgF9Tk98E7ovIy++Mpo/V9XFSyvZV9Q8BL9v/MxdVfVVVfyf/fQY4xcCtGtcn6Pk4eeYxvWnD8H7gj6r/ry3RvmFR4L+IyG/nUnGAb9FcJ5J/v/fGtNuXx+l1W8f5p7IL/ivVduxW6HpAMXBrx/VAT3hOY3rThuGpSrRvWD6iqh8CfhD4SRH5gZtW6P9BbuM4/xLwncD3YkRAv5CP37iuhxQDTzr1mmPvmK7X6PncxvSmDcOzl2i/w6KqX8u/vw78O8wFe81dxvz76zen4Z48Tq9bN86q+pqqTqqagF9mdm1vVNfrKAa4heP6OCqE5zWmN20Y/jfw3SLyARHpMK7IT92wTkVE5CTzXCIiJ8BfxsrLPwX8SD7tR4B/fzMaXpHH6fUp4IdzFP37gYc6l8zfiBzsxQ/L9n9IRBYi8gGeomz/Oep0LcUAt2xcH6fncx3TdyKK+jYR1o9iUdU/AH7upvU50O07sGju7wJfcP2Al4D/Bvx+/v3iDej2rzB3ccBWhB97nF6YK/lP8xh/DvjwLdD1n2ddPpsf3Jer838u6/ol4AffQT3/EuZifxb4TP756G0b1yfo+dzG9Ih8PMpRjnJFbnorcZSjHOUWytEwHOUoR7kiR8NwlKMc5YocDcNRjnKUK3I0DEc5ylGuyNEwHOUoR7kiR8NwlKMc5YocDcNRjnKUK/J/AVVuY6E215CHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "\n",
    "model = AttentionUnetModel()\n",
    "model.load_weights(model_localtion)\n",
    "\n",
    "\n",
    "for file in file_names:\n",
    "    print(file)\n",
    "    grey_img = load_img(os.path.join(test_data_dir,file), target_size=(img_rows, img_cols), grayscale=False)\n",
    "    img = img_to_array(grey_img)\n",
    "    mean = np.mean(img)  # mean for data centering\n",
    "    std = np.std(img)  # std for data normalization\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    img = np.reshape(img,(1,)+img.shape)\n",
    "    results = model.predict(img)\n",
    "\n",
    "    result_img = array_to_img(results[0] * 255 )\n",
    "    plt.imshow(result_img)\n",
    "    result_img.save(os.path.join(test_data_pred_dir, file.split('.')[0] + '_predict.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 256, 256, 32) 896         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_131 (LeakyReLU)     (None, 256, 256, 32) 0           conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 256, 256, 32) 9248        leaky_re_lu_131[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_132 (LeakyReLU)     (None, 256, 256, 32) 0           conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 128, 128, 32) 0           leaky_re_lu_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_133 (LeakyReLU)     (None, 128, 128, 64) 0           conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 128, 128, 64) 36928       leaky_re_lu_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_134 (LeakyReLU)     (None, 128, 128, 64) 0           conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 64, 64, 64)   0           leaky_re_lu_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_135 (LeakyReLU)     (None, 64, 64, 128)  0           conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 64, 64, 128)  147584      leaky_re_lu_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_136 (LeakyReLU)     (None, 64, 64, 128)  0           conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 32, 32, 128)  0           leaky_re_lu_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_137 (LeakyReLU)     (None, 32, 32, 256)  0           conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 32, 32, 256)  590080      leaky_re_lu_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_138 (LeakyReLU)     (None, 32, 32, 256)  0           conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 16, 16, 256)  0           leaky_re_lu_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_139 (LeakyReLU)     (None, 16, 16, 512)  0           conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 512)  2359808     leaky_re_lu_139[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_140 (LeakyReLU)     (None, 16, 16, 512)  0           conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 8, 8, 512)    0           leaky_re_lu_140[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 8, 8, 1024)   4719616     max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_141 (LeakyReLU)     (None, 8, 8, 1024)   0           conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 8, 8, 1024)   9438208     leaky_re_lu_141[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_142 (LeakyReLU)     (None, 8, 8, 1024)   0           conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling2D) (None, 4, 4, 1024)   0           leaky_re_lu_142[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 4, 4, 2048)   18876416    max_pooling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_143 (LeakyReLU)     (None, 4, 4, 2048)   0           conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 4, 4, 2048)   37750784    leaky_re_lu_143[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_144 (LeakyReLU)     (None, 4, 4, 2048)   0           conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_31 (UpSampling2D) (None, 8, 8, 2048)   0           leaky_re_lu_144[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att6skip_conv (Conv2D)          (None, 8, 8, 1024)   1049600     leaky_re_lu_142[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att6direct_conv (Conv2D)        (None, 8, 8, 1024)   2098176     up_sampling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 8, 1024)   0           att6skip_conv[0][0]              \n",
      "                                                                 att6direct_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att6_relu (LeakyReLU)           (None, 8, 8, 1024)   0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att6_att (Conv2D)               (None, 8, 8, 1)      1025        att6_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_31 (Multiply)          (None, 8, 8, 1024)   0           leaky_re_lu_142[0][0]            \n",
      "                                                                 att6_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 3072)   0           multiply_31[0][0]                \n",
      "                                                                 up_sampling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 8, 8, 1024)   28312576    concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_145 (LeakyReLU)     (None, 8, 8, 1024)   0           conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 8, 8, 1024)   9438208     leaky_re_lu_145[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_146 (LeakyReLU)     (None, 8, 8, 1024)   0           conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_32 (UpSampling2D) (None, 16, 16, 1024) 0           leaky_re_lu_146[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att5skip_conv (Conv2D)          (None, 16, 16, 512)  262656      leaky_re_lu_140[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att5direct_conv (Conv2D)        (None, 16, 16, 512)  524800      up_sampling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 16, 16, 512)  0           att5skip_conv[0][0]              \n",
      "                                                                 att5direct_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att5_relu (LeakyReLU)           (None, 16, 16, 512)  0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att5_att (Conv2D)               (None, 16, 16, 1)    513         att5_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_32 (Multiply)          (None, 16, 16, 512)  0           leaky_re_lu_140[0][0]            \n",
      "                                                                 att5_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 16, 16, 1536) 0           multiply_32[0][0]                \n",
      "                                                                 up_sampling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 512)  7078400     concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_147 (LeakyReLU)     (None, 16, 16, 512)  0           conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 512)  2359808     leaky_re_lu_147[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_148 (LeakyReLU)     (None, 16, 16, 512)  0           conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_33 (UpSampling2D) (None, 32, 32, 512)  0           leaky_re_lu_148[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att4skip_conv (Conv2D)          (None, 32, 32, 256)  65792       leaky_re_lu_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att4direct_conv (Conv2D)        (None, 32, 32, 256)  131328      up_sampling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 32, 32, 256)  0           att4skip_conv[0][0]              \n",
      "                                                                 att4direct_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att4_relu (LeakyReLU)           (None, 32, 32, 256)  0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att4_att (Conv2D)               (None, 32, 32, 1)    257         att4_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_33 (Multiply)          (None, 32, 32, 256)  0           leaky_re_lu_138[0][0]            \n",
      "                                                                 att4_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 32, 32, 768)  0           multiply_33[0][0]                \n",
      "                                                                 up_sampling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_149 (LeakyReLU)     (None, 32, 32, 256)  0           conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 32, 32, 256)  590080      leaky_re_lu_149[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_150 (LeakyReLU)     (None, 32, 32, 256)  0           conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_34 (UpSampling2D) (None, 64, 64, 256)  0           leaky_re_lu_150[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att3skip_conv (Conv2D)          (None, 64, 64, 128)  16512       leaky_re_lu_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att3direct_conv (Conv2D)        (None, 64, 64, 128)  32896       up_sampling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 64, 64, 128)  0           att3skip_conv[0][0]              \n",
      "                                                                 att3direct_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att3_relu (LeakyReLU)           (None, 64, 64, 128)  0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att3_att (Conv2D)               (None, 64, 64, 1)    129         att3_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_34 (Multiply)          (None, 64, 64, 128)  0           leaky_re_lu_136[0][0]            \n",
      "                                                                 att3_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 64, 64, 384)  0           multiply_34[0][0]                \n",
      "                                                                 up_sampling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_151 (LeakyReLU)     (None, 64, 64, 128)  0           conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 64, 64, 128)  147584      leaky_re_lu_151[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_152 (LeakyReLU)     (None, 64, 64, 128)  0           conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_35 (UpSampling2D) (None, 128, 128, 128 0           leaky_re_lu_152[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att2skip_conv (Conv2D)          (None, 128, 128, 64) 4160        leaky_re_lu_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att2direct_conv (Conv2D)        (None, 128, 128, 64) 8256        up_sampling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 128, 128, 64) 0           att2skip_conv[0][0]              \n",
      "                                                                 att2direct_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att2_relu (LeakyReLU)           (None, 128, 128, 64) 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att2_att (Conv2D)               (None, 128, 128, 1)  65          att2_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_35 (Multiply)          (None, 128, 128, 64) 0           leaky_re_lu_134[0][0]            \n",
      "                                                                 att2_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 128, 128, 192 0           multiply_35[0][0]                \n",
      "                                                                 up_sampling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_153 (LeakyReLU)     (None, 128, 128, 64) 0           conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 128, 128, 64) 36928       leaky_re_lu_153[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_154 (LeakyReLU)     (None, 128, 128, 64) 0           conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_36 (UpSampling2D) (None, 256, 256, 64) 0           leaky_re_lu_154[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att1skip_conv (Conv2D)          (None, 256, 256, 32) 1056        leaky_re_lu_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att1direct_conv (Conv2D)        (None, 256, 256, 32) 2080        up_sampling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 256, 256, 32) 0           att1skip_conv[0][0]              \n",
      "                                                                 att1direct_conv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "att1_relu (LeakyReLU)           (None, 256, 256, 32) 0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "att1_att (Conv2D)               (None, 256, 256, 1)  33          att1_relu[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_36 (Multiply)          (None, 256, 256, 32) 0           leaky_re_lu_132[0][0]            \n",
      "                                                                 att1_att[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 256, 256, 96) 0           multiply_36[0][0]                \n",
      "                                                                 up_sampling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_155 (LeakyReLU)     (None, 256, 256, 32) 0           conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 256, 256, 32) 9248        leaky_re_lu_155[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_156 (LeakyReLU)     (None, 256, 256, 32) 0           conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 256, 256, 1)  33          leaky_re_lu_156[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 130,020,007\n",
      "Trainable params: 130,020,007\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTestSet(model_location):\n",
    "    model.load_weights(model_location)\n",
    "\n",
    "    file_names = next(os.walk(test_data_dir))[2]\n",
    "    scores = []\n",
    "    for file in file_names:\n",
    "        print (file)\n",
    "        grey_img = load_img(os.path.join(test_data_dir,file), target_size=(img_rows, img_cols), grayscale=False)\n",
    "        mask_img = load_img(os.path.join(test_data_mask_dir,file.split('.')[0]+\"_segmentation.png\"), \n",
    "                            target_size=(img_rows, img_cols), grayscale=True)\n",
    "        img = img_to_array(grey_img)\n",
    "        img_mask = img_to_array(mask_img)\n",
    "\n",
    "        #Preprocess image mask\n",
    "        #img_mask = img_mask /255\n",
    "        #img_mask[img_mask > 0.5] = 1\n",
    "        #img_mask[img_mask <= 0.5] = 0\n",
    "        #Preprocess images\n",
    "        #mean = np.mean(img)  # mean for data centering\n",
    "        #std = np.std(img)  # std for data normalization\n",
    "        #img -= mean\n",
    "        #img /= std\n",
    "        #img, img_mask= normalizeData_rgb(img, img_mask)\n",
    "        img, img_mask = normalizeData(img, img_mask)\n",
    "        img = np.reshape(img,(1,)+img.shape)\n",
    "\n",
    "        pred = model.predict([img])\n",
    "        sess = tf.Session()\n",
    "        score = sess.run(jaccard_coef(img_mask, pred))\n",
    "        print(\"{} -- jaccard index: {}\".format(file,score))\n",
    "        scores.append([file,score])\n",
    "\n",
    "        result_img = array_to_img(pred[0] * 255 )\n",
    "        result_img.save(os.path.join(test_data_pred_dir, file.split('.')[0] + '_predict.jpg'))\n",
    "\n",
    "    with open(result_file, 'w') as f:\n",
    "        f.write(\"filename, jaccard_index\\n\")\n",
    "        for i in range(len(scores)):\n",
    "        #print(scores[i])\n",
    "            f.write(\"{},{}\\n\".format(scores[i][0], scores[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictTestSet(model_localtion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "image = file_names[1]\n",
    "fig = plt.figure()\n",
    "\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(load_img(os.path.join(test_data_dir,image)), shape = (256,256))\n",
    "a.set_title('Image')\n",
    "a.set_axis_off()\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(load_img(os.path.join(test_data_mask_dir,image.split('.')[0]+\"_segmentation.png\")), shape = (256,256))\n",
    "a.set_title('Mask')\n",
    "a.set_axis_off()\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(load_img(os.path.join(test_data_pred_dir,image.split('.')[0]+\"_predict.jpg\")), shape = (256,256))\n",
    "a.set_title('Prediction')\n",
    "a.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_file,\"r\") as f:\n",
    "    data = f.readlines()\n",
    "    scores = [];\n",
    "\n",
    "    for line in data[1:]:\n",
    "        result = line.split(',')\n",
    "        scores.append([result[0], result[1][:-1]])\n",
    "\n",
    "scores = np.array(scores)\n",
    "sorted_scores = scores[scores[:,1].argsort()]\n",
    "print('Test Jaccard Index:{}'.format(scores[:,1].astype(float).mean()))\n",
    "low_scores = scores[scores[:,1].astype(float)<0.65]\n",
    "low_scores = low_scores[low_scores[:,1].argsort()]\n",
    "print('Total number of low prediction (jaccard index < 0.65):{}'.format(len(low_scores)) )\n",
    "print('Threshold Jaccard Index:{}'.format(np.sum(scores[scores[:,1].astype(float)>0.65][:,1].astype(float)) / len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
