{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from imutils import paths\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as keras\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "\n",
    "#import skimage.io as io\n",
    "#import skimage.transform as trans\n",
    "\n",
    "#K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "root_dir = './data'\n",
    "training_data_dir = os.path.join(root_dir, 'train/images')\n",
    "training_data_mask_dir = os.path.join(root_dir, 'train/masks')\n",
    "\n",
    "val_data_dir = os.path.join(root_dir, 'val/images')\n",
    "val_data_pred_dir = os.path.join(root_dir, 'val/predict')\n",
    "val_data_mask_dir = os.path.join(root_dir, 'val/masks')\n",
    "\n",
    "test_data_dir = os.path.join(root_dir, 'test/images')\n",
    "test_data_pred_dir = os.path.join(root_dir, 'test/predict')\n",
    "test_data_mask_dir = os.path.join(root_dir, 'test/masks')\n",
    "\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "\n",
    "file_names = next(os.walk(training_data_dir))[2]\n",
    "image = file_names[0]\n",
    "fig = plt.figure()\n",
    "\n",
    "a = fig.add_subplot(2, 4, 1)\n",
    "imgplot = plt.imshow(load_img(os.path.join(training_data_dir, file_names[3])), shape = (256,256))\n",
    "a.set_title('Image 1')\n",
    "a.set_axis_off()\n",
    "\n",
    "a = fig.add_subplot(2, 4, 2)\n",
    "imgplot = plt.imshow(load_img(os.path.join(training_data_dir,file_names[1])), shape = (256,256))\n",
    "a.set_title('Image 2')\n",
    "a.set_axis_off()\n",
    "\n",
    "a = fig.add_subplot(2, 4, 3)\n",
    "imgplot = plt.imshow(load_img(os.path.join(training_data_dir,file_names[2])), shape = (256,256))\n",
    "a.set_title('Image 3')\n",
    "a.set_axis_off()\n",
    "\n",
    "b = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(load_img(os.path.join(training_data_mask_dir, file_names[3].split('.')[0]+\"_segmentation.png\")), shape = (256,256))\n",
    "b.set_title('Mask 1')\n",
    "b.set_axis_off()\n",
    "\n",
    "b = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(load_img(os.path.join(training_data_mask_dir,file_names[1].split('.')[0]+\"_segmentation.png\")), shape = (256,256))\n",
    "b.set_title('Mask 2')\n",
    "b.set_axis_off()\n",
    "\n",
    "b = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(load_img(os.path.join(training_data_mask_dir,file_names[2].split('.')[0]+\"_segmentation.png\")), shape = (256,256))\n",
    "b.set_title('Mask 3')\n",
    "b.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth)\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    j = -jaccard_coef(y_true, y_pred)\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData_rgb(img,mask):\n",
    "    for i in range(3):\n",
    "        mean = np.mean(img[:,:,i])  # mean for data centering\n",
    "        std = np.std(img[:,:,i])  # std for data normalization\n",
    "        img[:,:,i] -= mean\n",
    "        img[:,:,i] /= std\n",
    "    mask = mask /255\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask <= 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(img,mask):\n",
    "    mean = np.mean(img)  # mean for data centering\n",
    "    std = np.std(img)  # std for data normalization\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    mask = mask /255\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask <= 0.5] = 0\n",
    "    return (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"rgb\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = normalizeData_rgb(img,mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"rgb\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = normalizeData_rgb(img,mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BiggerLeakyUnetModel():\n",
    "    inputs = Input((img_rows, img_cols,3))\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\")(inputs)\n",
    "    acti1 = LeakyReLU(alpha=0.001)(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), padding=\"same\")(acti1)\n",
    "    acti1 = LeakyReLU(alpha=0.001)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(acti1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\")(pool1)\n",
    "    acti2 = LeakyReLU(alpha=0.001)(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), padding=\"same\")(acti2)\n",
    "    acti2 = LeakyReLU(alpha=0.001)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(acti2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\")(pool2)\n",
    "    acti3 = LeakyReLU(alpha=0.001)(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), padding=\"same\")(acti3)\n",
    "    acti3 = LeakyReLU(alpha=0.001)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(acti3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\")(pool3)\n",
    "    acti4 = LeakyReLU(alpha=0.001)(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), padding=\"same\")(acti4)\n",
    "    acti4 = LeakyReLU(alpha=0.001)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(acti4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\")(pool4)\n",
    "    acti5 = LeakyReLU(alpha=0.001)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), padding=\"same\")(acti5)\n",
    "    acti5 = LeakyReLU(alpha=0.001)(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(acti5)\n",
    "\n",
    "    conv6 = Conv2D(1024, (3, 3), padding=\"same\")(pool5)\n",
    "    acti6 = LeakyReLU(alpha=0.001)(conv6)\n",
    "    conv6 = Conv2D(1024, (3, 3), padding=\"same\")(acti6)\n",
    "    acti6 = LeakyReLU(alpha=0.001)(conv6)\n",
    "    pool6 = MaxPooling2D(pool_size=(2, 2))(acti6)\n",
    "    \n",
    "    conv7 = Conv2D(2048, (3, 3), padding=\"same\")(pool6)\n",
    "    acti7 = LeakyReLU(alpha=0.001)(conv7)\n",
    "    conv7 = Conv2D(2048, (3, 3), padding=\"same\")(acti7)\n",
    "    acti7 = LeakyReLU(alpha=0.001)(conv7)\n",
    "\n",
    "    right_up6 = concatenate([UpSampling2D(size=(2, 2))(acti7), acti6], axis=3)\n",
    "    right_conv6 = Conv2D(512, (3, 3), padding=\"same\")(right_up6)\n",
    "    right_acti6 = LeakyReLU(alpha=0.001)(right_conv6)\n",
    "    right_conv6 = Conv2D(512, (3, 3), padding=\"same\")(right_acti6)\n",
    "    right_acti6 = LeakyReLU(alpha=0.001)(right_conv6)\n",
    "\n",
    "    right_up5 = concatenate([UpSampling2D(size=(2, 2))(right_acti6), acti5], axis=3)\n",
    "    right_conv5 = Conv2D(512, (3, 3), padding=\"same\")(right_up5)\n",
    "    right_acti5 = LeakyReLU(alpha=0.001)(right_conv5)\n",
    "    right_conv5 = Conv2D(512, (3, 3), padding=\"same\")(right_acti5)\n",
    "    right_acti5 = LeakyReLU(alpha=0.001)(right_conv5)\n",
    "\n",
    "    right_up4 = concatenate([UpSampling2D(size=(2, 2))(right_acti5), acti4], axis=3)\n",
    "    right_conv4 = Conv2D(256, (3, 3), padding=\"same\")(right_up4)\n",
    "    right_acti4 = LeakyReLU(alpha=0.001)(right_conv4)\n",
    "    right_conv4 = Conv2D(256, (3, 3), padding=\"same\")(right_acti4)\n",
    "    right_acti4 = LeakyReLU(alpha=0.001)(right_conv4)\n",
    "\n",
    "    right_up3 = concatenate([UpSampling2D(size=(2, 2))(right_acti4), acti3], axis=3)\n",
    "    right_conv3 = Conv2D(128, (3, 3), padding=\"same\")(right_up3)\n",
    "    right_acti3 = LeakyReLU(alpha=0.001)(right_conv3)\n",
    "    right_conv3 = Conv2D(128, (3, 3), padding=\"same\")(right_acti3)\n",
    "    right_acti3 = LeakyReLU(alpha=0.001)(right_conv3)\n",
    "\n",
    "    right_up2 = concatenate([UpSampling2D(size=(2, 2))(right_acti3), acti2], axis=3)\n",
    "    right_conv2 = Conv2D(64, (3, 3), padding=\"same\")(right_up2)\n",
    "    right_acti2 = LeakyReLU(alpha=0.001)(right_conv2)\n",
    "    right_conv2 = Conv2D(64, (3, 3), padding=\"same\")(right_acti2)\n",
    "    right_acti2 = LeakyReLU(alpha=0.001)(right_conv2)\n",
    "\n",
    "    right_up1 = concatenate([UpSampling2D(size=(2, 2))(right_acti2), acti1], axis=3)\n",
    "    right_conv1 = Conv2D(32, (3, 3), padding=\"same\")(right_up1)\n",
    "    right_acti1 = LeakyReLU(alpha=0.001)(right_conv1)\n",
    "    right_conv1 = Conv2D(32, (3, 3), padding=\"same\")(right_acti1)\n",
    "    right_acti1 = LeakyReLU(alpha=0.001)(right_conv1)\n",
    "\n",
    "    output = Conv2D(1, (1, 1), activation='sigmoid')(right_acti1)\n",
    "\n",
    "    model = Model(input=inputs, output=output)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=5e-5), loss=jaccard_coef_loss, metrics=[jaccard_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define UNet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionUnetModel():\n",
    "    # 3 - RGB\n",
    "    inputs = Input((img_rows, img_cols, 3))\n",
    "    conv1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(conv5)\n",
    "\n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(conv7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(conv8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-5), loss=jaccard_coef_loss, metrics=[jaccard_coef])\n",
    "    \n",
    "    #model.compile(optimizer=Adam(lr=1e-5), loss=\"binary_crossentropy\", metrics=[jaccard_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/udir/gwang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:82: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "#Training data generation\n",
    "data_gen_args = dict(\n",
    "#    samplewise_center = True,\n",
    "#    samplewise_std_normalization = True,\n",
    "    rotation_range=180,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip = True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#Validation data generation\n",
    "data_val_gen_args = dict(\n",
    "    #samplewise_center = True,\n",
    "    #samplewise_std_normalization = True\n",
    "    )\n",
    "\n",
    "#Create UNet Model\n",
    "#model = FullUnetModel()\n",
    "#model = UnetModel()\n",
    "model = BiggerLeakyUnetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup generator\n",
    "batch_size = 3\n",
    "\n",
    "myGene = trainGenerator(batch_size,'data/train','images','masks',data_gen_args)\n",
    "myValGene = validationGenerator(batch_size,'data/val','images','masks',data_val_gen_args)\n",
    "\n",
    "#Setup Checkpoint to only capture best estimate\n",
    "model_checkpoint = ModelCheckpoint('attention_unet_lesion.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "\n",
    "#Enable tensorboard\n",
    "tensorBoard = TensorBoard(\n",
    "    log_dir='./logs', \n",
    "    histogram_freq=0, \n",
    "    batch_size=batch_size, \n",
    "    write_graph=True, \n",
    "    write_grads=False, \n",
    "    write_images=True, \n",
    "    embeddings_freq=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Found 518 images belonging to 1 classes.\n",
      "Found 1556 images belonging to 1 classes.\n",
      "Found 518 images belonging to 1 classes.\n",
      "Found 1556 images belonging to 1 classes.\n",
      "600/600 [==============================] - 532s 886ms/step - loss: -0.3192 - jaccard_coef: 0.3192 - val_loss: -0.5181 - val_jaccard_coef: 0.5181\n",
      "\n",
      "Epoch 00001: loss improved from inf to -0.31911, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 535s 891ms/step - loss: -0.5358 - jaccard_coef: 0.5358 - val_loss: -0.5974 - val_jaccard_coef: 0.5974\n",
      "\n",
      "Epoch 00002: loss improved from -0.31911 to -0.53573, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 514s 856ms/step - loss: -0.6229 - jaccard_coef: 0.6229 - val_loss: -0.6804 - val_jaccard_coef: 0.6804\n",
      "\n",
      "Epoch 00003: loss improved from -0.53573 to -0.62283, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 508s 847ms/step - loss: -0.6740 - jaccard_coef: 0.6740 - val_loss: -0.7227 - val_jaccard_coef: 0.7227\n",
      "\n",
      "Epoch 00004: loss improved from -0.62283 to -0.67391, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 525s 875ms/step - loss: -0.6971 - jaccard_coef: 0.6971 - val_loss: -0.7379 - val_jaccard_coef: 0.7379\n",
      "\n",
      "Epoch 00005: loss improved from -0.67391 to -0.69724, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 527s 879ms/step - loss: -0.7104 - jaccard_coef: 0.7104 - val_loss: -0.7274 - val_jaccard_coef: 0.7274\n",
      "\n",
      "Epoch 00006: loss improved from -0.69724 to -0.71037, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 523s 872ms/step - loss: -0.7152 - jaccard_coef: 0.7152 - val_loss: -0.7342 - val_jaccard_coef: 0.7342\n",
      "\n",
      "Epoch 00007: loss improved from -0.71037 to -0.71540, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 517s 862ms/step - loss: -0.7253 - jaccard_coef: 0.7253 - val_loss: -0.6773 - val_jaccard_coef: 0.6773\n",
      "\n",
      "Epoch 00008: loss improved from -0.71540 to -0.72535, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 524s 874ms/step - loss: -0.7270 - jaccard_coef: 0.7270 - val_loss: -0.7662 - val_jaccard_coef: 0.7662\n",
      "\n",
      "Epoch 00009: loss improved from -0.72535 to -0.72707, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 525s 876ms/step - loss: -0.7398 - jaccard_coef: 0.7398 - val_loss: -0.7601 - val_jaccard_coef: 0.7601\n",
      "\n",
      "Epoch 00010: loss improved from -0.72707 to -0.73966, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 518s 863ms/step - loss: -0.7343 - jaccard_coef: 0.7343 - val_loss: -0.7505 - val_jaccard_coef: 0.7505\n",
      "\n",
      "Epoch 00011: loss did not improve from -0.73966\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 525s 875ms/step - loss: -0.7418 - jaccard_coef: 0.7418 - val_loss: -0.7508 - val_jaccard_coef: 0.7508\n",
      "\n",
      "Epoch 00012: loss improved from -0.73966 to -0.74179, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 515s 859ms/step - loss: -0.7454 - jaccard_coef: 0.7454 - val_loss: -0.7771 - val_jaccard_coef: 0.7771\n",
      "\n",
      "Epoch 00013: loss improved from -0.74179 to -0.74558, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 525s 876ms/step - loss: -0.7454 - jaccard_coef: 0.7454 - val_loss: -0.7577 - val_jaccard_coef: 0.7577\n",
      "\n",
      "Epoch 00014: loss did not improve from -0.74558\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 520s 867ms/step - loss: -0.7586 - jaccard_coef: 0.7586 - val_loss: -0.7699 - val_jaccard_coef: 0.7699\n",
      "\n",
      "Epoch 00015: loss improved from -0.74558 to -0.75848, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 519s 865ms/step - loss: -0.7468 - jaccard_coef: 0.7468 - val_loss: -0.7715 - val_jaccard_coef: 0.7715\n",
      "\n",
      "Epoch 00016: loss did not improve from -0.75848\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 525s 875ms/step - loss: -0.7486 - jaccard_coef: 0.7486 - val_loss: -0.7730 - val_jaccard_coef: 0.7730\n",
      "\n",
      "Epoch 00017: loss did not improve from -0.75848\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 515s 858ms/step - loss: -0.7542 - jaccard_coef: 0.7542 - val_loss: -0.7489 - val_jaccard_coef: 0.7489\n",
      "\n",
      "Epoch 00018: loss did not improve from -0.75848\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 528s 881ms/step - loss: -0.7592 - jaccard_coef: 0.7592 - val_loss: -0.7905 - val_jaccard_coef: 0.7905\n",
      "\n",
      "Epoch 00019: loss improved from -0.75848 to -0.75914, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 531s 884ms/step - loss: -0.7630 - jaccard_coef: 0.7630 - val_loss: -0.7721 - val_jaccard_coef: 0.7721\n",
      "\n",
      "Epoch 00020: loss improved from -0.75914 to -0.76351, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 519s 866ms/step - loss: -0.7491 - jaccard_coef: 0.7491 - val_loss: -0.7305 - val_jaccard_coef: 0.7305\n",
      "\n",
      "Epoch 00021: loss did not improve from -0.76351\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 513s 854ms/step - loss: -0.7617 - jaccard_coef: 0.7617 - val_loss: -0.7777 - val_jaccard_coef: 0.7777\n",
      "\n",
      "Epoch 00022: loss did not improve from -0.76351\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 543s 905ms/step - loss: -0.7690 - jaccard_coef: 0.7690 - val_loss: -0.7607 - val_jaccard_coef: 0.7607\n",
      "\n",
      "Epoch 00023: loss improved from -0.76351 to -0.76895, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 514s 856ms/step - loss: -0.7686 - jaccard_coef: 0.7686 - val_loss: -0.7795 - val_jaccard_coef: 0.7795\n",
      "\n",
      "Epoch 00024: loss did not improve from -0.76895\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 519s 864ms/step - loss: -0.7702 - jaccard_coef: 0.7702 - val_loss: -0.7635 - val_jaccard_coef: 0.7635\n",
      "\n",
      "Epoch 00025: loss improved from -0.76895 to -0.77012, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 528s 880ms/step - loss: -0.7735 - jaccard_coef: 0.7735 - val_loss: -0.7851 - val_jaccard_coef: 0.7851\n",
      "\n",
      "Epoch 00026: loss improved from -0.77012 to -0.77348, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 518s 863ms/step - loss: -0.7671 - jaccard_coef: 0.7671 - val_loss: -0.7716 - val_jaccard_coef: 0.7716\n",
      "\n",
      "Epoch 00027: loss did not improve from -0.77348\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 523s 871ms/step - loss: -0.7743 - jaccard_coef: 0.7743 - val_loss: -0.7861 - val_jaccard_coef: 0.7861\n",
      "\n",
      "Epoch 00028: loss improved from -0.77348 to -0.77431, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 528s 881ms/step - loss: -0.7657 - jaccard_coef: 0.7657 - val_loss: -0.7672 - val_jaccard_coef: 0.7672\n",
      "\n",
      "Epoch 00029: loss did not improve from -0.77431\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 522s 869ms/step - loss: -0.7716 - jaccard_coef: 0.7716 - val_loss: -0.7516 - val_jaccard_coef: 0.7516\n",
      "\n",
      "Epoch 00030: loss did not improve from -0.77431\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 519s 864ms/step - loss: -0.7746 - jaccard_coef: 0.7746 - val_loss: -0.7840 - val_jaccard_coef: 0.7840\n",
      "\n",
      "Epoch 00031: loss improved from -0.77431 to -0.77465, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 528s 880ms/step - loss: -0.7753 - jaccard_coef: 0.7753 - val_loss: -0.7569 - val_jaccard_coef: 0.7569\n",
      "\n",
      "Epoch 00032: loss improved from -0.77465 to -0.77537, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 526s 876ms/step - loss: -0.7783 - jaccard_coef: 0.7783 - val_loss: -0.7759 - val_jaccard_coef: 0.7759\n",
      "\n",
      "Epoch 00033: loss improved from -0.77537 to -0.77853, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 513s 855ms/step - loss: -0.7728 - jaccard_coef: 0.7728 - val_loss: -0.7896 - val_jaccard_coef: 0.7896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: loss did not improve from -0.77853\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 524s 873ms/step - loss: -0.7779 - jaccard_coef: 0.7779 - val_loss: -0.7945 - val_jaccard_coef: 0.7945\n",
      "\n",
      "Epoch 00035: loss did not improve from -0.77853\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 523s 872ms/step - loss: -0.7724 - jaccard_coef: 0.7724 - val_loss: -0.7714 - val_jaccard_coef: 0.7714\n",
      "\n",
      "Epoch 00036: loss did not improve from -0.77853\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 525s 875ms/step - loss: -0.7739 - jaccard_coef: 0.7739 - val_loss: -0.7759 - val_jaccard_coef: 0.7759\n",
      "\n",
      "Epoch 00037: loss did not improve from -0.77853\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 518s 864ms/step - loss: -0.7836 - jaccard_coef: 0.7836 - val_loss: -0.7779 - val_jaccard_coef: 0.7779\n",
      "\n",
      "Epoch 00038: loss improved from -0.77853 to -0.78355, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 521s 868ms/step - loss: -0.7801 - jaccard_coef: 0.7801 - val_loss: -0.7696 - val_jaccard_coef: 0.7696\n",
      "\n",
      "Epoch 00039: loss did not improve from -0.78355\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 514s 857ms/step - loss: -0.7786 - jaccard_coef: 0.7786 - val_loss: -0.7881 - val_jaccard_coef: 0.7881\n",
      "\n",
      "Epoch 00040: loss did not improve from -0.78355\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 527s 879ms/step - loss: -0.7905 - jaccard_coef: 0.7905 - val_loss: -0.7947 - val_jaccard_coef: 0.7947\n",
      "\n",
      "Epoch 00041: loss improved from -0.78355 to -0.79053, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 526s 876ms/step - loss: -0.7790 - jaccard_coef: 0.7790 - val_loss: -0.7824 - val_jaccard_coef: 0.7824\n",
      "\n",
      "Epoch 00042: loss did not improve from -0.79053\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 522s 870ms/step - loss: -0.7809 - jaccard_coef: 0.7809 - val_loss: -0.7753 - val_jaccard_coef: 0.7753\n",
      "\n",
      "Epoch 00043: loss did not improve from -0.79053\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 531s 885ms/step - loss: -0.7873 - jaccard_coef: 0.7873 - val_loss: -0.7924 - val_jaccard_coef: 0.7924\n",
      "\n",
      "Epoch 00044: loss did not improve from -0.79053\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 517s 862ms/step - loss: -0.7852 - jaccard_coef: 0.7852 - val_loss: -0.7723 - val_jaccard_coef: 0.7723\n",
      "\n",
      "Epoch 00045: loss did not improve from -0.79053\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 522s 869ms/step - loss: -0.7745 - jaccard_coef: 0.7745 - val_loss: -0.7836 - val_jaccard_coef: 0.7836\n",
      "\n",
      "Epoch 00046: loss did not improve from -0.79053\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 517s 862ms/step - loss: -0.7803 - jaccard_coef: 0.7803 - val_loss: -0.7968 - val_jaccard_coef: 0.7968\n",
      "\n",
      "Epoch 00047: loss did not improve from -0.79053\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 532s 886ms/step - loss: -0.7972 - jaccard_coef: 0.7972 - val_loss: -0.7845 - val_jaccard_coef: 0.7845\n",
      "\n",
      "Epoch 00048: loss improved from -0.79053 to -0.79723, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 519s 865ms/step - loss: -0.7720 - jaccard_coef: 0.7720 - val_loss: -0.7807 - val_jaccard_coef: 0.7807\n",
      "\n",
      "Epoch 00049: loss did not improve from -0.79723\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 533s 888ms/step - loss: -0.7953 - jaccard_coef: 0.7953 - val_loss: -0.7759 - val_jaccard_coef: 0.7759\n",
      "\n",
      "Epoch 00050: loss did not improve from -0.79723\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 520s 867ms/step - loss: -0.7841 - jaccard_coef: 0.7841 - val_loss: -0.7853 - val_jaccard_coef: 0.7853\n",
      "\n",
      "Epoch 00051: loss did not improve from -0.79723\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 518s 864ms/step - loss: -0.7903 - jaccard_coef: 0.7903 - val_loss: -0.7886 - val_jaccard_coef: 0.7886\n",
      "\n",
      "Epoch 00052: loss did not improve from -0.79723\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 540s 899ms/step - loss: -0.7863 - jaccard_coef: 0.7863 - val_loss: -0.7905 - val_jaccard_coef: 0.7905\n",
      "\n",
      "Epoch 00053: loss did not improve from -0.79723\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 509s 849ms/step - loss: -0.7894 - jaccard_coef: 0.7894 - val_loss: -0.7927 - val_jaccard_coef: 0.7927\n",
      "\n",
      "Epoch 00054: loss did not improve from -0.79723\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 533s 888ms/step - loss: -0.7956 - jaccard_coef: 0.7956 - val_loss: -0.7969 - val_jaccard_coef: 0.7969\n",
      "\n",
      "Epoch 00055: loss did not improve from -0.79723\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 511s 852ms/step - loss: -0.7972 - jaccard_coef: 0.7972 - val_loss: -0.7896 - val_jaccard_coef: 0.7896\n",
      "\n",
      "Epoch 00056: loss did not improve from -0.79723\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 522s 870ms/step - loss: -0.7896 - jaccard_coef: 0.7896 - val_loss: -0.7561 - val_jaccard_coef: 0.7561\n",
      "\n",
      "Epoch 00057: loss did not improve from -0.79723\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 519s 864ms/step - loss: -0.7864 - jaccard_coef: 0.7864 - val_loss: -0.7859 - val_jaccard_coef: 0.7859\n",
      "\n",
      "Epoch 00058: loss did not improve from -0.79723\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 528s 880ms/step - loss: -0.7940 - jaccard_coef: 0.7940 - val_loss: -0.8008 - val_jaccard_coef: 0.8008\n",
      "\n",
      "Epoch 00059: loss did not improve from -0.79723\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 520s 867ms/step - loss: -0.7958 - jaccard_coef: 0.7958 - val_loss: -0.7920 - val_jaccard_coef: 0.7920\n",
      "\n",
      "Epoch 00060: loss did not improve from -0.79723\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 534s 889ms/step - loss: -0.7919 - jaccard_coef: 0.7919 - val_loss: -0.8154 - val_jaccard_coef: 0.8154\n",
      "\n",
      "Epoch 00061: loss did not improve from -0.79723\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 510s 851ms/step - loss: -0.7978 - jaccard_coef: 0.7978 - val_loss: -0.7832 - val_jaccard_coef: 0.7832\n",
      "\n",
      "Epoch 00062: loss improved from -0.79723 to -0.79774, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 525s 875ms/step - loss: -0.7953 - jaccard_coef: 0.7953 - val_loss: -0.8015 - val_jaccard_coef: 0.8015\n",
      "\n",
      "Epoch 00063: loss did not improve from -0.79774\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 525s 874ms/step - loss: -0.7919 - jaccard_coef: 0.7919 - val_loss: -0.7956 - val_jaccard_coef: 0.7956\n",
      "\n",
      "Epoch 00064: loss did not improve from -0.79774\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 517s 862ms/step - loss: -0.7979 - jaccard_coef: 0.7979 - val_loss: -0.7941 - val_jaccard_coef: 0.7941\n",
      "\n",
      "Epoch 00065: loss improved from -0.79774 to -0.79801, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 537s 894ms/step - loss: -0.7982 - jaccard_coef: 0.7982 - val_loss: -0.8033 - val_jaccard_coef: 0.8033\n",
      "\n",
      "Epoch 00066: loss improved from -0.79801 to -0.79819, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 523s 872ms/step - loss: -0.8035 - jaccard_coef: 0.8035 - val_loss: -0.8045 - val_jaccard_coef: 0.8045\n",
      "\n",
      "Epoch 00067: loss improved from -0.79819 to -0.80343, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 522s 871ms/step - loss: -0.7928 - jaccard_coef: 0.7928 - val_loss: -0.7999 - val_jaccard_coef: 0.7999\n",
      "\n",
      "Epoch 00068: loss did not improve from -0.80343\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 515s 859ms/step - loss: -0.8006 - jaccard_coef: 0.8006 - val_loss: -0.8077 - val_jaccard_coef: 0.8077\n",
      "\n",
      "Epoch 00069: loss did not improve from -0.80343\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 530s 883ms/step - loss: -0.8038 - jaccard_coef: 0.8038 - val_loss: -0.8110 - val_jaccard_coef: 0.8110\n",
      "\n",
      "Epoch 00070: loss improved from -0.80343 to -0.80374, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 524s 874ms/step - loss: -0.7983 - jaccard_coef: 0.7983 - val_loss: -0.7971 - val_jaccard_coef: 0.7971\n",
      "\n",
      "Epoch 00071: loss did not improve from -0.80374\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 527s 878ms/step - loss: -0.7911 - jaccard_coef: 0.7911 - val_loss: -0.7964 - val_jaccard_coef: 0.7964\n",
      "\n",
      "Epoch 00072: loss did not improve from -0.80374\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 520s 867ms/step - loss: -0.8030 - jaccard_coef: 0.8030 - val_loss: -0.8033 - val_jaccard_coef: 0.8033\n",
      "\n",
      "Epoch 00073: loss did not improve from -0.80374\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 523s 871ms/step - loss: -0.7888 - jaccard_coef: 0.7888 - val_loss: -0.7987 - val_jaccard_coef: 0.7987\n",
      "\n",
      "Epoch 00074: loss did not improve from -0.80374\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 518s 863ms/step - loss: -0.7939 - jaccard_coef: 0.7939 - val_loss: -0.7930 - val_jaccard_coef: 0.7930\n",
      "\n",
      "Epoch 00075: loss did not improve from -0.80374\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 516s 861ms/step - loss: -0.8021 - jaccard_coef: 0.8021 - val_loss: -0.8060 - val_jaccard_coef: 0.8060\n",
      "\n",
      "Epoch 00076: loss did not improve from -0.80374\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 527s 879ms/step - loss: -0.8016 - jaccard_coef: 0.8016 - val_loss: -0.7768 - val_jaccard_coef: 0.7768\n",
      "\n",
      "Epoch 00077: loss did not improve from -0.80374\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 516s 861ms/step - loss: -0.7988 - jaccard_coef: 0.7988 - val_loss: -0.8194 - val_jaccard_coef: 0.8194\n",
      "\n",
      "Epoch 00078: loss did not improve from -0.80374\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 517s 862ms/step - loss: -0.8003 - jaccard_coef: 0.8003 - val_loss: -0.7860 - val_jaccard_coef: 0.7860\n",
      "\n",
      "Epoch 00079: loss did not improve from -0.80374\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 539s 898ms/step - loss: -0.8014 - jaccard_coef: 0.8014 - val_loss: -0.8154 - val_jaccard_coef: 0.8154\n",
      "\n",
      "Epoch 00080: loss did not improve from -0.80374\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 507s 846ms/step - loss: -0.8071 - jaccard_coef: 0.8071 - val_loss: -0.8115 - val_jaccard_coef: 0.8115\n",
      "\n",
      "Epoch 00081: loss improved from -0.80374 to -0.80701, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 537s 895ms/step - loss: -0.7956 - jaccard_coef: 0.7956 - val_loss: -0.7946 - val_jaccard_coef: 0.7946\n",
      "\n",
      "Epoch 00082: loss did not improve from -0.80701\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 521s 868ms/step - loss: -0.8091 - jaccard_coef: 0.8091 - val_loss: -0.8159 - val_jaccard_coef: 0.8159\n",
      "\n",
      "Epoch 00083: loss improved from -0.80701 to -0.80906, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 517s 861ms/step - loss: -0.8011 - jaccard_coef: 0.8011 - val_loss: -0.8013 - val_jaccard_coef: 0.8013\n",
      "\n",
      "Epoch 00084: loss did not improve from -0.80906\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 533s 889ms/step - loss: -0.8002 - jaccard_coef: 0.8002 - val_loss: -0.8007 - val_jaccard_coef: 0.8007\n",
      "\n",
      "Epoch 00085: loss did not improve from -0.80906\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 520s 867ms/step - loss: -0.8121 - jaccard_coef: 0.8121 - val_loss: -0.7972 - val_jaccard_coef: 0.7972\n",
      "\n",
      "Epoch 00086: loss improved from -0.80906 to -0.81206, saving model to attention_unet_lesion.hdf5\n",
      "Epoch 87/100\n",
      "599/600 [============================>.] - ETA: 0s - loss: -0.8078 - jaccard_coef: 0.8078"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train\n",
    "history = model.fit_generator(\n",
    "    myGene,\n",
    "    steps_per_epoch = 600, \n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint,tensorBoard],\n",
    "    validation_data=myValGene,\n",
    "    validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue traing\n",
    "#Use initial_epoch \n",
    "'''\n",
    "history2 = model.fit_generator(\n",
    "    myGene,\n",
    "    steps_per_epoch = 1000, \n",
    "    epochs=100,\n",
    "    callbacks=[model_checkpoint,tensorBoard], \n",
    "    initial_epoch = 125,\n",
    "    validation_data=myValGene,\n",
    "    validation_steps=200)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['jaccard_coef'])\n",
    "plt.plot(history.history['val_jaccard_coef'])\n",
    "plt.title('Coefficiency')\n",
    "plt.ylabel('Coefficiency')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "plt.legend(['Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\li_ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    }
   ],
   "source": [
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "\n",
    "model = AttentionUnetModel()\n",
    "model.load_weights(\"unet_lesion.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Model in module keras.engine.training object:\n",
      "\n",
      "class Model(keras.engine.network.Network)\n",
      " |  Model(*args, **kwargs)\n",
      " |  \n",
      " |  The `Model` class adds training & evaluation routines to a `Network`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Model\n",
      " |      keras.engine.network.Network\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          optimizer: String (name of optimizer) or optimizer instance.\n",
      " |              See [optimizers](/optimizers).\n",
      " |          loss: String (name of objective function) or objective function.\n",
      " |              See [losses](/losses).\n",
      " |              If the model has multiple outputs, you can use a different loss\n",
      " |              on each output by passing a dictionary or a list of losses.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the sum of all individual losses.\n",
      " |          metrics: List of metrics to be evaluated by the model\n",
      " |              during training and testing.\n",
      " |              Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary,\n",
      " |              such as `metrics={'output_a': 'accuracy'}`.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions\n",
      " |              of different model outputs.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the *weighted sum* of all individual losses,\n",
      " |              weighted by the `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping\n",
      " |              to the model's outputs. If a tensor, it is expected to map\n",
      " |              output names (strings) to scalar coefficients.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create placeholders for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensors (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument. It can be\n",
      " |              a single tensor (for a single-output model), a list of tensors,\n",
      " |              or a dict mapping output names to target tensors.\n",
      " |          **kwargs: When using the Theano/CNTK backends, these arguments\n",
      " |              are passed into `K.function`.\n",
      " |              When using the TensorFlow backend,\n",
      " |              these arguments are passed into `tf.Session.run`.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of test data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per evaluation step.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          verbose: 0 or 1. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the test samples, used for weighting the loss function.\n",
      " |              You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data\n",
      " |      as accepted by `test_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding tuples (inputs, targets)\n",
      " |              or (inputs, targets, sample_weights)\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: if True, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
      " |      Trains the model for a given number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling.\n",
      " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
      " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined.\n",
      " |          validation_steps: Only relevant if `steps_per_epoch`\n",
      " |              is specified. Total number of steps (batches of samples)\n",
      " |              to validate before stopping.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Trains the model on data generated batch-by-batch by a Python generator\n",
      " |      (or an instance of `Sequence`).\n",
      " |      \n",
      " |      The generator is run in parallel to the model, for efficiency.\n",
      " |      For instance, this allows you to do real-time data augmentation\n",
      " |      on images on CPU in parallel to training your model on GPU.\n",
      " |      \n",
      " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
      " |      and guarantees the single use of every input per epoch when\n",
      " |      using `use_multiprocessing=True`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: A generator or an instance of `Sequence`\n",
      " |              (`keras.utils.Sequence`) object in order to avoid\n",
      " |              duplicate data when using multiprocessing.\n",
      " |              The output of the generator must be either\n",
      " |              - a tuple `(inputs, targets)`\n",
      " |              - a tuple `(inputs, targets, sample_weights)`.\n",
      " |              This tuple (a single output of the generator) makes a single\n",
      " |              batch. Therefore, all arrays in this tuple must have the same\n",
      " |              length (equal to the size of this batch). Different batches may\n",
      " |              have different sizes. For example, the last batch of the epoch\n",
      " |              is commonly smaller than the others, if the size of the dataset\n",
      " |              is not divisible by the batch size.\n",
      " |              The generator is expected to loop over its data\n",
      " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
      " |              batches have been seen by the model.\n",
      " |          steps_per_epoch: Integer.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before declaring one epoch\n",
      " |              finished and starting the next epoch. It should typically\n",
      " |              be equal to the number of samples of your dataset\n",
      " |              divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire data provided,\n",
      " |              as defined by `steps_per_epoch`.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_data: This can be either\n",
      " |              - a generator or a `Sequence` object for the validation data\n",
      " |              - tuple `(x_val, y_val)`\n",
      " |              - tuple `(x_val, y_val, val_sample_weights)`\n",
      " |              on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |          validation_steps: Only relevant if `validation_data`\n",
      " |              is a generator. Total number of steps (batches of samples)\n",
      " |              to yield from `validation_data` generator before stopping\n",
      " |              at the end of every epoch. It should typically\n",
      " |              be equal to the number of samples of your\n",
      " |              validation dataset divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(validation_data)` as a number of steps.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only). This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples\n",
      " |              from an under-represented class.\n",
      " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation\n",
      " |              relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      " |              the beginning of each epoch. Only used with instances\n",
      " |              of `Sequence` (`keras.utils.Sequence`).\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields data in an invalid format.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |      def generate_arrays_from_file(path):\n",
      " |          while True:\n",
      " |              with open(path) as f:\n",
      " |                  for line in f:\n",
      " |                      # create numpy arrays of input data\n",
      " |                      # and labels, from each line in the file\n",
      " |                      x1, x2, y = process_line(line)\n",
      " |                      yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      " |      \n",
      " |      model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      " |                          steps_per_epoch=10000, epochs=10)\n",
      " |      ```\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: The input data, as a Numpy array\n",
      " |              (or list of Numpy arrays if the model has multiple inputs).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data as accepted by\n",
      " |      `predict_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding batches of input samples\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: Maximum size for the generator queue.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: If `True`, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Input samples, as a Numpy array.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |  \n",
      " |  test_on_batch(self, x, y, sample_weight=None)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of test data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |          class_weight: Optional dictionary mapping\n",
      " |              class indices (integers) to\n",
      " |              a weight (float) to apply to the model's loss for the samples\n",
      " |              from this class during training.\n",
      " |              This can be useful to tell the model to \"pay more attention\" to\n",
      " |              samples from an under-represented class.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.network.Network:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  call(self, inputs, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      A model is callable on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      \n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False)\n",
      " |      Loads all layer weights from a HDF5 save file.\n",
      " |      \n",
      " |      If `by_name` is False (default) weights are loaded\n",
      " |      based on the network's topology, meaning the architecture\n",
      " |      should be the same as when the weights were saved.\n",
      " |      Note that layers that don't have weights are not taken\n",
      " |      into account in the topological ordering, so adding or\n",
      " |      removing layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers\n",
      " |      only if they share the same name. This is useful\n",
      " |      for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |          by_name: Boolean, whether to load weights by name\n",
      " |              or by topological order.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers\n",
      " |              where there is a mismatch in the number of weights,\n",
      " |              or a mismatch in the shape of the weight\n",
      " |              (only valid when `by_name`=True).\n",
      " |          reshape: Reshape weights to fit the layer when the correct number\n",
      " |              of weight arrays is present but their shape does not match.\n",
      " |      \n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  run_internal_graph(self, inputs, masks=None)\n",
      " |      Computes output tensors for new inputs.\n",
      " |      \n",
      " |      # Note:\n",
      " |          - Expects `inputs` to be a list (potentially with 1 element).\n",
      " |          - Can be run on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: List of tensors\n",
      " |          masks: List of masks (tensors or None).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Three lists: output_tensors, output_masks, output_shapes\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
      " |      Saves the model to a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model`\n",
      " |      is a compiled model ready to be used (unless the saved model\n",
      " |      was never compiled in the first place).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Dumps all layer weights to a HDF5 file.\n",
      " |      \n",
      " |      The weight file has:\n",
      " |          - `layer_names` (attribute), a list of strings\n",
      " |              (ordered names of model layers).\n",
      " |          - For every layer, a `group` named `layer.name`\n",
      " |              - For every such layer group, a group attribute `weight_names`,\n",
      " |                  a list of strings\n",
      " |                  (ordered names of weights tensor of the layer).\n",
      " |              - For every weight in the layer, a dataset\n",
      " |                  storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the model.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: A list of Numpy arrays with shapes and types matching\n",
      " |              the output of `model.get_weights()`.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |              It defaults to `print` (prints to stdout).\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A YAML string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.network.Network:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A model instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.network.Network:\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the model's input specs.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  layers\n",
      " |  \n",
      " |  losses\n",
      " |      Retrieves the model's losses.\n",
      " |      \n",
      " |      Will only include losses that are either\n",
      " |      unconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include losses that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of loss tensors.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |      Retrieves the model's updates.\n",
      " |      \n",
      " |      Will only include updates that are either\n",
      " |      unconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include updates that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  uses_learning_phase\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\li_ni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC_0000034.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\li_ni\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC_0000054.jpg\n",
      "ISIC_0000058.jpg\n",
      "ISIC_0000060.jpg\n",
      "ISIC_0000064.jpg\n",
      "ISIC_0000079.jpg\n",
      "ISIC_0000081.jpg\n",
      "ISIC_0000112.jpg\n",
      "ISIC_0000117.jpg\n",
      "ISIC_0000128.jpg\n",
      "ISIC_0000137.jpg\n",
      "ISIC_0000172.jpg\n",
      "ISIC_0000184.jpg\n",
      "ISIC_0000207.jpg\n",
      "ISIC_0000219.jpg\n",
      "ISIC_0000222.jpg\n",
      "ISIC_0000224.jpg\n",
      "ISIC_0000228.jpg\n",
      "ISIC_0000262.jpg\n",
      "ISIC_0000269.jpg\n",
      "ISIC_0000274.jpg\n",
      "ISIC_0000277.jpg\n",
      "ISIC_0000279.jpg\n",
      "ISIC_0000299.jpg\n",
      "ISIC_0000310.jpg\n",
      "ISIC_0000331.jpg\n",
      "ISIC_0000336.jpg\n",
      "ISIC_0000349.jpg\n",
      "ISIC_0000350.jpg\n",
      "ISIC_0000357.jpg\n",
      "ISIC_0000361.jpg\n",
      "ISIC_0000363.jpg\n",
      "ISIC_0000376.jpg\n",
      "ISIC_0000383.jpg\n",
      "ISIC_0000425.jpg\n",
      "ISIC_0000426.jpg\n",
      "ISIC_0000470.jpg\n",
      "ISIC_0000479.jpg\n",
      "ISIC_0000482.jpg\n",
      "ISIC_0000487.jpg\n",
      "ISIC_0000490.jpg\n",
      "ISIC_0000513.jpg\n",
      "ISIC_0000519.jpg\n",
      "ISIC_0000520.jpg\n",
      "ISIC_0000900.jpg\n",
      "ISIC_0001103.jpg\n",
      "ISIC_0001163.jpg\n",
      "ISIC_0001385.jpg\n",
      "ISIC_0002093.jpg\n",
      "ISIC_0002353.jpg\n",
      "ISIC_0002780.jpg\n",
      "ISIC_0002871.jpg\n",
      "ISIC_0003005.jpg\n",
      "ISIC_0006114.jpg\n",
      "ISIC_0006350.jpg\n",
      "ISIC_0006711.jpg\n",
      "ISIC_0006776.jpg\n",
      "ISIC_0007156.jpg\n",
      "ISIC_0007788.jpg\n",
      "ISIC_0007796.jpg\n",
      "ISIC_0008116.jpg\n",
      "ISIC_0008280.jpg\n",
      "ISIC_0008528.jpg\n",
      "ISIC_0008785.jpg\n",
      "ISIC_0008998.jpg\n",
      "ISIC_0009583.jpg\n",
      "ISIC_0009873.jpg\n",
      "ISIC_0009883.jpg\n",
      "ISIC_0009914.jpg\n",
      "ISIC_0009927.jpg\n",
      "ISIC_0009939.jpg\n",
      "ISIC_0009951.jpg\n",
      "ISIC_0009958.jpg\n",
      "ISIC_0009975.jpg\n",
      "ISIC_0009977.jpg\n",
      "ISIC_0009990.jpg\n",
      "ISIC_0009992.jpg\n",
      "ISIC_0010011.jpg\n",
      "ISIC_0010025.jpg\n",
      "ISIC_0010029.jpg\n",
      "ISIC_0010041.jpg\n",
      "ISIC_0010042.jpg\n",
      "ISIC_0010054.jpg\n",
      "ISIC_0010066.jpg\n",
      "ISIC_0010077.jpg\n",
      "ISIC_0010102.jpg\n",
      "ISIC_0010184.jpg\n",
      "ISIC_0010186.jpg\n",
      "ISIC_0010201.jpg\n",
      "ISIC_0010215.jpg\n",
      "ISIC_0010228.jpg\n",
      "ISIC_0010231.jpg\n",
      "ISIC_0010233.jpg\n",
      "ISIC_0010241.jpg\n",
      "ISIC_0010256.jpg\n",
      "ISIC_0010262.jpg\n",
      "ISIC_0010264.jpg\n",
      "ISIC_0010326.jpg\n",
      "ISIC_0010330.jpg\n",
      "ISIC_0010341.jpg\n",
      "ISIC_0010361.jpg\n",
      "ISIC_0010380.jpg\n",
      "ISIC_0010382.jpg\n",
      "ISIC_0010441.jpg\n",
      "ISIC_0010445.jpg\n",
      "ISIC_0010448.jpg\n",
      "ISIC_0010466.jpg\n",
      "ISIC_0010475.jpg\n",
      "ISIC_0010490.jpg\n",
      "ISIC_0010494.jpg\n",
      "ISIC_0010557.jpg\n",
      "ISIC_0010562.jpg\n",
      "ISIC_0010570.jpg\n",
      "ISIC_0010854.jpg\n",
      "ISIC_0011085.jpg\n",
      "ISIC_0011115.jpg\n",
      "ISIC_0011158.jpg\n",
      "ISIC_0011166.jpg\n",
      "ISIC_0011169.jpg\n",
      "ISIC_0011208.jpg\n",
      "ISIC_0011225.jpg\n",
      "ISIC_0011295.jpg\n",
      "ISIC_0011322.jpg\n",
      "ISIC_0011338.jpg\n",
      "ISIC_0011339.jpg\n",
      "ISIC_0011345.jpg\n",
      "ISIC_0011361.jpg\n",
      "ISIC_0011362.jpg\n",
      "ISIC_0012086.jpg\n",
      "ISIC_0012116.jpg\n",
      "ISIC_0012127.jpg\n",
      "ISIC_0012151.jpg\n",
      "ISIC_0012156.jpg\n",
      "ISIC_0012160.jpg\n",
      "ISIC_0012182.jpg\n",
      "ISIC_0012203.jpg\n",
      "ISIC_0012207.jpg\n",
      "ISIC_0012208.jpg\n",
      "ISIC_0012223.jpg\n",
      "ISIC_0012248.jpg\n",
      "ISIC_0012356.jpg\n",
      "ISIC_0012376.jpg\n",
      "ISIC_0012381.jpg\n",
      "ISIC_0012413.jpg\n",
      "ISIC_0012442.jpg\n",
      "ISIC_0012473.jpg\n",
      "ISIC_0012478.jpg\n",
      "ISIC_0012487.jpg\n",
      "ISIC_0012551.jpg\n",
      "ISIC_0012653.jpg\n",
      "ISIC_0012656.jpg\n",
      "ISIC_0012671.jpg\n",
      "ISIC_0012835.jpg\n",
      "ISIC_0012883.jpg\n",
      "ISIC_0012891.jpg\n",
      "ISIC_0012898.jpg\n",
      "ISIC_0012911.jpg\n",
      "ISIC_0012949.jpg\n",
      "ISIC_0012988.jpg\n",
      "ISIC_0013044.jpg\n",
      "ISIC_0013073.jpg\n",
      "ISIC_0013087.jpg\n",
      "ISIC_0013106.jpg\n",
      "ISIC_0013128.jpg\n",
      "ISIC_0013169.jpg\n",
      "ISIC_0013196.jpg\n",
      "ISIC_0013197.jpg\n",
      "ISIC_0013207.jpg\n",
      "ISIC_0013227.jpg\n",
      "ISIC_0013229.jpg\n",
      "ISIC_0013269.jpg\n",
      "ISIC_0013274.jpg\n",
      "ISIC_0013275.jpg\n",
      "ISIC_0013356.jpg\n",
      "ISIC_0013360.jpg\n",
      "ISIC_0013365.jpg\n",
      "ISIC_0013378.jpg\n",
      "ISIC_0013414.jpg\n",
      "ISIC_0013455.jpg\n",
      "ISIC_0013518.jpg\n",
      "ISIC_0013565.jpg\n",
      "ISIC_0013580.jpg\n",
      "ISIC_0013585.jpg\n",
      "ISIC_0013621.jpg\n",
      "ISIC_0013626.jpg\n",
      "ISIC_0013636.jpg\n",
      "ISIC_0013651.jpg\n",
      "ISIC_0013667.jpg\n",
      "ISIC_0013670.jpg\n",
      "ISIC_0013689.jpg\n",
      "ISIC_0013733.jpg\n",
      "ISIC_0013748.jpg\n",
      "ISIC_0013775.jpg\n",
      "ISIC_0013796.jpg\n",
      "ISIC_0013802.jpg\n",
      "ISIC_0013806.jpg\n",
      "ISIC_0013830.jpg\n",
      "ISIC_0013861.jpg\n",
      "ISIC_0013897.jpg\n",
      "ISIC_0013918.jpg\n",
      "ISIC_0013982.jpg\n",
      "ISIC_0014073.jpg\n",
      "ISIC_0014157.jpg\n",
      "ISIC_0014181.jpg\n",
      "ISIC_0014217.jpg\n",
      "ISIC_0014233.jpg\n",
      "ISIC_0014248.jpg\n",
      "ISIC_0014253.jpg\n",
      "ISIC_0014273.jpg\n",
      "ISIC_0014289.jpg\n",
      "ISIC_0014324.jpg\n",
      "ISIC_0014331.jpg\n",
      "ISIC_0014361.jpg\n",
      "ISIC_0014365.jpg\n",
      "ISIC_0014366.jpg\n",
      "ISIC_0014438.jpg\n",
      "ISIC_0014489.jpg\n",
      "ISIC_0014525.jpg\n",
      "ISIC_0014585.jpg\n",
      "ISIC_0014609.jpg\n",
      "ISIC_0014625.jpg\n",
      "ISIC_0014688.jpg\n",
      "ISIC_0014694.jpg\n",
      "ISIC_0014713.jpg\n",
      "ISIC_0014716.jpg\n",
      "ISIC_0014760.jpg\n",
      "ISIC_0014770.jpg\n",
      "ISIC_0014783.jpg\n",
      "ISIC_0014806.jpg\n",
      "ISIC_0014818.jpg\n",
      "ISIC_0014825.jpg\n",
      "ISIC_0014830.jpg\n",
      "ISIC_0014835.jpg\n",
      "ISIC_0014867.jpg\n",
      "ISIC_0014883.jpg\n",
      "ISIC_0014968.jpg\n",
      "ISIC_0014969.jpg\n",
      "ISIC_0015003.jpg\n",
      "ISIC_0015016.jpg\n",
      "ISIC_0015019.jpg\n",
      "ISIC_0015037.jpg\n",
      "ISIC_0015040.jpg\n",
      "ISIC_0015078.jpg\n",
      "ISIC_0015113.jpg\n",
      "ISIC_0015118.jpg\n",
      "ISIC_0015133.jpg\n",
      "ISIC_0015167.jpg\n",
      "ISIC_0015226.jpg\n",
      "ISIC_0015250.jpg\n",
      "ISIC_0015254.jpg\n",
      "ISIC_0015353.jpg\n",
      "ISIC_0015411.jpg\n",
      "ISIC_0015419.jpg\n",
      "ISIC_0015627.jpg\n",
      "ISIC_0015951.jpg\n",
      "ISIC_0015952.jpg\n",
      "ISIC_0016023.jpg\n",
      "ISIC_0016028.jpg\n",
      "ISIC_0016043.jpg\n",
      "ISIC_0016060.jpg\n"
     ]
    }
   ],
   "source": [
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "\n",
    "model = AttentionUnetModel()\n",
    "model.load_weights(\"attention_unet_lesion.hdf5\")\n",
    "\n",
    "\n",
    "for file in file_names:\n",
    "    print(file)\n",
    "    grey_img = load_img(os.path.join(test_data_dir,file), target_size=(img_rows, img_cols), grayscale=True)\n",
    "    img = img_to_array(grey_img)\n",
    "    mean = np.mean(img)  # mean for data centering\n",
    "    std = np.std(img)  # std for data normalization\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    img = np.reshape(img,(1,)+img.shape)\n",
    "    results = model.predict(img)\n",
    "\n",
    "    result_img = array_to_img(results[0] * 255 )\n",
    "    #plt.imshow(result_img)\n",
    "    result_img.save(os.path.join(test_data_pred_dir, file.split('.')[0] + '_predict.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 1, 256, 256)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 32, 256, 256) 320         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 32, 256, 256) 9248        conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling2D) (None, 32, 128, 128) 0           conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 64, 128, 128) 18496       max_pooling2d_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 64, 128, 128) 36928       conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 128, 64, 64)  73856       max_pooling2d_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 128, 64, 64)  147584      conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling2D) (None, 128, 32, 32)  0           conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 256, 32, 32)  295168      max_pooling2d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 256, 32, 32)  590080      conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling2D) (None, 256, 16, 16)  0           conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 512, 16, 16)  1180160     max_pooling2d_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 512, 16, 16)  2359808     conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_36 (UpSampling2D) (None, 512, 32, 32)  0           conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "merge_36 (Merge)                (None, 768, 32, 32)  0           up_sampling2d_36[0][0]           \n",
      "                                                                 conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 256, 32, 32)  1769728     merge_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 256, 32, 32)  590080      conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_37 (UpSampling2D) (None, 256, 64, 64)  0           conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "merge_37 (Merge)                (None, 384, 64, 64)  0           up_sampling2d_37[0][0]           \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 128, 64, 64)  442496      merge_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 128, 64, 64)  147584      conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_38 (UpSampling2D) (None, 128, 128, 128 0           conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "merge_38 (Merge)                (None, 192, 128, 128 0           up_sampling2d_38[0][0]           \n",
      "                                                                 conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 64, 128, 128) 110656      merge_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 64, 128, 128) 36928       conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_39 (UpSampling2D) (None, 64, 256, 256) 0           conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "merge_39 (Merge)                (None, 96, 256, 256) 0           up_sampling2d_39[0][0]           \n",
      "                                                                 conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 32, 256, 256) 27680       merge_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 32, 256, 256) 9248        conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 1, 256, 256)  33          conv2d_206[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 7,846,081\n",
      "Trainable params: 7,846,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "image = file_names[0]\n",
    "fig = plt.figure()\n",
    "\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(load_img(os.path.join(test_data_dir,image)), shape = (256,256))\n",
    "a.set_title('Image')\n",
    "a.set_axis_off()\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(load_img(os.path.join(test_data_mask_dir,image.split('.')[0]+\"_segmentation.png\")), shape = (256,256))\n",
    "a.set_title('Mask')\n",
    "a.set_axis_off()\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(load_img(os.path.join(test_data_pred_dir,image.split('.')[0]+\"_predict.jpg\")), shape = (256,256))\n",
    "a.set_title('Prediction')\n",
    "a.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_e66lephant_output = model.output[:, 386]                          \n",
    "\n",
    "last_conv_layer = model.get_layer('block5_conv3')                         \n",
    "\n",
    "grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]   \n",
    "\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))                              \n",
    "\n",
    "iterate = K.function([model.input],\n",
    "                     [pooled_grads, last_conv_layer.output[0]])           \n",
    "\n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])                \n",
    "\n",
    "for i in range(512):                                                      \n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]             \n",
    "\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_207/Sigmoid:0' shape=(?, 1, 256, 256) dtype=float32>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Model in module keras.engine.training object:\n",
      "\n",
      "class Model(keras.engine.topology.Container)\n",
      " |  The `Model` class adds training & evaluation routines to a `Container`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Model\n",
      " |      keras.engine.topology.Container\n",
      " |      keras.engine.topology.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)\n",
      " |      Configures the model for training.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          optimizer: String (name of optimizer) or optimizer instance.\n",
      " |              See [optimizers](/optimizers).\n",
      " |          loss: String (name of objective function) or objective function.\n",
      " |              See [losses](/losses).\n",
      " |              If the model has multiple outputs, you can use a different loss\n",
      " |              on each output by passing a dictionary or a list of losses.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the sum of all individual losses.\n",
      " |          metrics: List of metrics to be evaluated by the model\n",
      " |              during training and testing.\n",
      " |              Typically you will use `metrics=['accuracy']`.\n",
      " |              To specify different metrics for different outputs of a\n",
      " |              multi-output model, you could also pass a dictionary,\n",
      " |              such as `metrics={'output_a': 'accuracy'}`.\n",
      " |          loss_weights: Optional list or dictionary specifying scalar\n",
      " |              coefficients (Python floats) to weight the loss contributions\n",
      " |              of different model outputs.\n",
      " |              The loss value that will be minimized by the model\n",
      " |              will then be the *weighted sum* of all individual losses,\n",
      " |              weighted by the `loss_weights` coefficients.\n",
      " |              If a list, it is expected to have a 1:1 mapping\n",
      " |              to the model's outputs. If a tensor, it is expected to map\n",
      " |              output names (strings) to scalar coefficients.\n",
      " |          sample_weight_mode: If you need to do timestep-wise\n",
      " |              sample weighting (2D weights), set this to `\"temporal\"`.\n",
      " |              `None` defaults to sample-wise weights (1D).\n",
      " |              If the model has multiple outputs, you can use a different\n",
      " |              `sample_weight_mode` on each output by passing a\n",
      " |              dictionary or a list of modes.\n",
      " |          weighted_metrics: List of metrics to be evaluated and weighted\n",
      " |              by sample_weight or class_weight during training and testing.\n",
      " |          target_tensors: By default, Keras will create placeholders for the\n",
      " |              model's target, which will be fed with the target data during\n",
      " |              training. If instead you would like to use your own\n",
      " |              target tensors (in turn, Keras will not expect external\n",
      " |              Numpy data for these targets at training time), you\n",
      " |              can specify them via the `target_tensors` argument. It can be\n",
      " |              a single tensor (for a single-output model), a list of tensors,\n",
      " |              or a dict mapping output names to target tensors.\n",
      " |          **kwargs: When using the Theano/CNTK backends, these arguments\n",
      " |              are passed into `K.function`.\n",
      " |              When using the TensorFlow backend,\n",
      " |              these arguments are passed into `tf.Session.run`.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid arguments for\n",
      " |              `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      " |  \n",
      " |  evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)\n",
      " |      Returns the loss value & metrics values for the model in test mode.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of test data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per evaluation step.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          verbose: 0 or 1. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the test samples, used for weighting the loss function.\n",
      " |              You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          steps: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring the evaluation round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  evaluate_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      " |      Evaluates the model on a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data\n",
      " |      as accepted by `test_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding tuples (inputs, targets)\n",
      " |              or (inputs, targets, sample_weights)\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: maximum size for the generator queue\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: if True, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)\n",
      " |      Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data (if the model has a single input),\n",
      " |              or list of Numpy arrays (if the model has multiple inputs).\n",
      " |              If input layers in the model are named, you can also pass a\n",
      " |              dictionary mapping input names to Numpy arrays.\n",
      " |              `x` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          y: Numpy array of target (label) data\n",
      " |              (if the model has a single output),\n",
      " |              or list of Numpy arrays (if the model has multiple outputs).\n",
      " |              If output layers in the model are named, you can also pass a\n",
      " |              dictionary mapping output names to Numpy arrays.\n",
      " |              `y` can be `None` (default) if feeding from\n",
      " |              framework-native tensors (e.g. TensorFlow data tensors).\n",
      " |          batch_size: Integer or `None`.\n",
      " |              Number of samples per gradient update.\n",
      " |              If unspecified, `batch_size` will default to 32.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire `x` and `y`\n",
      " |              data provided.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_split: Float between 0 and 1.\n",
      " |              Fraction of the training data to be used as validation data.\n",
      " |              The model will set apart this fraction of the training data,\n",
      " |              will not train on it, and will evaluate\n",
      " |              the loss and any model metrics\n",
      " |              on this data at the end of each epoch.\n",
      " |              The validation data is selected from the last samples\n",
      " |              in the `x` and `y` data provided, before shuffling.\n",
      " |          validation_data: tuple `(x_val, y_val)` or tuple\n",
      " |              `(x_val, y_val, val_sample_weights)` on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |              `validation_data` will override `validation_split`.\n",
      " |          shuffle: Boolean (whether to shuffle the training data\n",
      " |              before each epoch) or str (for 'batch').\n",
      " |              'batch' is a special option for dealing with the\n",
      " |              limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only).\n",
      " |              This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from\n",
      " |              an under-represented class.\n",
      " |          sample_weight: Optional Numpy array of weights for\n",
      " |              the training samples, used for weighting the loss function\n",
      " |              (during training only). You can either pass a flat (1D)\n",
      " |              Numpy array with the same length as the input samples\n",
      " |              (1:1 mapping between weights and samples),\n",
      " |              or in the case of temporal data,\n",
      " |              you can pass a 2D array with shape\n",
      " |              `(samples, sequence_length)`,\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              `sample_weight_mode=\"temporal\"` in `compile()`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |          steps_per_epoch: Integer or `None`.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              before declaring one epoch finished and starting the\n",
      " |              next epoch. When training with input tensors such as\n",
      " |              TensorFlow data tensors, the default `None` is equal to\n",
      " |              the number of samples in your dataset divided by\n",
      " |              the batch size, or 1 if that cannot be determined.\n",
      " |          validation_steps: Only relevant if `steps_per_epoch`\n",
      " |              is specified. Total number of steps (batches of samples)\n",
      " |              to validate before stopping.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: If the model was never compiled.\n",
      " |          ValueError: In case of mismatch between the provided input data\n",
      " |              and what the model expects.\n",
      " |  \n",
      " |  fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      " |      Trains the model on data generated batch-by-batch by a Python generator or an instance of `Sequence`.\n",
      " |      \n",
      " |      The generator is run in parallel to the model, for efficiency.\n",
      " |      For instance, this allows you to do real-time data augmentation\n",
      " |      on images on CPU in parallel to training your model on GPU.\n",
      " |      \n",
      " |      The use of `keras.utils.Sequence` guarantees the ordering\n",
      " |      and guarantees the single use of every input per epoch when\n",
      " |      using `use_multiprocessing=True`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: A generator or an instance of `Sequence`\n",
      " |              (`keras.utils.Sequence`) object in order to avoid\n",
      " |              duplicate data when using multiprocessing.\n",
      " |              The output of the generator must be either\n",
      " |              - a tuple `(inputs, targets)`\n",
      " |              - a tuple `(inputs, targets, sample_weights)`.\n",
      " |              This tuple (a single output of the generator) makes a single\n",
      " |              batch. Therefore, all arrays in this tuple must have the same\n",
      " |              length (equal to the size of this batch). Different batches may\n",
      " |              have different sizes. For example, the last batch of the epoch\n",
      " |              is commonly smaller than the others, if the size of the dataset\n",
      " |              is not divisible by the batch size.\n",
      " |              The generator is expected to loop over its data\n",
      " |              indefinitely. An epoch finishes when `steps_per_epoch`\n",
      " |              batches have been seen by the model.\n",
      " |          steps_per_epoch: Integer.\n",
      " |              Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before declaring one epoch\n",
      " |              finished and starting the next epoch. It should typically\n",
      " |              be equal to the number of samples of your dataset\n",
      " |              divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          epochs: Integer. Number of epochs to train the model.\n",
      " |              An epoch is an iteration over the entire data provided,\n",
      " |              as defined by `steps_per_epoch`.\n",
      " |              Note that in conjunction with `initial_epoch`,\n",
      " |              `epochs` is to be understood as \"final epoch\".\n",
      " |              The model is not trained for a number of iterations\n",
      " |              given by `epochs`, but merely until the epoch\n",
      " |              of index `epochs` is reached.\n",
      " |          verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      " |              0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      " |          callbacks: List of `keras.callbacks.Callback` instances.\n",
      " |              List of callbacks to apply during training.\n",
      " |              See [callbacks](/callbacks).\n",
      " |          validation_data: This can be either\n",
      " |              - a generator for the validation data\n",
      " |              - tuple `(x_val, y_val)`\n",
      " |              - tuple `(x_val, y_val, val_sample_weights)`\n",
      " |              on which to evaluate\n",
      " |              the loss and any model metrics at the end of each epoch.\n",
      " |              The model will not be trained on this data.\n",
      " |          validation_steps: Only relevant if `validation_data`\n",
      " |              is a generator. Total number of steps (batches of samples)\n",
      " |              to yield from `validation_data` generator before stopping\n",
      " |              at the end of every epoch. It should typically\n",
      " |              be equal to the number of samples of your\n",
      " |              validation dataset divided by the batch size.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(validation_data)` as a number of steps.\n",
      " |          class_weight: Optional dictionary mapping class indices (integers)\n",
      " |              to a weight (float) value, used for weighting the loss function\n",
      " |              (during training only). This can be useful to tell the model to\n",
      " |              \"pay more attention\" to samples from an under-represented class.\n",
      " |          max_queue_size: Integer. Maximum size for the generator queue.\n",
      " |              If unspecified, `max_queue_size` will default to 10.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process-based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: Boolean.\n",
      " |              If `True`, use process-based threading.\n",
      " |              If unspecified, `use_multiprocessing` will default to `False`.\n",
      " |              Note that because this implementation relies on multiprocessing,\n",
      " |              you should not pass non-picklable arguments to the generator\n",
      " |              as they can't be passed easily to children processes.\n",
      " |          shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      " |              the beginning of each epoch. Only used with instances\n",
      " |              of `Sequence` (`keras.utils.Sequence`).\n",
      " |              Has no effect when `steps_per_epoch` is not `None`.\n",
      " |          initial_epoch: Integer.\n",
      " |              Epoch at which to start training\n",
      " |              (useful for resuming a previous training run).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `History` object. Its `History.history` attribute is\n",
      " |          a record of training loss values and metrics values\n",
      " |          at successive epochs, as well as validation loss values\n",
      " |          and validation metrics values (if applicable).\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields data in an invalid format.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |          def generate_arrays_from_file(path):\n",
      " |              while True:\n",
      " |                  with open(path) as f:\n",
      " |                      for line in f:\n",
      " |                          # create numpy arrays of input data\n",
      " |                          # and labels, from each line in the file\n",
      " |                          x1, x2, y = process_line(line)\n",
      " |                          yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      " |      \n",
      " |          model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      " |                              steps_per_epoch=10000, epochs=10)\n",
      " |      ```\n",
      " |  \n",
      " |  predict(self, x, batch_size=None, verbose=0, steps=None)\n",
      " |      Generates output predictions for the input samples.\n",
      " |      \n",
      " |      Computation is done in batches.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: The input data, as a Numpy array\n",
      " |              (or list of Numpy arrays if the model has multiple outputs).\n",
      " |          batch_size: Integer. If unspecified, it will default to 32.\n",
      " |          verbose: Verbosity mode, 0 or 1.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              before declaring the prediction round finished.\n",
      " |              Ignored with the default value of `None`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of mismatch between the provided\n",
      " |              input data and the model's expectations,\n",
      " |              or in case a stateful model receives a number of samples\n",
      " |              that is not a multiple of the batch size.\n",
      " |  \n",
      " |  predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
      " |      Generates predictions for the input samples from a data generator.\n",
      " |      \n",
      " |      The generator should return the same kind of data as accepted by\n",
      " |      `predict_on_batch`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          generator: Generator yielding batches of input samples\n",
      " |              or an instance of Sequence (keras.utils.Sequence)\n",
      " |              object in order to avoid duplicate data\n",
      " |              when using multiprocessing.\n",
      " |          steps: Total number of steps (batches of samples)\n",
      " |              to yield from `generator` before stopping.\n",
      " |              Optional for `Sequence`: if unspecified, will use\n",
      " |              the `len(generator)` as a number of steps.\n",
      " |          max_queue_size: Maximum size for the generator queue.\n",
      " |          workers: Integer. Maximum number of processes to spin up\n",
      " |              when using process based threading.\n",
      " |              If unspecified, `workers` will default to 1. If 0, will\n",
      " |              execute the generator on the main thread.\n",
      " |          use_multiprocessing: If `True`, use process based threading.\n",
      " |              Note that because\n",
      " |              this implementation relies on multiprocessing,\n",
      " |              you should not pass\n",
      " |              non picklable arguments to the generator\n",
      " |              as they can't be passed\n",
      " |              easily to children processes.\n",
      " |          verbose: verbosity mode, 0 or 1.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case the generator yields\n",
      " |              data in an invalid format.\n",
      " |  \n",
      " |  predict_on_batch(self, x)\n",
      " |      Returns predictions for a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Input samples, as a Numpy array.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Numpy array(s) of predictions.\n",
      " |  \n",
      " |  test_on_batch(self, x, y, sample_weight=None)\n",
      " |      Test the model on a single batch of samples.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of test data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar test loss (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  train_on_batch(self, x, y, sample_weight=None, class_weight=None)\n",
      " |      Runs a single gradient update on a single batch of data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Numpy array of training data,\n",
      " |              or list of Numpy arrays if the model has multiple inputs.\n",
      " |              If all inputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping input names to Numpy arrays.\n",
      " |          y: Numpy array of target data,\n",
      " |              or list of Numpy arrays if the model has multiple outputs.\n",
      " |              If all outputs in the model are named,\n",
      " |              you can also pass a dictionary\n",
      " |              mapping output names to Numpy arrays.\n",
      " |          sample_weight: Optional array of the same length as x, containing\n",
      " |              weights to apply to the model's loss for each sample.\n",
      " |              In the case of temporal data, you can pass a 2D array\n",
      " |              with shape (samples, sequence_length),\n",
      " |              to apply a different weight to every timestep of every sample.\n",
      " |              In this case you should make sure to specify\n",
      " |              sample_weight_mode=\"temporal\" in compile().\n",
      " |          class_weight: Optional dictionary mapping\n",
      " |              class indices (integers) to\n",
      " |              a weight (float) to apply to the model's loss for the samples\n",
      " |              from this class during training.\n",
      " |              This can be useful to tell the model to \"pay more attention\" to\n",
      " |              samples from an under-represented class.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Scalar training loss\n",
      " |          (if the model has a single output and no metrics)\n",
      " |          or list of scalars (if the model has multiple outputs\n",
      " |          and/or metrics). The attribute `model.metrics_names` will give you\n",
      " |          the display labels for the scalar outputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.topology.Container:\n",
      " |  \n",
      " |  __init__(self, inputs, outputs, name=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  call(self, inputs, mask=None)\n",
      " |      Calls the model on new inputs.\n",
      " |      \n",
      " |      In this case `call` just reapplies\n",
      " |      all ops in the graph to the new inputs\n",
      " |      (e.g. build a new computational graph from the provided inputs).\n",
      " |      \n",
      " |      A model is callable on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: A tensor or list of tensors.\n",
      " |          mask: A mask or list of masks. A mask can be\n",
      " |              either a tensor or None (no mask).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor if there is a single output, or\n",
      " |          a list of tensors if there are more than one outputs.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      # Returns\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      Assumes that the layer will be built\n",
      " |      to match that input shape provided.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Container` (one layer of abstraction above).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_layer(self, name=None, index=None)\n",
      " |      Retrieves a layer based on either its name (unique) or index.\n",
      " |      \n",
      " |      If `name` and `index` are both provided, `index` will take precedence.\n",
      " |      \n",
      " |      Indices are based on order of horizontal graph traversal (bottom-up).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, name of layer.\n",
      " |          index: Integer, index of layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A layer instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of invalid layer name or index.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Retrieves the weights of the model.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A flat list of Numpy arrays.\n",
      " |  \n",
      " |  load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False)\n",
      " |      Loads all layer weights from a HDF5 save file.\n",
      " |      \n",
      " |      If `by_name` is False (default) weights are loaded\n",
      " |      based on the network's topology, meaning the architecture\n",
      " |      should be the same as when the weights were saved.\n",
      " |      Note that layers that don't have weights are not taken\n",
      " |      into account in the topological ordering, so adding or\n",
      " |      removing layers is fine as long as they don't have weights.\n",
      " |      \n",
      " |      If `by_name` is True, weights are loaded into layers\n",
      " |      only if they share the same name. This is useful\n",
      " |      for fine-tuning or transfer-learning models where\n",
      " |      some of the layers have changed.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the weights file to load.\n",
      " |          by_name: Boolean, whether to load weights by name\n",
      " |              or by topological order.\n",
      " |          skip_mismatch: Boolean, whether to skip loading of layers\n",
      " |              where there is a mismatch in the number of weights,\n",
      " |              or a mismatch in the shape of the weight\n",
      " |              (only valid when `by_name`=True).\n",
      " |          reshape: Reshape weights to fit the layer when the correct number\n",
      " |              of weight arrays is present but their shape does not match.\n",
      " |      \n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  reset_states(self)\n",
      " |  \n",
      " |  run_internal_graph(self, inputs, masks=None)\n",
      " |      Computes output tensors for new inputs.\n",
      " |      \n",
      " |      # Note:\n",
      " |          - Expects `inputs` to be a list (potentially with 1 element).\n",
      " |          - Can be run on non-Keras tensors.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: List of tensors\n",
      " |          masks: List of masks (tensors or None).\n",
      " |      \n",
      " |      # Returns\n",
      " |          Three lists: output_tensors, output_masks, output_shapes\n",
      " |  \n",
      " |  save(self, filepath, overwrite=True, include_optimizer=True)\n",
      " |      Saves the model to a single HDF5 file.\n",
      " |      \n",
      " |      The savefile includes:\n",
      " |          - The model architecture, allowing to re-instantiate the model.\n",
      " |          - The model weights.\n",
      " |          - The state of the optimizer, allowing to resume training\n",
      " |              exactly where you left off.\n",
      " |      \n",
      " |      This allows you to save the entirety of the state of a model\n",
      " |      in a single file.\n",
      " |      \n",
      " |      Saved models can be reinstantiated via `keras.models.load_model`.\n",
      " |      The model returned by `load_model`\n",
      " |      is a compiled model ready to be used (unless the saved model\n",
      " |      was never compiled in the first place).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |          include_optimizer: If True, save optimizer's state together.\n",
      " |      \n",
      " |      # Example\n",
      " |      \n",
      " |      ```python\n",
      " |      from keras.models import load_model\n",
      " |      \n",
      " |      model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
      " |      del model  # deletes the existing model\n",
      " |      \n",
      " |      # returns a compiled model\n",
      " |      # identical to the previous one\n",
      " |      model = load_model('my_model.h5')\n",
      " |      ```\n",
      " |  \n",
      " |  save_weights(self, filepath, overwrite=True)\n",
      " |      Dumps all layer weights to a HDF5 file.\n",
      " |      \n",
      " |      The weight file has:\n",
      " |          - `layer_names` (attribute), a list of strings\n",
      " |              (ordered names of model layers).\n",
      " |          - For every layer, a `group` named `layer.name`\n",
      " |              - For every such layer group, a group attribute `weight_names`,\n",
      " |                  a list of strings\n",
      " |                  (ordered names of weights tensor of the layer).\n",
      " |              - For every weight in the layer, a dataset\n",
      " |                  storing the weight value, named after the weight tensor.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          filepath: String, path to the file to save the weights to.\n",
      " |          overwrite: Whether to silently overwrite any existing file at the\n",
      " |              target location, or provide the user with a manual prompt.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ImportError: If h5py is not available.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the model.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          weights: A list of Numpy arrays with shapes and types matching\n",
      " |              the output of `model.get_weights()`.\n",
      " |  \n",
      " |  summary(self, line_length=None, positions=None, print_fn=None)\n",
      " |      Prints a string summary of the network.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          line_length: Total length of printed lines\n",
      " |              (e.g. set this to adapt the display to different\n",
      " |              terminal window sizes).\n",
      " |          positions: Relative or absolute positions of log elements\n",
      " |              in each line. If not provided,\n",
      " |              defaults to `[.33, .55, .67, 1.]`.\n",
      " |          print_fn: Print function to use.\n",
      " |              It will be called on each line of the summary.\n",
      " |              You can set it to a custom function\n",
      " |              in order to capture the string summary.\n",
      " |              It defaults to `print` (prints to stdout).\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a JSON save file, use\n",
      " |      `keras.models.model_from_json(json_string, custom_objects={})`.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A JSON string.\n",
      " |  \n",
      " |  to_yaml(self, **kwargs)\n",
      " |      Returns a yaml string containing the network configuration.\n",
      " |      \n",
      " |      To load a network from a yaml save file, use\n",
      " |      `keras.models.model_from_yaml(yaml_string, custom_objects={})`.\n",
      " |      \n",
      " |      `custom_objects` should be a dictionary mapping\n",
      " |      the names of custom losses / layers / etc to the corresponding\n",
      " |      functions / classes.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `yaml.dump()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A YAML string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.topology.Container:\n",
      " |  \n",
      " |  from_config(config, custom_objects=None) from builtins.type\n",
      " |      Instantiates a Model from its config (output of `get_config()`).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          config: Model config dictionary.\n",
      " |          custom_objects: Optional dictionary mapping names\n",
      " |              (strings) to custom classes or functions to be\n",
      " |              considered during deserialization.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A model instance.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: In case of improperly formatted config dict.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.topology.Container:\n",
      " |  \n",
      " |  input_spec\n",
      " |      Gets the model's input specs.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of `InputSpec` instances (one per input to the model)\n",
      " |              or a single instance if the model has only one input.\n",
      " |  \n",
      " |  losses\n",
      " |      Retrieves the model's losses.\n",
      " |      \n",
      " |      Will only include losses that are either\n",
      " |      inconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include losses that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of loss tensors.\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |  \n",
      " |  state_updates\n",
      " |      Returns the `updates` from all layers that are stateful.\n",
      " |      \n",
      " |      This is useful for separating training updates and\n",
      " |      state updates, e.g. when we need to update a layer's internal state\n",
      " |      during prediction.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable_weights\n",
      " |  \n",
      " |  updates\n",
      " |      Retrieves the model's updates.\n",
      " |      \n",
      " |      Will only include updates that are either\n",
      " |      inconditional, or conditional on inputs to this model\n",
      " |      (e.g. will not include updates that depend on tensors\n",
      " |      that aren't inputs to this model).\n",
      " |      \n",
      " |      # Returns\n",
      " |          A list of update ops.\n",
      " |  \n",
      " |  uses_learning_phase\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __call__(self, inputs, **kwargs)\n",
      " |      Wrapper around self.call(), for handling internal references.\n",
      " |      \n",
      " |      If a Keras tensor is passed:\n",
      " |          - We call self._add_inbound_node().\n",
      " |          - If necessary, we `build` the layer to match\n",
      " |              the _keras_shape of the input(s).\n",
      " |          - We update the _keras_shape of every input tensor with\n",
      " |              its new shape (obtained via self.compute_output_shape).\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |          - We update the _keras_history of the output tensor(s)\n",
      " |              with the current layer.\n",
      " |              This is done as part of _add_inbound_node().\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: Can be a tensor or list/tuple of tensors.\n",
      " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output of the layer's `call` method.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case the layer is missing shape information\n",
      " |              for its `build` call.\n",
      " |  \n",
      " |  add_loss(self, losses, inputs=None)\n",
      " |      Adds losses to the layer.\n",
      " |      \n",
      " |      The loss may potentially be conditional on some inputs tensors,\n",
      " |      for instance activity losses are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          losses: loss tensor or list of loss tensors\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the losses as conditional on these inputs.\n",
      " |              If None is passed, the loss is assumed unconditional\n",
      " |              (e.g. L2 weight regularization, which only depends\n",
      " |              on the layer's weights variables, not on any inputs tensors).\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Adds updates to the layer.\n",
      " |      \n",
      " |      The updates may potentially be conditional on some inputs tensors,\n",
      " |      for instance batch norm updates are conditional on the layer's inputs.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          updates: update op or list of update ops\n",
      " |              to add to the layer.\n",
      " |          inputs: input tensor or list of inputs tensors to mark\n",
      " |              the updates as conditional on these inputs.\n",
      " |              If None is passed, the updates are assumed unconditional.\n",
      " |  \n",
      " |  add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
      " |      Adds a weight variable to the layer.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          name: String, the name for the weight variable.\n",
      " |          shape: The shape tuple of the weight.\n",
      " |          dtype: The dtype of the weight.\n",
      " |          initializer: An Initializer instance (callable).\n",
      " |          regularizer: An optional Regularizer instance.\n",
      " |          trainable: A boolean, whether the weight should\n",
      " |              be trained via backprop or not (assuming\n",
      " |              that the layer itself is also trainable).\n",
      " |          constraint: An optional Constraint instance.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The created weight variable.\n",
      " |  \n",
      " |  assert_input_compatibility(self, inputs)\n",
      " |      Checks compatibility between the layer and provided inputs.\n",
      " |      \n",
      " |      This checks that the tensor(s) `input`\n",
      " |      verify the input assumptions of the layer\n",
      " |      (if any). If not, exceptions are raised.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          inputs: input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          ValueError: in case of mismatch between\n",
      " |              the provided inputs and the expectations of the layer.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the layer weights.\n",
      " |      \n",
      " |      Must be implemented on all layers that have weights.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          input_shape: Keras tensor (future input to layer)\n",
      " |              or list/tuple of Keras tensors to reference\n",
      " |              for weight shape computations.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Counts the total number of scalars composing the weights.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An integer count.\n",
      " |      \n",
      " |      # Raises\n",
      " |          RuntimeError: if the layer isn't yet built\n",
      " |              (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.topology.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  built\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Input shape tuple\n",
      " |          (or list of input shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output tensor or list of output tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape tuple(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one inbound node,\n",
      " |      or if all inbound nodes have the same output shape.\n",
      " |      \n",
      " |      # Returns\n",
      " |          Output shape tuple\n",
      " |          (or list of input shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      # Raises\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  weights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
